[
  {
    "objectID": "UMAP.html",
    "href": "UMAP.html",
    "title": "",
    "section": "",
    "text": "æ— ç›‘ç£å­¦ä¹ ï¼ˆUnsupervised Learningï¼‰UMAP CodeShow All CodeHide All CodeView Source\n1 UMAP\nUniform Manifold Approximation and Projection (UMAP)\nhttps://github.com/jlmelville/uwot\nhttps://github.com/tkonopka/umap\n2018å¹´McInnesæå‡ºäº†ç®—æ³•ï¼ŒUMAPï¼ˆUniform Manifold Approximation and Projection for Dimension Reductionï¼Œä¸€è‡´çš„æµå½¢é€¼è¿‘å’ŒæŠ•å½±ä»¥è¿›è¡Œé™ç»´ï¼‰ã€‚ ä¸€è‡´çš„æµå½¢è¿‘ä¼¼å’ŒæŠ•å½±ï¼ˆUMAPï¼‰æ˜¯ä¸€ç§é™ç»´æŠ€æœ¯ï¼Œç±»ä¼¼äºt-SNEï¼Œå¯ç”¨äºå¯è§†åŒ–ï¼Œä½†ä¹Ÿå¯ç”¨äºä¸€èˆ¬çš„éçº¿æ€§é™ç»´ã€‚ è¯¥ç®—æ³•åŸºäºå…³äºæ•°æ®çš„ä¸‰ä¸ªå‡è®¾ï¼š\n\næ•°æ®å‡åŒ€åˆ†å¸ƒåœ¨é»æ›¼æµå½¢ä¸Šï¼ˆRiemannian manifoldï¼‰ï¼›\né»æ›¼åº¦é‡æ˜¯å±€éƒ¨æ’å®šçš„ï¼ˆæˆ–å¯ä»¥è¿™æ ·è¿‘ä¼¼ï¼‰ï¼›\næµå½¢æ˜¯å±€éƒ¨è¿æ¥çš„ã€‚\n\nhttps://jlmelville.github.io/uwot/index.html\n\nCodelibrary(uwot)\nhead(iris)\n#&gt;   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#&gt; 1          5.1         3.5          1.4         0.2  setosa\n#&gt; 2          4.9         3.0          1.4         0.2  setosa\n#&gt; 3          4.7         3.2          1.3         0.2  setosa\n#&gt; 4          4.6         3.1          1.5         0.2  setosa\n#&gt; 5          5.0         3.6          1.4         0.2  setosa\n#&gt; 6          5.4         3.9          1.7         0.4  setosa\ncolors = rainbow(length(unique(iris$Species)))\nnames(colors) = unique(iris$Species)\n# umap2 is a version of the umap() function with better defaults\niris_umap2 &lt;- umap2(iris[1:4]) |&gt; as_tibble()\n#&gt; Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n#&gt; `.name_repair` is omitted as of tibble 2.0.0.\n#&gt; â„¹ Using compatibility `.name_repair`.\n\nggplot(iris_umap2,aes(V1,V2))+\n    geom_text(aes(label=iris$Species),color=colors[iris$Species])\n\n\n\n\n\n\n\n\nCode# but you can still use the umap function (which most of the existing \n# documentation does)\niris_umap &lt;- umap(iris[1:4]) |&gt; as_tibble()\n\nggplot(iris_umap,aes(V1,V2))+\n    geom_text(aes(label=iris$Species),color=colors[iris$Species])\n\n\n\n\n\n\n\n\nCodelibrary(uwot)\n\nset.seed(42) # ä¸ºäº†ç»“æœå¯é‡å¤\nuwot_result &lt;- umap(iris[1:4])\n\n# å°†ç»“æœè½¬æ¢ä¸ºæ•°æ®æ¡†\nuwot_df &lt;- as.data.frame(uwot_result)\ncolnames(uwot_df) &lt;- c(\"UMAP1\", \"UMAP2\")\nuwot_df$Species &lt;- iris$Species\n\n# å¯è§†åŒ–\nggplot(uwot_df, aes(x = UMAP1, y = UMAP2, color = Species)) +\n  geom_point(size = 2) +\n  labs(title = \"UMAP of Iris Dataset (uwot)\",\n       x = \"UMAP Dimension 1\",\n       y = \"UMAP Dimension 2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "æ— ç›‘ç£å­¦ä¹ ï¼ˆUnsupervised Learningï¼‰",
      "UMAP"
    ]
  },
  {
    "objectID": "t-SNE.html",
    "href": "t-SNE.html",
    "title": "",
    "section": "",
    "text": "æ— ç›‘ç£å­¦ä¹ ï¼ˆUnsupervised Learningï¼‰t-SNE CodeShow All CodeHide All CodeView Source\n1 t-SNE\nt-Distributed Stochastic Neighbor Embedding (t-SNE)æ˜¯ä¸€ç§é™ç»´æŠ€æœ¯ï¼Œç”¨äºåœ¨äºŒç»´æˆ–ä¸‰ç»´çš„ä½ç»´ç©ºé—´ä¸­è¡¨ç¤ºé«˜ç»´æ•°æ®é›†ï¼Œä»è€Œä½¿å…¶å¯è§†åŒ–ã€‚ä¸å…¶ä»–é™ç»´ç®—æ³•(å¦‚PCA)ç›¸æ¯”ï¼Œt-SNEåˆ›å»ºäº†ä¸€ä¸ªç¼©å°çš„ç‰¹å¾ç©ºé—´ï¼Œç›¸ä¼¼çš„æ ·æœ¬ç”±é™„è¿‘çš„ç‚¹å»ºæ¨¡ï¼Œä¸ç›¸ä¼¼çš„æ ·æœ¬ç”±é«˜æ¦‚ç‡çš„è¿œç‚¹å»ºæ¨¡ã€‚\nt-Distributed Stochastic Neighbor Embedding (t-SNE)\n\nCodelibrary(tsne)\n\nhead(iris)\n#&gt;   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#&gt; 1          5.1         3.5          1.4         0.2  setosa\n#&gt; 2          4.9         3.0          1.4         0.2  setosa\n#&gt; 3          4.7         3.2          1.3         0.2  setosa\n#&gt; 4          4.6         3.1          1.5         0.2  setosa\n#&gt; 5          5.0         3.6          1.4         0.2  setosa\n#&gt; 6          5.4         3.9          1.7         0.4  setosa\n\ncolors = rainbow(length(unique(iris$Species)))\nnames(colors) = unique(iris$Species)\necb = function(x, y) {\n    plot(x, t = 'n')\n    text(x, labels = iris$Species, col = colors[iris$Species])\n}\ntsne_iris = tsne(iris[,1:4], epoch_callback = ecb, perplexity=50)\n#&gt; sigma summary: Min. : 0.565012665854053 |1st Qu. : 0.681985646004023 |Median : 0.713004330336136 |Mean : 0.716213420895748 |3rd Qu. : 0.74581655363904 |Max. : 0.874979764925049 |\n#&gt; Epoch: Iteration #100 error is: 12.0521968962333\n#&gt; Epoch: Iteration #200 error is: 0.278293775495887\n\n\n\n\n\n\n#&gt; Epoch: Iteration #300 error is: 0.277972566238466\n\n\n\n\n\n\n#&gt; Epoch: Iteration #400 error is: 0.277972360425316\n\n\n\n\n\n\n#&gt; Epoch: Iteration #500 error is: 0.277972360400364\n\n\n\n\n\n\n#&gt; Epoch: Iteration #600 error is: 0.277972360400287\n\n\n\n\n\n\n#&gt; Epoch: Iteration #700 error is: 0.277972360400287\n\n\n\n\n\n\n#&gt; Epoch: Iteration #800 error is: 0.277972360400287\n\n\n\n\n\n\n#&gt; Epoch: Iteration #900 error is: 0.277972360400287\n\n\n\n\n\n\n#&gt; Epoch: Iteration #1000 error is: 0.277972360400287\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# compare to PCA\ndev.new()\npca_iris = princomp(iris[,1:4])$scores[,1:2]\nplot(pca_iris, t='n')\ntext(pca_iris, labels=iris$Species,col=colors[iris$Species])\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "æ— ç›‘ç£å­¦ä¹ ï¼ˆUnsupervised Learningï¼‰",
      "t-SNE"
    ]
  },
  {
    "objectID": "Regression.html",
    "href": "Regression.html",
    "title": "",
    "section": "",
    "text": "ç›‘ç£å­¦ä¹ ï¼ˆSupervised Learningï¼‰å›å½’ Code\n\n\n\n\n\n1 å›å½’\nç”¨äºé¢„æµ‹è¿ç»­å‹æ•°å€¼ã€‚\n\nçº¿æ€§å›å½’ï¼ˆLinear Regressionï¼‰ï¼šé¢„æµ‹å› å˜é‡ä¸ä¸€ä¸ªæˆ–å¤šä¸ªè‡ªå˜é‡ä¹‹é—´çš„çº¿æ€§å…³ç³»ã€‚\nå²­å›å½’ï¼ˆRidge Regressionï¼‰ï¼šçº¿æ€§å›å½’çš„æ­£åˆ™åŒ–ç‰ˆæœ¬ï¼Œå‡å°‘è¿‡æ‹Ÿåˆã€‚\nLassoå›å½’ï¼ˆLasso Regressionï¼‰ï¼šå¦ä¸€ç§æ­£åˆ™åŒ–çš„çº¿æ€§å›å½’ï¼Œé€šè¿‡L1æ­£åˆ™åŒ–è¿›è¡Œç‰¹å¾é€‰æ‹©ã€‚\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "ç›‘ç£å­¦ä¹ ï¼ˆSupervised Learningï¼‰",
      "å›å½’"
    ]
  },
  {
    "objectID": "PCA.html",
    "href": "PCA.html",
    "title": "",
    "section": "",
    "text": "æ— ç›‘ç£å­¦ä¹ ï¼ˆUnsupervised Learningï¼‰ä¸»æˆåˆ†åˆ†æ CodeShow All CodeHide All CodeView Source",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "æ— ç›‘ç£å­¦ä¹ ï¼ˆUnsupervised Learningï¼‰",
      "ä¸»æˆåˆ†åˆ†æ"
    ]
  },
  {
    "objectID": "PCA.html#è‡ªå®šä¹‰-pca-æ­¥éª¤",
    "href": "PCA.html#è‡ªå®šä¹‰-pca-æ­¥éª¤",
    "title": "",
    "section": "\n1.1 è‡ªå®šä¹‰ PCA æ­¥éª¤",
    "text": "1.1 è‡ªå®šä¹‰ PCA æ­¥éª¤\n\n1.1.1 æ•°æ®æ ‡å‡†åŒ–\nRä¸­æ•°æ®æ¡† nä¸ªè§‚æµ‹ï¼Œpä¸ªå˜é‡\n\\[\nX=\n\\begin{bmatrix}\n  x_{11}& x_{12} & ... & x_{1p}\\\\\nx_{21} & x_{22} & ... & x_{2p}\\\\\n\\vdots  &  \\vdots &   & \\vdots \\\\\n  x_{n1}& x_{n2} & ... & x_{np}\n\\end{bmatrix}\n=(X_1,X_2,...X_p)\n\\]\nå¯¹åŸå§‹æ•°æ®çŸ©é˜µæ ‡å‡†åŒ–ï¼Œæ¶ˆé™¤é‡çº²å’Œæ•°é‡çº§çš„å½±å“ã€‚\næ•°æ®æ ‡å‡†åŒ–ç¡®ä¿å˜é‡åœ¨ç›¸åŒçš„å°ºåº¦ä¸Šï¼Œè¿™å¯¹äºPCAéå¸¸é‡è¦ã€‚\nä½¿ç”¨Rè¯­è¨€å†…ç½®çš„ USArrests æ•°æ®é›†ï¼š\n\nCodedf &lt;- as_tibble(USArrests, rownames = \"state\") |&gt; column_to_rownames(\"state\")\nhead(df)\n#&gt;            Murder Assault UrbanPop Rape\n#&gt; Alabama      13.2     236       58 21.2\n#&gt; Alaska       10.0     263       48 44.5\n#&gt; Arizona       8.1     294       80 31.0\n#&gt; Arkansas      8.8     190       50 19.5\n#&gt; California    9.0     276       91 40.6\n#&gt; Colorado      7.9     204       78 38.7\n\ndf_center &lt;- scale(df,center = T,scale = T)\n\n\n\n1.1.2 è®¡ç®—åæ–¹å·®çŸ©é˜µ\nå½“\\(\\Sigma\\) æœªçŸ¥æ—¶ï¼Œå…¶ç”¨å…¶ä¼°è®¡å€¼æ ·æœ¬åæ–¹å·®çŸ©é˜µSpÃ—p ä»£æ›¿\n\\[\nS=\\frac{AA^T}{n-1}\n\\]\n\n\n\\(A_{ pÃ—n}\\) æ˜¾ç¤ºæ¥è‡ªæ¯ä¸ªå˜é‡å€¼ä¸å…¶å‡å€¼çš„åå·® \\(X_i-\\bar X\\)ï¼›\n\n\\((AA^T)_{ii}\\) æ˜¾ç¤ºåå·®å¹³æ–¹å’Œ ï¼ˆæ ·æœ¬æ–¹å·® \\(s_i^2\\) ï¼‰ï¼›\n\n\\((AA^T)_{ij}ï¼Œi\\ne j\\) æ˜¾ç¤ºæ ·æœ¬åæ–¹å·® \\(s_{ij} = (A çš„è¡Œ i) Â· (A çš„è¡Œ j)\\)ã€‚\n\n\nCode# æ‰‹åŠ¨è®¡ç®—åæ–¹å·®çŸ©é˜µ\nA &lt;- as.matrix(t((df_center)))\nAA_T &lt;- A %*% t(A)\nS &lt;- AA_T / (nrow(df_center) - 1)\nS\n#&gt;              Murder   Assault   UrbanPop      Rape\n#&gt; Murder   1.00000000 0.8018733 0.06957262 0.5635788\n#&gt; Assault  0.80187331 1.0000000 0.25887170 0.6652412\n#&gt; UrbanPop 0.06957262 0.2588717 1.00000000 0.4113412\n#&gt; Rape     0.56357883 0.6652412 0.41134124 1.0000000\n# å†…ç½®åæ–¹å·®çŸ©é˜µå‡½æ•°\ncov(df_center)\n#&gt;              Murder   Assault   UrbanPop      Rape\n#&gt; Murder   1.00000000 0.8018733 0.06957262 0.5635788\n#&gt; Assault  0.80187331 1.0000000 0.25887170 0.6652412\n#&gt; UrbanPop 0.06957262 0.2588717 1.00000000 0.4113412\n#&gt; Rape     0.56357883 0.6652412 0.41134124 1.0000000\n\n\n\n1.1.3 è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ\nç›¸å…³ç³»æ•°çŸ©é˜µ \\(R=(r_{ij})\\) çš„å…¬å¼ä¸ºï¼š\n\\[\nr_{ij}=\\frac {S_{ij}}{\\sqrt{S_{ii}Ã—S_{jj}}}\n\\]\n\nCode# å®šä¹‰è‡ªå®šä¹‰å‡½æ•°è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ\nr &lt;- function(df){\n    df &lt;- as.data.frame(df)\n    n=length(df)\n    names &lt;- colnames(df)\n    df &lt;- scale(df)\n    S &lt;- cov(df)\n    r &lt;- matrix(data = NA,n,n,dimnames = list(names,names))\n    for(i in 1:n){\n        for(j in 1:n){\n            r[i,j]=S[i,j]/sqrt(S[i,i]*S[j,j])\n        }\n    }\n    return(r)\n}\nr(df)\n#&gt;              Murder   Assault   UrbanPop      Rape\n#&gt; Murder   1.00000000 0.8018733 0.06957262 0.5635788\n#&gt; Assault  0.80187331 1.0000000 0.25887170 0.6652412\n#&gt; UrbanPop 0.06957262 0.2588717 1.00000000 0.4113412\n#&gt; Rape     0.56357883 0.6652412 0.41134124 1.0000000\n\n# ä½¿ç”¨å†…ç½®çš„ç›¸å…³ç³»æ•°çŸ©é˜µå‡½æ•°\ncor(df_center)\n#&gt;              Murder   Assault   UrbanPop      Rape\n#&gt; Murder   1.00000000 0.8018733 0.06957262 0.5635788\n#&gt; Assault  0.80187331 1.0000000 0.25887170 0.6652412\n#&gt; UrbanPop 0.06957262 0.2588717 1.00000000 0.4113412\n#&gt; Rape     0.56357883 0.6652412 0.41134124 1.0000000\n\n\n\n1.1.4 æ€»æ–¹å·®\næ€»æ–¹å·®T=æ‰€æœ‰ç‰¹å¾å€¼çš„æ€»å’Œ=æ ·æœ¬æ–¹å·®çš„æ€»å’Œ=åæ–¹å·®çŸ©é˜µçš„è¿¹(å¯¹è§’çº¿çš„æ€»å’Œ)\nsum(eigen(AA_T)$values)=sum(diag(AA_T))= sum(svd$d^2)\n\nCode# AA^Tçš„ç‰¹å¾å€¼\ny &lt;- eigen(AA_T)\ny$values\n#&gt; [1] 121.531837  48.498492  17.471596   8.498074\ny$vectors\n#&gt;            [,1]       [,2]       [,3]        [,4]\n#&gt; [1,] -0.5358995  0.4181809 -0.3412327  0.64922780\n#&gt; [2,] -0.5831836  0.1879856 -0.2681484 -0.74340748\n#&gt; [3,] -0.2781909 -0.8728062 -0.3780158  0.13387773\n#&gt; [4,] -0.5434321 -0.1673186  0.8177779  0.08902432\n\n# ç‰¹å¾å€¼çš„å’Œ\nsum(y$values)\n#&gt; [1] 196\n\n# è¿¹\nsum(diag(AA_T))\n#&gt; [1] 196\n\n\n\n1.1.5 å¥‡å¼‚å€¼åˆ†è§£ä¸ä¸»æˆåˆ†æ¨å¯¼\nSVDå…¬å¼ï¼š\n\\[\nA_{pÃ—n}=U\\Sigma V^T\n\\]\nA æ˜¯ä¸­å¿ƒåŒ–åçš„æ•°æ®çŸ©é˜µï¼Œ U æ˜¯å·¦å¥‡å¼‚çŸ©é˜µï¼Œ \\(\\Sigma\\) æ˜¯å¥‡å¼‚å€¼å¯¹è§’çŸ©é˜µï¼Œ V æ˜¯å³å¥‡å¼‚çŸ©é˜µï¼ˆï¼ˆä¹Ÿæ˜¯ä¸»æˆåˆ†æ–¹å‘ï¼‰ï¼‰\nä¸»æˆåˆ†æ¨å¯¼ï¼š\n\\[ PC=A\\cdot V=U \\Sigma  \\]\nåœ¨è¿™ä¸ªå…¬å¼ä¸­ï¼š\n\n\\(U\\) æ˜¯åŒ…å«å·¦å¥‡å¼‚å‘é‡çš„çŸ©é˜µï¼Œè¡¨ç¤ºæ ·æœ¬åœ¨æ–°åæ ‡ç³»ä¸­çš„åæ ‡ã€‚\n\\(\\Sigma\\) æ˜¯åŒ…å«å¥‡å¼‚å€¼çš„å¯¹è§’çŸ©é˜µï¼Œè¿™äº›å¥‡å¼‚å€¼ä¸ç‰¹å¾å€¼ç›¸å…³ï¼Œè¡¨ç¤ºæ¯ä¸ªä¸»æˆåˆ†çš„æ–¹å·®å¤§å°ã€‚\n\n\nCode\n# è¿›è¡Œå¥‡å¼‚å€¼åˆ†è§£\n\nsvd &lt;- svd(df_center)\nsvd$d\n#&gt; [1] 11.024148  6.964086  4.179904  2.915146\nsvd$u\n#&gt;               [,1]        [,2]         [,3]          [,4]\n#&gt;  [1,] -0.088502119 -0.16111249  0.105218608  0.0530665011\n#&gt;  [2,] -0.175119011 -0.15255799 -0.483145153 -0.1489378244\n#&gt;  [3,] -0.158329049  0.10603826 -0.012974042 -0.2834384049\n#&gt;  [4,]  0.012699298 -0.15917987 -0.027135114 -0.0620804497\n#&gt;  [5,] -0.226649068  0.21932910 -0.141759482 -0.1161380178\n#&gt;  [6,] -0.136005136  0.14038162 -0.259336498  0.0004974586\n#&gt;  [7,]  0.122004202  0.15479183  0.152346210 -0.0402308322\n#&gt;  [8,] -0.004284214  0.04624999  0.170197773 -0.2995093258\n#&gt;  [9,] -0.270566006 -0.00557636  0.136613685 -0.0326971796\n#&gt; [10,] -0.147204794 -0.18180252  0.081106695  0.3656676469\n#&gt; [11,]  0.081955040  0.22324195 -0.012026954  0.3065826885\n#&gt; [12,]  0.147251202 -0.02998994 -0.061530174 -0.1694899353\n#&gt; [13,] -0.123823808  0.09692418  0.160455000 -0.0414370084\n#&gt; [14,]  0.045389560  0.02154472 -0.054011475  0.1442115222\n#&gt; [15,]  0.202373535  0.01479136 -0.038974667  0.0059617844\n#&gt; [16,]  0.071558552  0.03840409 -0.006051930  0.0701237800\n#&gt; [17,]  0.067425851 -0.13624293  0.006718884  0.2277132300\n#&gt; [18,] -0.140517959 -0.12382100  0.185555941  0.1544203417\n#&gt; [19,]  0.215231159 -0.05350432  0.015555919 -0.1122203023\n#&gt; [20,] -0.158347534 -0.06079147  0.037242408 -0.1898534932\n#&gt; [21,]  0.043656895  0.20960067  0.144350623 -0.0609897144\n#&gt; [22,] -0.189334383  0.02208976 -0.091150533  0.0347643443\n#&gt; [23,]  0.151999911  0.08987636 -0.036252509  0.0228600295\n#&gt; [24,] -0.089483486 -0.34027971  0.175449706  0.0731840095\n#&gt; [25,] -0.062570302  0.03743606 -0.089392087  0.0766873549\n#&gt; [26,]  0.106451539 -0.07631705 -0.058472149  0.0420214180\n#&gt; [27,]  0.113651981  0.02757065 -0.041582129  0.0053970395\n#&gt; [28,] -0.258115678  0.11025209 -0.275529769  0.1068057898\n#&gt; [29,]  0.214071497  0.00257041 -0.008728664 -0.0112530536\n#&gt; [30,] -0.016304324  0.20604821  0.181049718  0.0826499280\n#&gt; [31,] -0.177802722 -0.02030605 -0.043504824 -0.1153016522\n#&gt; [32,] -0.151092550  0.11701618  0.152302994 -0.0045791345\n#&gt; [33,] -0.100877464 -0.31671218  0.204524432 -0.3240968906\n#&gt; [34,]  0.268696706 -0.08516514 -0.071353148 -0.0862511360\n#&gt; [35,]  0.020291306  0.10550966  0.007374849  0.1609363198\n#&gt; [36,]  0.027997563  0.04091867  0.003625902  0.0035087357\n#&gt; [37,] -0.005309061  0.07696200 -0.222585787 -0.0807475502\n#&gt; [38,]  0.079778211  0.08118230  0.094883089  0.1219329726\n#&gt; [39,]  0.077565243  0.21208574  0.324451737 -0.2083610270\n#&gt; [40,] -0.118598723 -0.27483477  0.071178009 -0.0446445538\n#&gt; [41,]  0.178498756 -0.11703880 -0.092198470 -0.0372092940\n#&gt; [42,] -0.089775081 -0.12228530 -0.044544714  0.2217051038\n#&gt; [43,] -0.121689077  0.05863443  0.116539361  0.2184216921\n#&gt; [44,]  0.049439812  0.20917537 -0.069565218 -0.0279528910\n#&gt; [45,]  0.251561949 -0.19933619 -0.199240942 -0.0492029259\n#&gt; [46,]  0.008650709 -0.02839251 -0.002773945  0.0717790646\n#&gt; [47,]  0.019477550  0.13790380 -0.147991604 -0.0749973365\n#&gt; [48,]  0.189347337 -0.20254292 -0.024814359  0.0447947015\n#&gt; [49,]  0.186754750  0.08689225  0.032888157  0.0625194853\n#&gt; [50,]  0.056521430 -0.04563221  0.056996643 -0.0565930091\nsvd$v\n#&gt;            [,1]       [,2]       [,3]        [,4]\n#&gt; [1,] -0.5358995 -0.4181809  0.3412327  0.64922780\n#&gt; [2,] -0.5831836 -0.1879856  0.2681484 -0.74340748\n#&gt; [3,] -0.2781909  0.8728062  0.3780158  0.13387773\n#&gt; [4,] -0.5434321  0.1673186 -0.8177779  0.08902432\n# å¥‡å¼‚å€¼çš„å¹³æ–¹å’Œ\nsum(svd$d^2)\n#&gt; [1] 196\n\n# å¥‡å¼‚å€¼çš„å¯¹è§’çŸ©é˜µ\nD &lt;- diag(svd$d)\n\n#  df_center  X = U D V'\nX &lt;- svd$u %*% D %*% t(svd$v) \n\n#  D = U' X V\nt(svd$u) %*% X %*% svd$v\n#&gt;               [,1]         [,2]          [,3]          [,4]\n#&gt; [1,]  1.102415e+01 1.110223e-15  8.881784e-16  3.219647e-15\n#&gt; [2,]  2.220446e-16 6.964086e+00  6.661338e-16  1.318390e-15\n#&gt; [3,] -2.220446e-16 4.440892e-16  4.179904e+00 -8.881784e-16\n#&gt; [4,] -4.440892e-16 1.436351e-15 -9.436896e-16  2.915146e+00\n\n\n\n1.1.6 ç»“æœè§£é‡Š\n\n1.1.6.1 ä¸»æˆåˆ†è·è½½ç³»æ•°\n\nCode# ä¸»æˆåˆ†è·è½½ç³»æ•°\nsvd$v\n#&gt;            [,1]       [,2]       [,3]        [,4]\n#&gt; [1,] -0.5358995 -0.4181809  0.3412327  0.64922780\n#&gt; [2,] -0.5831836 -0.1879856  0.2681484 -0.74340748\n#&gt; [3,] -0.2781909  0.8728062  0.3780158  0.13387773\n#&gt; [4,] -0.5434321  0.1673186 -0.8177779  0.08902432\n\n\nåœ¨PCAä¸­ï¼Œå³å¥‡å¼‚å‘é‡çŸ©é˜µğ‘‰ çš„åˆ—å‘é‡ä»£è¡¨æ•°æ®åœ¨æ–°çš„æ­£äº¤åŸºä¸Šçš„æ–¹å‘ï¼Œè¿™äº›åŸºæ˜¯æŒ‰æ•°æ®ä¸­æ–¹å·®æœ€å¤§åŒ–çš„æ–¹å‘æ’åˆ—çš„ã€‚æ¯ä¸ªå‘é‡å°±æ˜¯ä¸€ä¸ªä¸»æˆåˆ†æ–¹å‘ã€‚å…·ä½“æ¥è¯´ï¼ŒçŸ©é˜µ svd$v ä¸­çš„æ¯ä¸€åˆ—éƒ½æ˜¯ä¸€ä¸ªä¸»æˆåˆ†ï¼Œ ä¸”è¿™äº›åˆ—å‘é‡å¯ä»¥çœ‹ä½œæ˜¯åŸå§‹å˜é‡åœ¨æ–°ä¸»æˆåˆ†ç©ºé—´ä¸­çš„çº¿æ€§ç»„åˆç³»æ•°ã€‚\nå› æ­¤ï¼Œsvd$v ä¸­çš„å…ƒç´ è¡¨ç¤ºçš„æ˜¯æ¯ä¸ªåŸå§‹å˜é‡åœ¨å¯¹åº”ä¸»æˆåˆ†ä¸Šçš„è´¡çŒ®ï¼Œå³ä¸»æˆåˆ†è·è½½ç³»æ•°ï¼ˆloadingsï¼‰ã€‚\nä¾‹å¦‚ï¼Œå¦‚æœ V çš„ç¬¬j ä¸ªåˆ—å‘é‡ä¸º \\([v_{1j},v_{2j},...,v_{pj}]\\) ï¼Œè¿™æ„å‘³ç€ç¬¬j ä¸ªä¸»æˆåˆ†å¯ä»¥è¡¨ç¤ºä¸ºåŸå§‹å˜é‡çš„çº¿æ€§ç»„åˆï¼š\n\\[\nPC_j=v_{1j}\\cdot x_1+v_{2j}\\cdot x_2+...+v_{pj}\\cdot x_p\n\\]\nå…¶ä¸­ï¼Œ \\(x_1,x_2,...,x_p\\) æ˜¯åŸå§‹å˜é‡ï¼Œ \\(v_{1j},v_{2j},...,v_{pj}\\) æ˜¯å®ƒä»¬åœ¨ç¬¬j ä¸ªä¸»æˆåˆ†ä¸Šçš„è·è½½ç³»æ•°ã€‚\n\n1.1.6.2 ä¸»æˆåˆ†å¾—åˆ†\nä¸»æˆåˆ†å¾—åˆ† (principal component scores) ä»£è¡¨äº†åŸå§‹æ•°æ®åœ¨æ–°ä¸»æˆåˆ†è½´ä¸Šçš„åæ ‡ã€‚\nå…·ä½“æ¥è¯´ï¼Œä¸»æˆåˆ†å¾—åˆ†å¯ä»¥è¡¨ç¤ºä¸ºï¼š\n\\[\nScores = U\\Sigma\n\\]\n\nCode# å¾—åˆ†\nsvd$u %*% D\n#&gt;              [,1]        [,2]        [,3]         [,4]\n#&gt;  [1,] -0.97566045 -1.12200121  0.43980366  0.154696581\n#&gt;  [2,] -1.93053788 -1.06242692 -2.01950027 -0.434175454\n#&gt;  [3,] -1.74544285  0.73845954 -0.05423025 -0.826264240\n#&gt;  [4,]  0.13999894 -1.10854226 -0.11342217 -0.180973554\n#&gt;  [5,] -2.49861285  1.52742672 -0.59254100 -0.338559240\n#&gt;  [6,] -1.49934074  0.97762966 -1.08400162  0.001450164\n#&gt;  [7,]  1.34499236  1.07798362  0.63679250 -0.117278736\n#&gt;  [8,] -0.04722981  0.32208890  0.71141032 -0.873113315\n#&gt;  [9,] -2.98275967 -0.03883425  0.57103206 -0.095317042\n#&gt; [10,] -1.62280742 -1.26608838  0.33901818  1.065974459\n#&gt; [11,]  0.90348448  1.55467609 -0.05027151  0.893733198\n#&gt; [12,]  1.62331903 -0.20885253 -0.25719021 -0.494087852\n#&gt; [13,] -1.36505197  0.67498834  0.67068647 -0.120794916\n#&gt; [14,]  0.50038122  0.15003926 -0.22576277  0.420397595\n#&gt; [15,]  2.23099579  0.10300828 -0.16291036  0.017379470\n#&gt; [16,]  0.78887206  0.26744941 -0.02529648  0.204421034\n#&gt; [17,]  0.74331256 -0.94880748  0.02808429  0.663817237\n#&gt; [18,] -1.54909076 -0.86230011  0.77560598  0.450157791\n#&gt; [19,]  2.37274014 -0.37260865  0.06502225 -0.327138529\n#&gt; [20,] -1.74564663 -0.42335704  0.15566968 -0.553450589\n#&gt; [21,]  0.48128007  1.45967706  0.60337172 -0.177793902\n#&gt; [22,] -2.08725025  0.15383500 -0.38100046  0.101343128\n#&gt; [23,]  1.67566951  0.62590670 -0.15153200  0.066640316\n#&gt; [24,] -0.98647919 -2.36973712  0.73336290  0.213342049\n#&gt; [25,] -0.68978426  0.26070794 -0.37365033  0.223554811\n#&gt; [26,]  1.17353751 -0.53147851 -0.24440796  0.122498555\n#&gt; [27,]  1.25291625  0.19200440 -0.17380930  0.015733156\n#&gt; [28,] -2.84550542  0.76780502 -1.15168793  0.311354436\n#&gt; [29,]  2.35995585  0.01790055 -0.03648498 -0.032804291\n#&gt; [30,] -0.17974128  1.43493745  0.75677041  0.240936580\n#&gt; [31,] -1.96012351 -0.14141308 -0.18184598 -0.336121113\n#&gt; [32,] -1.66566662  0.81491072  0.63661186 -0.013348844\n#&gt; [33,] -1.11208808 -2.20561081  0.85489245 -0.944789648\n#&gt; [34,]  2.96215223 -0.59309738 -0.29824930 -0.251434626\n#&gt; [35,]  0.22369436  0.73477837  0.03082616  0.469152817\n#&gt; [36,]  0.30864928  0.28496113  0.01515592  0.010228476\n#&gt; [37,] -0.05852787  0.53596999 -0.93038718 -0.235390872\n#&gt; [38,]  0.87948680  0.56536050  0.39660218  0.355452378\n#&gt; [39,]  0.85509072  1.47698328  1.35617705 -0.607402746\n#&gt; [40,] -1.30744986 -1.91397297  0.29751723 -0.130145378\n#&gt; [41,]  1.96779669 -0.81506822 -0.38538073 -0.108470512\n#&gt; [42,] -0.98969377 -0.85160534 -0.18619262  0.646302674\n#&gt; [43,] -1.34151838  0.40833518  0.48712332  0.636731051\n#&gt; [44,]  0.54503180  1.45671524 -0.29077592 -0.081486749\n#&gt; [45,]  2.77325613 -1.38819435 -0.83280797 -0.143433697\n#&gt; [46,]  0.09536670 -0.19772785 -0.01159482  0.209246429\n#&gt; [47,]  0.21472339  0.96037394 -0.61859067 -0.218628161\n#&gt; [48,]  2.08739306 -1.41052627 -0.10372163  0.130583080\n#&gt; [49,]  2.05881199  0.60512507  0.13746933  0.182253407\n#&gt; [50,]  0.62310061 -0.31778662  0.23824049 -0.164976866\n\n\n\n1.1.6.3 ä¸»æˆåˆ†æ ‡å‡†å·®\n\n\\[\nStandard \\ Deviation \\ of \\ PC_i =\\frac{\\sigma_i}{\\sqrt{n-1}}\n\\]\nå…¶ä¸­ï¼Œn æ˜¯æ ·æœ¬é‡ï¼ŒÏƒ æ˜¯å¥‡å¼‚å€¼ã€‚\n\nCodesvd$d /sqrt(nrow(df_center)-1)\n#&gt; [1] 1.5748783 0.9948694 0.5971291 0.4164494\n\n\n\n1.1.6.4 æ–¹å·®è´¡çŒ®ç™¾åˆ†æ¯”\nåœ¨ä¸»æˆåˆ†åˆ†æ (PCA) ä¸­ï¼Œæ–¹å·®è´¡çŒ®ç™¾åˆ†æ¯”ï¼ˆvariance explained ratioï¼‰æ˜¯ç”¨æ¥è¡¡é‡æ¯ä¸ªä¸»æˆåˆ†è§£é‡Šäº†æ•°æ®æ€»æ–¹å·®çš„æ¯”ä¾‹ã€‚è¿™ä¸ªæ¯”ä¾‹å¯ä»¥é€šè¿‡å¥‡å¼‚å€¼åˆ†è§£ (SVD) çš„å¥‡å¼‚å€¼æ¥è®¡ç®—ã€‚\nå…·ä½“æ¥è¯´ï¼Œæ–¹å·®è´¡çŒ®ç™¾åˆ†æ¯”çš„è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹ï¼š\n\nè®¡ç®—æ¯ä¸ªä¸»æˆåˆ†çš„æ–¹å·®ã€‚å¥‡å¼‚å€¼çš„å¹³æ–¹ \\(\\sigma_i^2\\) è¡¨ç¤ºä¸»æˆåˆ† \\(ğ‘–\\)çš„æ–¹å·®ã€‚\nè®¡ç®—æ€»æ–¹å·®ï¼Œå³æ‰€æœ‰å¥‡å¼‚å€¼çš„å¹³æ–¹å’Œã€‚\næ¯ä¸ªä¸»æˆåˆ†çš„æ–¹å·®è´¡çŒ®ç™¾åˆ†æ¯”å¯ä»¥é€šè¿‡å°†æ¯ä¸ªå¥‡å¼‚å€¼çš„å¹³æ–¹é™¤ä»¥æ€»æ–¹å·®æ¥è®¡ç®—ã€‚\n\n\nCode# æ–¹å·®è´¡çŒ®ç™¾åˆ†æ¯”\npct &lt;- svd$d^2/sum(svd$d^2)\npct\n#&gt; [1] 0.62006039 0.24744129 0.08914080 0.04335752\n\n# ç´¯è®¡æ–¹å·®è´¡çŒ®ç™¾åˆ†æ¯”\ncumsum(pct)\n#&gt; [1] 0.6200604 0.8675017 0.9566425 1.0000000",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "æ— ç›‘ç£å­¦ä¹ ï¼ˆUnsupervised Learningï¼‰",
      "ä¸»æˆåˆ†åˆ†æ"
    ]
  },
  {
    "objectID": "PCA.html#å†…ç½®pca",
    "href": "PCA.html#å†…ç½®pca",
    "title": "",
    "section": "\n1.2 å†…ç½®PCA",
    "text": "1.2 å†…ç½®PCA\n\nCodepca &lt;- prcomp(df_center)\npca\n#&gt; Standard deviations (1, .., p=4):\n#&gt; [1] 1.5748783 0.9948694 0.5971291 0.4164494\n#&gt; \n#&gt; Rotation (n x k) = (4 x 4):\n#&gt;                 PC1        PC2        PC3         PC4\n#&gt; Murder   -0.5358995 -0.4181809  0.3412327  0.64922780\n#&gt; Assault  -0.5831836 -0.1879856  0.2681484 -0.74340748\n#&gt; UrbanPop -0.2781909  0.8728062  0.3780158  0.13387773\n#&gt; Rape     -0.5434321  0.1673186 -0.8177779  0.08902432\nsummary(pca)\n#&gt; Importance of components:\n#&gt;                           PC1    PC2     PC3     PC4\n#&gt; Standard deviation     1.5749 0.9949 0.59713 0.41645\n#&gt; Proportion of Variance 0.6201 0.2474 0.08914 0.04336\n#&gt; Cumulative Proportion  0.6201 0.8675 0.95664 1.00000",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "æ— ç›‘ç£å­¦ä¹ ï¼ˆUnsupervised Learningï¼‰",
      "ä¸»æˆåˆ†åˆ†æ"
    ]
  },
  {
    "objectID": "PCA.html#å¯è§†åŒ–åˆ†æ",
    "href": "PCA.html#å¯è§†åŒ–åˆ†æ",
    "title": "",
    "section": "\n1.3 å¯è§†åŒ–åˆ†æ",
    "text": "1.3 å¯è§†åŒ–åˆ†æ\n\n1.3.1 åˆ¤æ–­ä¸»æˆåˆ†çš„ä¸ªæ•°\n\nCattellç¢çŸ³å›¾ å›¾å½¢å˜åŒ–æœ€å¤§å¤„ï¼Œå³æ‹è§’å¤„\nKaiser-Harriså‡†åˆ™ ç‰¹å¾å€¼å¤§äº1ï¼Œç›´çº¿y=1ä»¥ä¸Š\nå¹³è¡Œåˆ†æ åŸºäºçœŸå®æ•°æ®çš„ç‰¹å¾å€¼å¤§äºä¸€ç»„éšæœºæ•°æ®çŸ©é˜µç›¸åº”çš„ç‰¹å¾å€¼ï¼ˆè™šçº¿ï¼‰\n\n1.3.2 ç¢çŸ³å›¾\n\nCode# Create Scree Plot\nscreeplot(pca, type = \"lines\", main = \"Scree Plot\")\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\nexplained_variance &lt;- pca$sdev^2 / sum(pca$sdev^2)\nexplained_variance_df &lt;- data.frame(\n  Principal_Component = paste0(\"PC\", 1:length(explained_variance)),\n  Explained_Variance = explained_variance\n)\n\nggplot(explained_variance_df, aes(x = Principal_Component, y = Explained_Variance)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  geom_line(aes(group = 1), color = \"blue\") +\n  geom_point(color = \"red\") +\n  labs(title = \"Scree Plot\", x = \"Principal Component\", y = \"Explained Variance\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n1.3.2.1 å¹³è¡Œåˆ†æ\n\nCodefa_parallel &lt;- psych::fa.parallel(df_center, fa = \"pc\", n.iter = 100)\n\n\n\n\n\n\n#&gt; Parallel analysis suggests that the number of factors =  NA  and the number of components =  1\n\n\nCodesvd$v\n#&gt;            [,1]       [,2]       [,3]        [,4]\n#&gt; [1,] -0.5358995 -0.4181809  0.3412327  0.64922780\n#&gt; [2,] -0.5831836 -0.1879856  0.2681484 -0.74340748\n#&gt; [3,] -0.2781909  0.8728062  0.3780158  0.13387773\n#&gt; [4,] -0.5434321  0.1673186 -0.8177779  0.08902432\ntibble(x = 1:4, pc1 = svd$v[, 1]) %&gt;%\n    ggplot(aes(x, pc1)) +\n    geom_point() +\n    theme_classic() +\n    theme(panel.border = element_rect(\n        color = \"black\",\n        fill = NA,\n    ), # æ·»åŠ å››å‘¨æ¡†çº¿\n    )\n\n\n\n\n\n\nCode\nplot(svd$v[,1],ylab = \"1st PC\")\n\n\n\n\n\n\nCodeplot(svd$v[,1],svd$v[,2],xlab=\"lst PC\",ylab=\"2nd PC\")",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "æ— ç›‘ç£å­¦ä¹ ï¼ˆUnsupervised Learningï¼‰",
      "ä¸»æˆåˆ†åˆ†æ"
    ]
  },
  {
    "objectID": "PCA.html#tidy-ä¸»æˆåˆ†åˆ†æ",
    "href": "PCA.html#tidy-ä¸»æˆåˆ†åˆ†æ",
    "title": "",
    "section": "\n1.4 tidy ä¸»æˆåˆ†åˆ†æ",
    "text": "1.4 tidy ä¸»æˆåˆ†åˆ†æ\n\nCodelibrary(tidymodels)\n\n\n\nCodedf &lt;- as_tibble(USArrests, rownames = \"state\")\ndf\n#&gt; # A tibble: 50 Ã— 5\n#&gt;    state       Murder Assault UrbanPop  Rape\n#&gt;    &lt;chr&gt;        &lt;dbl&gt;   &lt;int&gt;    &lt;int&gt; &lt;dbl&gt;\n#&gt;  1 Alabama       13.2     236       58  21.2\n#&gt;  2 Alaska        10       263       48  44.5\n#&gt;  3 Arizona        8.1     294       80  31  \n#&gt;  4 Arkansas       8.8     190       50  19.5\n#&gt;  5 California     9       276       91  40.6\n#&gt;  6 Colorado       7.9     204       78  38.7\n#&gt;  7 Connecticut    3.3     110       77  11.1\n#&gt;  8 Delaware       5.9     238       72  15.8\n#&gt;  9 Florida       15.4     335       80  31.9\n#&gt; 10 Georgia       17.4     211       60  25.8\n#&gt; # â„¹ 40 more rows\n\ndf |&gt;\n  select(-state) |&gt;\n  map_dfr(mean)  #apply(.,2,mean)\n#&gt; # A tibble: 1 Ã— 4\n#&gt;   Murder Assault UrbanPop  Rape\n#&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1   7.79    171.     65.5  21.2\n\n\n\nCodedf_pca &lt;- df |&gt;\n  select(-state) |&gt;\n  stats::prcomp(scale = TRUE)\n\n\nä¸»æˆåˆ†å¾—åˆ†ï¼Œè¡¨ç¤ºä¸»æˆåˆ†ä¸åŸæœ‰è§‚æµ‹çš„ç›¸å…³ç³»æ•°\n\nCodedf_pca$x\n#&gt;               PC1         PC2         PC3          PC4\n#&gt;  [1,] -0.97566045 -1.12200121  0.43980366  0.154696581\n#&gt;  [2,] -1.93053788 -1.06242692 -2.01950027 -0.434175454\n#&gt;  [3,] -1.74544285  0.73845954 -0.05423025 -0.826264240\n#&gt;  [4,]  0.13999894 -1.10854226 -0.11342217 -0.180973554\n#&gt;  [5,] -2.49861285  1.52742672 -0.59254100 -0.338559240\n#&gt;  [6,] -1.49934074  0.97762966 -1.08400162  0.001450164\n#&gt;  [7,]  1.34499236  1.07798362  0.63679250 -0.117278736\n#&gt;  [8,] -0.04722981  0.32208890  0.71141032 -0.873113315\n#&gt;  [9,] -2.98275967 -0.03883425  0.57103206 -0.095317042\n#&gt; [10,] -1.62280742 -1.26608838  0.33901818  1.065974459\n#&gt; [11,]  0.90348448  1.55467609 -0.05027151  0.893733198\n#&gt; [12,]  1.62331903 -0.20885253 -0.25719021 -0.494087852\n#&gt; [13,] -1.36505197  0.67498834  0.67068647 -0.120794916\n#&gt; [14,]  0.50038122  0.15003926 -0.22576277  0.420397595\n#&gt; [15,]  2.23099579  0.10300828 -0.16291036  0.017379470\n#&gt; [16,]  0.78887206  0.26744941 -0.02529648  0.204421034\n#&gt; [17,]  0.74331256 -0.94880748  0.02808429  0.663817237\n#&gt; [18,] -1.54909076 -0.86230011  0.77560598  0.450157791\n#&gt; [19,]  2.37274014 -0.37260865  0.06502225 -0.327138529\n#&gt; [20,] -1.74564663 -0.42335704  0.15566968 -0.553450589\n#&gt; [21,]  0.48128007  1.45967706  0.60337172 -0.177793902\n#&gt; [22,] -2.08725025  0.15383500 -0.38100046  0.101343128\n#&gt; [23,]  1.67566951  0.62590670 -0.15153200  0.066640316\n#&gt; [24,] -0.98647919 -2.36973712  0.73336290  0.213342049\n#&gt; [25,] -0.68978426  0.26070794 -0.37365033  0.223554811\n#&gt; [26,]  1.17353751 -0.53147851 -0.24440796  0.122498555\n#&gt; [27,]  1.25291625  0.19200440 -0.17380930  0.015733156\n#&gt; [28,] -2.84550542  0.76780502 -1.15168793  0.311354436\n#&gt; [29,]  2.35995585  0.01790055 -0.03648498 -0.032804291\n#&gt; [30,] -0.17974128  1.43493745  0.75677041  0.240936580\n#&gt; [31,] -1.96012351 -0.14141308 -0.18184598 -0.336121113\n#&gt; [32,] -1.66566662  0.81491072  0.63661186 -0.013348844\n#&gt; [33,] -1.11208808 -2.20561081  0.85489245 -0.944789648\n#&gt; [34,]  2.96215223 -0.59309738 -0.29824930 -0.251434626\n#&gt; [35,]  0.22369436  0.73477837  0.03082616  0.469152817\n#&gt; [36,]  0.30864928  0.28496113  0.01515592  0.010228476\n#&gt; [37,] -0.05852787  0.53596999 -0.93038718 -0.235390872\n#&gt; [38,]  0.87948680  0.56536050  0.39660218  0.355452378\n#&gt; [39,]  0.85509072  1.47698328  1.35617705 -0.607402746\n#&gt; [40,] -1.30744986 -1.91397297  0.29751723 -0.130145378\n#&gt; [41,]  1.96779669 -0.81506822 -0.38538073 -0.108470512\n#&gt; [42,] -0.98969377 -0.85160534 -0.18619262  0.646302674\n#&gt; [43,] -1.34151838  0.40833518  0.48712332  0.636731051\n#&gt; [44,]  0.54503180  1.45671524 -0.29077592 -0.081486749\n#&gt; [45,]  2.77325613 -1.38819435 -0.83280797 -0.143433697\n#&gt; [46,]  0.09536670 -0.19772785 -0.01159482  0.209246429\n#&gt; [47,]  0.21472339  0.96037394 -0.61859067 -0.218628161\n#&gt; [48,]  2.08739306 -1.41052627 -0.10372163  0.130583080\n#&gt; [49,]  2.05881199  0.60512507  0.13746933  0.182253407\n#&gt; [50,]  0.62310061 -0.31778662  0.23824049 -0.164976866\n\n# by default    df_pca$x\nbroom::tidy(df_pca, matrix = \"scores\") |&gt; \n    pivot_wider(id_cols = everything(),\n                names_from = PC,\n                names_prefix = \"PC\",\n               values_from = value)\n#&gt; # A tibble: 50 Ã— 5\n#&gt;      row     PC1     PC2     PC3      PC4\n#&gt;    &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1     1 -0.976  -1.12    0.440   0.155  \n#&gt;  2     2 -1.93   -1.06   -2.02   -0.434  \n#&gt;  3     3 -1.75    0.738  -0.0542 -0.826  \n#&gt;  4     4  0.140  -1.11   -0.113  -0.181  \n#&gt;  5     5 -2.50    1.53   -0.593  -0.339  \n#&gt;  6     6 -1.50    0.978  -1.08    0.00145\n#&gt;  7     7  1.34    1.08    0.637  -0.117  \n#&gt;  8     8 -0.0472  0.322   0.711  -0.873  \n#&gt;  9     9 -2.98   -0.0388  0.571  -0.0953 \n#&gt; 10    10 -1.62   -1.27    0.339   1.07   \n#&gt; # â„¹ 40 more rows\n\n\nä¸»æˆåˆ†è·è½½ï¼ˆloadingï¼‰ï¼šè¡¨ç¤ºä¸»æˆåˆ†ä¸åŸæœ‰å˜é‡çš„ç›¸å…³ç³»æ•°\n\nCodedf_pca$rotation\n#&gt;                 PC1        PC2        PC3         PC4\n#&gt; Murder   -0.5358995 -0.4181809  0.3412327  0.64922780\n#&gt; Assault  -0.5831836 -0.1879856  0.2681484 -0.74340748\n#&gt; UrbanPop -0.2781909  0.8728062  0.3780158  0.13387773\n#&gt; Rape     -0.5434321  0.1673186 -0.8177779  0.08902432\n\n# df_pca$Rotation\ntidy(df_pca, matrix = \"loadings\") |&gt; \n    pivot_wider(\n        names_from = PC,\n        names_prefix = \"PC\",\n        values_from = value,\n    )\n#&gt; # A tibble: 4 Ã— 5\n#&gt;   column      PC1    PC2    PC3     PC4\n#&gt;   &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 Murder   -0.536 -0.418  0.341  0.649 \n#&gt; 2 Assault  -0.583 -0.188  0.268 -0.743 \n#&gt; 3 UrbanPop -0.278  0.873  0.378  0.134 \n#&gt; 4 Rape     -0.543  0.167 -0.818  0.0890\n\n\nä¾‹å¦‚ï¼š\n\\[\nPC_1=-0.536Murrder-0.583Assault-0.278UrbanPop-0.543Rape\n\\]\n\nCode\ntidy(df_pca, matrix = \"loadings\") |&gt;\n  ggplot(aes(value, column)) +\n  facet_wrap(~ PC) +\n  geom_col() +\n  scale_x_continuous(labels = scales::percent)\n\n\n\n\n\n\n\nç‰¹å¾å€¼ eigenvaluesï¼Œé«˜ç»´æ¤­çƒçš„ä¸»è½´é•¿åº¦ï¼Œç›¸å…³çŸ©é˜µçš„ç‰¹å¾å€¼ã€‚\næ–¹å·®ç™¾åˆ†æ¯”è´¡çŒ®ã€‚\n\nCode# screen plot\ntidy(df_pca, matrix = \"eigenvalues\") |&gt;\n    ggplot(aes(PC, percent)) +\n    geom_point(color = \"red\") +\n    geom_line()+\n    scale_y_continuous(labels = scales::percent)",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "æ— ç›‘ç£å­¦ä¹ ï¼ˆUnsupervised Learningï¼‰",
      "ä¸»æˆåˆ†åˆ†æ"
    ]
  },
  {
    "objectID": "LDA.html",
    "href": "LDA.html",
    "title": "",
    "section": "",
    "text": "æ— ç›‘ç£å­¦ä¹ ï¼ˆUnsupervised Learningï¼‰çº¿æ€§åˆ¤åˆ«åˆ†æ CodeShow All CodeHide All CodeView Source\n1 çº¿æ€§åˆ¤åˆ«åˆ†æ\nFisher çº¿æ€§åˆ¤åˆ«åˆ†æï¼ˆLinear Discriminant Analysis, LDAï¼‰ï¼šç”¨äºåˆ†ç±»ä»»åŠ¡çš„é™ç»´æŠ€æœ¯ã€‚\nFisheråˆ¤åˆ«æ³•è¯•å›¾æœ€å¤§åŒ–ç±»é—´å·®å¼‚ï¼ˆä¸åŒç±»åˆ«çš„æ•°æ®ç‚¹å½¼æ­¤è¿œç¦»ï¼‰å¹¶æœ€å°åŒ–ç±»å†…å·®å¼‚ï¼ˆåŒä¸€ç±»åˆ«çš„æ•°æ®ç‚¹å°½å¯èƒ½èšé›†ã€‚\nå®ƒä¾§é‡äºæœ€å¤§åŒ–ç±»é—´å·®å¼‚ï¼ˆbetween-class varianceï¼‰ä¸ç±»å†…å·®å¼‚ï¼ˆwithin-class varianceï¼‰çš„æ¯”ç‡\n\n1.0.1 MASS\n\nCode# åŠ è½½MASSåŒ…ï¼Œå®ƒåŒ…å«äº†ldaå‡½æ•°\nlibrary(MASS)\n\n# åŠ è½½å†…ç½®çš„é¸¢å°¾èŠ±æ•°æ®é›†\ndata(iris)\n\n# æŸ¥çœ‹æ•°æ®é›†ç»“æ„\nstr(iris)\n#&gt; 'data.frame':    150 obs. of  5 variables:\n#&gt;  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n#&gt;  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n#&gt;  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n#&gt;  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n#&gt;  $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n# åº”ç”¨Fisherçº¿æ€§åˆ¤åˆ«åˆ†æ\n# ä½¿ç”¨é¸¢å°¾èŠ±æ•°æ®é›†çš„å‰å››åˆ—ä½œä¸ºç‰¹å¾ï¼ŒSpeciesä½œä¸ºç±»åˆ«\nlda_model &lt;- lda(Species ~ ., data=iris)\n\n# æŸ¥çœ‹åˆ¤åˆ«æ¨¡å‹çš„æ‘˜è¦\nsummary(lda_model)\n#&gt;         Length Class  Mode     \n#&gt; prior    3     -none- numeric  \n#&gt; counts   3     -none- numeric  \n#&gt; means   12     -none- numeric  \n#&gt; scaling  8     -none- numeric  \n#&gt; lev      3     -none- character\n#&gt; svd      2     -none- numeric  \n#&gt; N        1     -none- numeric  \n#&gt; call     3     -none- call     \n#&gt; terms    3     terms  call     \n#&gt; xlevels  0     -none- list\nlda_model\n#&gt; Call:\n#&gt; lda(Species ~ ., data = iris)\n#&gt; \n#&gt; Prior probabilities of groups:\n#&gt;     setosa versicolor  virginica \n#&gt;  0.3333333  0.3333333  0.3333333 \n#&gt; \n#&gt; Group means:\n#&gt;            Sepal.Length Sepal.Width Petal.Length Petal.Width\n#&gt; setosa            5.006       3.428        1.462       0.246\n#&gt; versicolor        5.936       2.770        4.260       1.326\n#&gt; virginica         6.588       2.974        5.552       2.026\n#&gt; \n#&gt; Coefficients of linear discriminants:\n#&gt;                     LD1         LD2\n#&gt; Sepal.Length  0.8293776 -0.02410215\n#&gt; Sepal.Width   1.5344731 -2.16452123\n#&gt; Petal.Length -2.2012117  0.93192121\n#&gt; Petal.Width  -2.8104603 -2.83918785\n#&gt; \n#&gt; Proportion of trace:\n#&gt;    LD1    LD2 \n#&gt; 0.9912 0.0088\n# æ‰“å°åˆ¤åˆ«å‡½æ•°çš„ç³»æ•°\nprint(lda_model$coefficients)\n#&gt; NULL\n\n# ä½¿ç”¨åˆ¤åˆ«æ¨¡å‹å¯¹æ•°æ®è¿›è¡Œåˆ†ç±»\npredicted_species &lt;- predict(lda_model, iris)\n\n# è®¡ç®—å‡†ç¡®ç‡\naccuracy &lt;- sum(predicted_species$class == iris$Species) / nrow(iris)\nprint(paste(\"åˆ†ç±»å‡†ç¡®ç‡:\", accuracy))\n#&gt; [1] \"åˆ†ç±»å‡†ç¡®ç‡: 0.98\"\n\n# å¯è§†åŒ–åˆ¤åˆ«ç»“æœ\nplot(lda_model)\n\n\n\n\n\n\n\n\n\næ¨¡å‹è°ƒç”¨ï¼ˆCallï¼‰:\n\næ˜¾ç¤ºäº†åˆ›å»ºLDAæ¨¡å‹æ—¶ä½¿ç”¨çš„å‡½æ•°è°ƒç”¨ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæ¨¡å‹ä½¿ç”¨é¸¢å°¾èŠ±æ•°æ®é›†çš„æ‰€æœ‰ç‰¹å¾ï¼ˆSepal.Length, Sepal.Width, Petal.Length, Petal.Widthï¼‰æ¥é¢„æµ‹ç‰©ç§ï¼ˆSpeciesï¼‰ã€‚\n\n\n\nç»„çš„å…ˆéªŒæ¦‚ç‡ï¼ˆPrior probabilities of groupsï¼‰:\n\næ˜¾ç¤ºäº†æ¯ä¸ªç‰©ç§ï¼ˆsetosa, versicolor, virginicaï¼‰çš„å…ˆéªŒæ¦‚ç‡ã€‚è¿™é‡Œæ¯ä¸ªç‰©ç§çš„å…ˆéªŒæ¦‚ç‡éƒ½æ˜¯0.3333ï¼Œæ„å‘³ç€åœ¨æ²¡æœ‰ä»»ä½•é¢å¤–ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œæ¯ä¸ªç‰©ç§å‡ºç°çš„æ¦‚ç‡æ˜¯ç›¸åŒçš„ã€‚\n\n\n\nç»„å†…å‡å€¼ï¼ˆGroup meansï¼‰:\n\næ˜¾ç¤ºäº†æ¯ä¸ªç‰©ç§åœ¨å„ä¸ªç‰¹å¾ä¸Šçš„å‡å€¼ã€‚ä¾‹å¦‚ï¼Œsetosaç‰©ç§çš„èŠ±è¼é•¿åº¦ï¼ˆSepal.Lengthï¼‰å‡å€¼æ˜¯5.006ï¼ŒèŠ±è¼å®½åº¦ï¼ˆSepal.Widthï¼‰å‡å€¼æ˜¯3.428ï¼ŒèŠ±ç“£é•¿åº¦ï¼ˆPetal.Lengthï¼‰å‡å€¼æ˜¯1.462ï¼ŒèŠ±ç“£å®½åº¦ï¼ˆPetal.Widthï¼‰å‡å€¼æ˜¯0.246ã€‚\n\n\n\nçº¿æ€§åˆ¤åˆ«ç³»æ•°ï¼ˆCoefficients of linear discriminantsï¼‰:\n\næ˜¾ç¤ºäº†ä¸¤ä¸ªçº¿æ€§åˆ¤åˆ«å‡½æ•°ï¼ˆLD1å’ŒLD2ï¼‰çš„ç³»æ•°ã€‚è¿™äº›ç³»æ•°ç”¨äºè®¡ç®—åˆ¤åˆ«åˆ†æ•°ï¼Œä»¥åŒºåˆ†ä¸åŒçš„ç‰©ç§ã€‚ä¾‹å¦‚ï¼ŒLD1åˆ¤åˆ«å‡½æ•°ä¸­ï¼ŒèŠ±è¼é•¿åº¦ï¼ˆSepal.Lengthï¼‰çš„ç³»æ•°æ˜¯0.8293776ï¼ŒèŠ±è¼å®½åº¦ï¼ˆSepal.Widthï¼‰çš„ç³»æ•°æ˜¯1.5344731ï¼Œä»¥æ­¤ç±»æ¨ã€‚\n\n\n\nç‰¹å¾å€¼çš„æ¯”ä¾‹ï¼ˆProportion of traceï¼‰:\n\næ˜¾ç¤ºäº†æ¯ä¸ªçº¿æ€§åˆ¤åˆ«å‡½æ•°å¯¹æ€»æ–¹å·®çš„è§£é‡Šæ¯”ä¾‹ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼ŒLD1è§£é‡Šäº†99.12%çš„æ–¹å·®ï¼Œè€ŒLD2ä»…è§£é‡Šäº†0.88%çš„æ–¹å·®ã€‚è¿™è¡¨æ˜LD1æ˜¯ä¸»è¦çš„åˆ¤åˆ«æ–¹å‘ï¼Œè€ŒLD2çš„è´¡çŒ®ç›¸å¯¹è¾ƒå°ã€‚\n\n\n\nå¦‚ä½•ä½¿ç”¨è¿™äº›ä¿¡æ¯ï¼š\n\nå¯ä»¥ä½¿ç”¨è¿™äº›ç³»æ•°æ¥è®¡ç®—æ¯ä¸ªè§‚æµ‹å€¼åœ¨LD1å’ŒLD2ä¸Šçš„åˆ¤åˆ«åˆ†æ•°ã€‚åˆ¤åˆ«åˆ†æ•°çš„è®¡ç®—å…¬å¼ä¸ºï¼š LD1=0.8293776Ã—Sepal.Length+1.5344731Ã—Sepal.Widthâˆ’2.2012117Ã—Petal.Lengthâˆ’2.8104603Ã—Petal.WidthLD1=0.8293776Ã—Sepal.Length+1.5344731Ã—Sepal.Widthâˆ’2.2012117Ã—Petal.Lengthâˆ’2.8104603Ã—Petal.Width LD2=âˆ’0.02410215Ã—Sepal.Lengthâˆ’2.16452123Ã—Sepal.Width+0.93192121Ã—Petal.Lengthâˆ’2.83918785Ã—Petal.WidthLD2=âˆ’0.02410215Ã—Sepal.Lengthâˆ’2.16452123Ã—Sepal.Width+0.93192121Ã—Petal.Lengthâˆ’2.83918785Ã—Petal.Width\né€šå¸¸ï¼Œä¸»è¦çš„åˆ¤åˆ«å‡½æ•°ï¼ˆåœ¨è¿™ä¸ªä¾‹å­ä¸­æ˜¯LD1ï¼‰è¶³ä»¥è¿›è¡Œæœ‰æ•ˆçš„åˆ†ç±»ã€‚å¦‚æœéœ€è¦ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨LD2ä½œä¸ºè¾…åŠ©ã€‚\næ ¹æ®åˆ¤åˆ«åˆ†æ•°ï¼Œå¯ä»¥ç¡®å®šæ¯ä¸ªè§‚æµ‹å€¼æœ€æœ‰å¯èƒ½å±äºçš„ç‰©ç§ç±»åˆ«ã€‚\n\n1.0.2 tidymodels\n\nCodelibrary(tidymodels)\n#&gt; â”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels 1.2.0 â”€â”€\n#&gt; âœ” broom        1.0.6     âœ” rsample      1.2.1\n#&gt; âœ” dials        1.2.1     âœ” tune         1.2.1\n#&gt; âœ” infer        1.0.7     âœ” workflows    1.1.4\n#&gt; âœ” modeldata    1.4.0     âœ” workflowsets 1.1.0\n#&gt; âœ” parsnip      1.2.1     âœ” yardstick    1.3.1\n#&gt; âœ” recipes      1.1.0\nlibrary(discrim)\n\n\n\nCodeSmarket &lt;- read_csv(\"data/Smarket.csv\")\n#&gt; Rows: 1250 Columns: 9\n#&gt; â”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Delimiter: \",\"\n#&gt; chr (1): Direction\n#&gt; dbl (8): Year, Lag1, Lag2, Lag3, Lag4, Lag5, Volume, Today\n#&gt; \n#&gt; â„¹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; â„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nSmarket$Direction &lt;- factor(Smarket$Direction)\nhead(Smarket)\n#&gt; # A tibble: 6 Ã— 9\n#&gt;    Year   Lag1   Lag2   Lag3   Lag4   Lag5 Volume  Today Direction\n#&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;    \n#&gt; 1  2001  0.381 -0.192 -2.62  -1.06   5.01    1.19  0.959 Up       \n#&gt; 2  2001  0.959  0.381 -0.192 -2.62  -1.06    1.30  1.03  Up       \n#&gt; 3  2001  1.03   0.959  0.381 -0.192 -2.62    1.41 -0.623 Down     \n#&gt; 4  2001 -0.623  1.03   0.959  0.381 -0.192   1.28  0.614 Up       \n#&gt; 5  2001  0.614 -0.623  1.03   0.959  0.381   1.21  0.213 Up       \n#&gt; 6  2001  0.213  0.614 -0.623  1.03   0.959   1.35  1.39  Up\n\n\n\nCodelda_spec &lt;- discrim_linear() %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"MASS\")\nlda_fit &lt;- lda_spec %&gt;%\n  fit(Direction ~ Lag1 + Lag2, data = Smarket)\n\nlda_fit\n#&gt; parsnip model object\n#&gt; \n#&gt; Call:\n#&gt; lda(Direction ~ Lag1 + Lag2, data = data)\n#&gt; \n#&gt; Prior probabilities of groups:\n#&gt;   Down     Up \n#&gt; 0.4816 0.5184 \n#&gt; \n#&gt; Group means:\n#&gt;             Lag1        Lag2\n#&gt; Down  0.05068605  0.03229734\n#&gt; Up   -0.03969136 -0.02244444\n#&gt; \n#&gt; Coefficients of linear discriminants:\n#&gt;             LD1\n#&gt; Lag1 -0.7567605\n#&gt; Lag2 -0.4707872\n\n\n\nCodepredict(lda_fit, new_data = Smarket)\n#&gt; # A tibble: 1,250 Ã— 1\n#&gt;    .pred_class\n#&gt;    &lt;fct&gt;      \n#&gt;  1 Up         \n#&gt;  2 Down       \n#&gt;  3 Down       \n#&gt;  4 Up         \n#&gt;  5 Up         \n#&gt;  6 Up         \n#&gt;  7 Down       \n#&gt;  8 Up         \n#&gt;  9 Up         \n#&gt; 10 Down       \n#&gt; # â„¹ 1,240 more rows\npredict(lda_fit, new_data = Smarket, type = \"prob\")\n#&gt; # A tibble: 1,250 Ã— 2\n#&gt;    .pred_Down .pred_Up\n#&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1      0.486    0.514\n#&gt;  2      0.503    0.497\n#&gt;  3      0.510    0.490\n#&gt;  4      0.482    0.518\n#&gt;  5      0.485    0.515\n#&gt;  6      0.492    0.508\n#&gt;  7      0.509    0.491\n#&gt;  8      0.490    0.510\n#&gt;  9      0.477    0.523\n#&gt; 10      0.505    0.495\n#&gt; # â„¹ 1,240 more rows\naugment(lda_fit, new_data = Smarket) %&gt;%\n  conf_mat(truth = Direction, estimate = .pred_class) \n#&gt;           Truth\n#&gt; Prediction Down  Up\n#&gt;       Down  114 102\n#&gt;       Up    488 546\n\naugment(lda_fit, new_data = Smarket) %&gt;%\n  accuracy(truth = Direction, estimate = .pred_class) \n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.528\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "æ— ç›‘ç£å­¦ä¹ ï¼ˆUnsupervised Learningï¼‰",
      "çº¿æ€§åˆ¤åˆ«åˆ†æ"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\næ¨èé˜…è¯»\n\nMachine Learning with R (4E)\nTidy Modeling with R\ntidymodels\nt-Distributed Stochastic Neighbor Embedding (t-SNE)\nUniform Manifold Approximation and Projection (UMAP)\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cross_validation.html",
    "href": "cross_validation.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All CodeView Source",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "äº¤å‰éªŒè¯"
    ]
  },
  {
    "objectID": "cross_validation.html#tidymodels",
    "href": "cross_validation.html#tidymodels",
    "title": "",
    "section": "\n1.1 tidymodels",
    "text": "1.1 tidymodels\n\nCode# å®šä¹‰æ¨¡å‹\nlog_reg &lt;- logistic_reg() %&gt;% \n  set_engine(\"glm\")\n\n# å»ºç«‹å·¥ä½œæµ\nfull_workflow &lt;- workflow() %&gt;%\n  add_model(log_reg) %&gt;%\n  add_formula(default ~ .)\n\n# æ‹Ÿåˆå…¨æ¨¡å‹\nfull_fit &lt;- full_workflow %&gt;% \n  fit(data = train_data)\n\n# é¢„æµ‹å¹¶è¯„ä¼°å…¨æ¨¡å‹\nfull_predictions &lt;- full_fit %&gt;% \n  predict(test_data, type = \"prob\") %&gt;%\n  bind_cols(test_data %&gt;% select(default))\n\n\n# ç»˜åˆ¶å…¨æ¨¡å‹çš„ROCæ›²çº¿\nfull_roc &lt;- roc_curve(full_predictions,\n                      truth = default,\n                      .pred_1,\n                      event_level = \"second\") %&gt;%\n         #ç¬¬äºŒçº§é€»è¾‘å°†ç»“æœç¼–ç ä¸º0/1ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç¬¬äºŒä¸ªå€¼æ˜¯äº‹ä»¶ï¼‰\n    autoplot() + ggtitle(\"ROC Curve - Full Model\")\n\nprint(full_roc)\n\n\n\n\n\n\nCode\n# è®¡ç®—æ ¡å‡†æ›²çº¿\nfull_predictions &lt;- full_predictions %&gt;%\n  mutate(pred_bin = cut(.pred_1, breaks = seq(0, 1, by = 0.1)))\n\ncalibration_data &lt;- full_predictions %&gt;%\n  group_by(pred_bin) %&gt;%\n  dplyr::summarize(mean_pred = mean(.pred_1), \n            mean_actual = mean(default == \"1\"))\n\nggplot(calibration_data, aes(x = mean_pred, y = mean_actual)) +\n  geom_point() +\n  geom_line() +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") +\n  xlim(0, 1) + ylim(0, 1) +\n  ggtitle(\"Calibration Curve - Full Model\") +\n  xlab(\"Predicted Probability\") +\n  ylab(\"Observed Probability\")\n\n\n\n\n\n\n\n\nCode# å®šä¹‰äº¤å‰éªŒè¯\ncv_5 &lt;- vfold_cv(train_data, v = 5)\ncv_10 &lt;- vfold_cv(train_data, v = 10)\n\n# 5æŠ˜äº¤å‰éªŒè¯\ncv_5_results &lt;- fit_resamples(\n  full_workflow,\n  resamples = cv_5,\n  metrics = metric_set(roc_auc),\n  control = control_resamples(save_pred = TRUE)\n)\n\n# 10æŠ˜äº¤å‰éªŒè¯\ncv_10_results &lt;- fit_resamples(\n  full_workflow,\n  resamples = cv_10,\n  metrics = metric_set(roc_auc),\n  control = control_resamples(save_pred = TRUE)\n)\n\n# é¢„æµ‹å’Œè¯„ä¼°5æŠ˜äº¤å‰éªŒè¯æ¨¡å‹\ncv_5_predictions &lt;- collect_predictions(cv_5_results)\n\n# ç»˜åˆ¶5æŠ˜äº¤å‰éªŒè¯æ¨¡å‹çš„ROCæ›²çº¿\ncv_5_roc &lt;- roc_curve(cv_5_predictions, truth = default, .pred_1 , event_level = \"second\") %&gt;% \n  autoplot() + ggtitle(\"ROC Curve - 5-fold Cross-Validation Model\")\n\nprint(cv_5_roc)\n\n\n\n\n\n\nCode\n# è®¡ç®—5æŠ˜äº¤å‰éªŒè¯çš„æ ¡å‡†æ›²çº¿\ncv_5_predictions &lt;- cv_5_predictions %&gt;%\n  mutate(pred_bin = cut(.pred_1, breaks = seq(0, 1, by = 0.1)))\n\ncalibration_data_5 &lt;- cv_5_predictions %&gt;%\n  group_by(pred_bin) %&gt;%\n  dplyr::summarize(mean_pred = mean(.pred_1), \n            mean_actual = mean(default == \"1\"))\n\nggplot(calibration_data_5, aes(x = mean_pred, y = mean_actual)) +\n  geom_point() +\n  geom_line() +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") +\n  xlim(0, 1) + ylim(0, 1) +\n  ggtitle(\"Calibration Curve - 5-fold Cross-Validation Model\") +\n  xlab(\"Predicted Probability\") +\n  ylab(\"Observed Probability\")\n\n\n\n\n\n\nCode\n# é¢„æµ‹å’Œè¯„ä¼°10æŠ˜äº¤å‰éªŒè¯æ¨¡å‹\ncv_10_predictions &lt;- collect_predictions(cv_10_results)\n\n# ç»˜åˆ¶10æŠ˜äº¤å‰éªŒè¯æ¨¡å‹çš„ROCæ›²çº¿\ncv_10_roc &lt;- roc_curve(cv_10_predictions, truth = default, .pred_1,  event_level = \"second\") %&gt;% \n  autoplot() + ggtitle(\"ROC Curve - 10-fold Cross-Validation Model\")\n\nprint(cv_10_roc)\n\n\n\n\n\n\nCode\n# è®¡ç®—10æŠ˜äº¤å‰éªŒè¯çš„æ ¡å‡†æ›²çº¿\ncv_10_predictions &lt;- cv_10_predictions %&gt;%\n  mutate(pred_bin = cut(.pred_1, breaks = seq(0, 1, by = 0.1)))\n\ncalibration_data_10 &lt;- cv_10_predictions %&gt;%\n  group_by(pred_bin) %&gt;%\n  dplyr::summarize(mean_pred = mean(.pred_1), \n            mean_actual = mean(default == \"1\"))\n\nggplot(calibration_data_10, aes(x = mean_pred, y = mean_actual)) +\n  geom_point() +\n  geom_line() +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") +\n  xlim(0, 1) + ylim(0, 1) +\n  ggtitle(\"Calibration Curve - 10-fold Cross-Validation Model\") +\n  xlab(\"Predicted Probability\") +\n  ylab(\"Observed Probability\")",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "äº¤å‰éªŒè¯"
    ]
  },
  {
    "objectID": "Classification.html",
    "href": "Classification.html",
    "title": "",
    "section": "",
    "text": "ç›‘ç£å­¦ä¹ ï¼ˆSupervised Learningï¼‰åˆ†ç±» CodeShow All CodeHide All CodeView Source",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "ç›‘ç£å­¦ä¹ ï¼ˆSupervised Learningï¼‰",
      "åˆ†ç±»"
    ]
  },
  {
    "objectID": "Classification.html#æ”¯æŒå‘é‡æœº",
    "href": "Classification.html#æ”¯æŒå‘é‡æœº",
    "title": "",
    "section": "\n1.1 æ”¯æŒå‘é‡æœº",
    "text": "1.1 æ”¯æŒå‘é‡æœº\næ”¯æŒå‘é‡æœºï¼ˆSupport Vector Machine, SVMï¼‰ï¼šç”¨äºäºŒåˆ†ç±»å’Œå¤šåˆ†ç±»é—®é¢˜ï¼Œå¯»æ‰¾æœ€ä½³å†³ç­–è¾¹ç•Œã€‚",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "ç›‘ç£å­¦ä¹ ï¼ˆSupervised Learningï¼‰",
      "åˆ†ç±»"
    ]
  },
  {
    "objectID": "Classification.html#k-nearest-neighbors",
    "href": "Classification.html#k-nearest-neighbors",
    "title": "",
    "section": "\n1.2 K-Nearest Neighbors",
    "text": "1.2 K-Nearest Neighbors\n\nCodeSmarket &lt;- read_csv(\"data/Smarket.csv\")\n#&gt; Rows: 1250 Columns: 9\n#&gt; â”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Delimiter: \",\"\n#&gt; chr (1): Direction\n#&gt; dbl (8): Year, Lag1, Lag2, Lag3, Lag4, Lag5, Volume, Today\n#&gt; \n#&gt; â„¹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; â„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nSmarket$Direction &lt;- factor(Smarket$Direction)\nhead(Smarket)\n#&gt; # A tibble: 6 Ã— 9\n#&gt;    Year   Lag1   Lag2   Lag3   Lag4   Lag5 Volume  Today Direction\n#&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;    \n#&gt; 1  2001  0.381 -0.192 -2.62  -1.06   5.01    1.19  0.959 Up       \n#&gt; 2  2001  0.959  0.381 -0.192 -2.62  -1.06    1.30  1.03  Up       \n#&gt; 3  2001  1.03   0.959  0.381 -0.192 -2.62    1.41 -0.623 Down     \n#&gt; 4  2001 -0.623  1.03   0.959  0.381 -0.192   1.28  0.614 Up       \n#&gt; 5  2001  0.614 -0.623  1.03   0.959  0.381   1.21  0.213 Up       \n#&gt; 6  2001  0.213  0.614 -0.623  1.03   0.959   1.35  1.39  Up\n\n\n\nCodeknn_spec &lt;- nearest_neighbor(neighbors = 3) |&gt; \n  set_mode(\"classification\") |&gt; \n  set_engine(\"kknn\")\n\nknn_fit &lt;- knn_spec |&gt;\n  fit(Direction ~ Lag1 + Lag2, data = Smarket)\n\nknn_fit\n#&gt; parsnip model object\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; kknn::train.kknn(formula = Direction ~ Lag1 + Lag2, data = data,     ks = min_rows(3, data, 5))\n#&gt; \n#&gt; Type of response variable: nominal\n#&gt; Minimal misclassification: 0.5064\n#&gt; Best kernel: optimal\n#&gt; Best k: 3\n\n\n\nCodeaugment(knn_fit, new_data = Smarket) |&gt; \n  conf_mat(truth = Direction, estimate = .pred_class) \n#&gt;           Truth\n#&gt; Prediction Down  Up\n#&gt;       Down  602   0\n#&gt;       Up      0 648\n\n\n\nCodeaugment(knn_fit, new_data = Smarket) |&gt;\n  accuracy(truth = Direction, estimate = .pred_class) \n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary             1",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "ç›‘ç£å­¦ä¹ ï¼ˆSupervised Learningï¼‰",
      "åˆ†ç±»"
    ]
  },
  {
    "objectID": "Classification.html#lda",
    "href": "Classification.html#lda",
    "title": "",
    "section": "\n1.3 LDA",
    "text": "1.3 LDA\n\nCodelda_spec &lt;- discrim_linear() %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"MASS\")\nlda_fit &lt;- lda_spec %&gt;%\n  fit(Direction ~ Lag1 + Lag2, data = Smarket)\n\nlda_fit\n#&gt; parsnip model object\n#&gt; \n#&gt; Call:\n#&gt; lda(Direction ~ Lag1 + Lag2, data = data)\n#&gt; \n#&gt; Prior probabilities of groups:\n#&gt;   Down     Up \n#&gt; 0.4816 0.5184 \n#&gt; \n#&gt; Group means:\n#&gt;             Lag1        Lag2\n#&gt; Down  0.05068605  0.03229734\n#&gt; Up   -0.03969136 -0.02244444\n#&gt; \n#&gt; Coefficients of linear discriminants:\n#&gt;             LD1\n#&gt; Lag1 -0.7567605\n#&gt; Lag2 -0.4707872",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "ç›‘ç£å­¦ä¹ ï¼ˆSupervised Learningï¼‰",
      "åˆ†ç±»"
    ]
  },
  {
    "objectID": "Classification.html#æ¨¡å‹æ¯”è¾ƒ",
    "href": "Classification.html#æ¨¡å‹æ¯”è¾ƒ",
    "title": "",
    "section": "\n1.4 æ¨¡å‹æ¯”è¾ƒ",
    "text": "1.4 æ¨¡å‹æ¯”è¾ƒ\n\nCodemodels &lt;- list(\"LDA\" = lda_fit,\n               \"KNN\" = knn_fit)\npreds &lt;- imap_dfr(models, augment, \n                  new_data = Smarket, .id = \"model\")\n\npreds %&gt;%\n  dplyr::select(model, Direction, .pred_class, .pred_Down, .pred_Up)\n#&gt; # A tibble: 2,500 Ã— 5\n#&gt;    model Direction .pred_class .pred_Down .pred_Up\n#&gt;    &lt;chr&gt; &lt;fct&gt;     &lt;fct&gt;            &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1 LDA   Up        Up               0.486    0.514\n#&gt;  2 LDA   Up        Down             0.503    0.497\n#&gt;  3 LDA   Down      Down             0.510    0.490\n#&gt;  4 LDA   Up        Up               0.482    0.518\n#&gt;  5 LDA   Up        Up               0.485    0.515\n#&gt;  6 LDA   Up        Up               0.492    0.508\n#&gt;  7 LDA   Down      Down             0.509    0.491\n#&gt;  8 LDA   Up        Up               0.490    0.510\n#&gt;  9 LDA   Up        Up               0.477    0.523\n#&gt; 10 LDA   Up        Down             0.505    0.495\n#&gt; # â„¹ 2,490 more rows\n\n\n\n1.4.1 çµæ•åº¦å’Œç‰¹å¼‚æ€§\n\nCodemulti_metric &lt;- metric_set( sensitivity, specificity)  # accuracy\n\n\npreds %&gt;%\n  group_by(model) %&gt;%\n  multi_metric(truth = Direction, estimate = .pred_class)\n#&gt; # A tibble: 4 Ã— 4\n#&gt;   model .metric     .estimator .estimate\n#&gt;   &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 KNN   sensitivity binary         1    \n#&gt; 2 LDA   sensitivity binary         0.189\n#&gt; 3 KNN   specificity binary         1    \n#&gt; 4 LDA   specificity binary         0.843\n\n\n\n1.4.2 ROC æ›²çº¿\n\nCodepreds %&gt;%\n  group_by(model) %&gt;%\n  roc_curve(Direction, .pred_Down) %&gt;%\n  autoplot()",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "ç›‘ç£å­¦ä¹ ï¼ˆSupervised Learningï¼‰",
      "åˆ†ç±»"
    ]
  },
  {
    "objectID": "Classification.html#æœ´ç´ è´å¶æ–¯",
    "href": "Classification.html#æœ´ç´ è´å¶æ–¯",
    "title": "",
    "section": "\n1.5 æœ´ç´ è´å¶æ–¯\n",
    "text": "1.5 æœ´ç´ è´å¶æ–¯\n\næœ´ç´ è´å¶æ–¯ï¼ˆNaive Bayesï¼‰ï¼šåŸºäºè´å¶æ–¯å®šç†çš„ç®€å•è€Œé«˜æ•ˆçš„åˆ†ç±»ç®—æ³•ã€‚",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "ç›‘ç£å­¦ä¹ ï¼ˆSupervised Learningï¼‰",
      "åˆ†ç±»"
    ]
  },
  {
    "objectID": "Classification.html#åŸºäºæ ‘çš„æ¨¡å‹",
    "href": "Classification.html#åŸºäºæ ‘çš„æ¨¡å‹",
    "title": "",
    "section": "\n1.6 åŸºäºæ ‘çš„æ¨¡å‹",
    "text": "1.6 åŸºäºæ ‘çš„æ¨¡å‹\n\nå†³ç­–æ ‘ï¼ˆDecision Treeï¼‰ï¼šåŸºäºæ ‘çŠ¶æ¨¡å‹è¿›è¡Œå†³ç­–çš„åˆ†ç±»ç®—æ³•ã€‚\néšæœºæ£®æ—ï¼ˆRandom Forestï¼‰ï¼šç”±å¤šæ£µå†³ç­–æ ‘ç»„æˆçš„é›†æˆå­¦ä¹ æ¨¡å‹ã€‚\næ¢¯åº¦æå‡æ ‘ï¼ˆGradient Boosting Treesï¼‰ï¼šé€šè¿‡åŠ æ³•æ¨¡å‹å’Œå‰å‘åˆ†æ­¥ç®—æ³•å®ç°çš„é›†æˆå­¦ä¹ æ¨¡å‹ã€‚",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "ç›‘ç£å­¦ä¹ ï¼ˆSupervised Learningï¼‰",
      "åˆ†ç±»"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\nCode1 + 1\n#&gt; [1] 2\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Clustering.html",
    "href": "Clustering.html",
    "title": "",
    "section": "",
    "text": "æ— ç›‘ç£å­¦ä¹ ï¼ˆUnsupervised Learningï¼‰èšç±» CodeShow All CodeHide All CodeView Source",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "æ— ç›‘ç£å­¦ä¹ ï¼ˆUnsupervised Learningï¼‰",
      "èšç±»"
    ]
  },
  {
    "objectID": "Clustering.html#åˆ’åˆ†èšç±»-partitioning-clustering",
    "href": "Clustering.html#åˆ’åˆ†èšç±»-partitioning-clustering",
    "title": "",
    "section": "\n1.1 åˆ’åˆ†èšç±» partitioning clustering",
    "text": "1.1 åˆ’åˆ†èšç±» partitioning clustering\n\n1.1.1 K Means Cluster Specification\nnum_clusters = 3æŒ‡å®šä¸­å¿ƒç‚¹ï¼ˆcentroidsï¼‰å³ç±»çš„ä¸ªæ•°ï¼Œnstart = 20æŒ‡å®šåˆå§‹ä½ç½®çš„ä¸ªæ•°ï¼Œå¸Œæœ›æ‰¾åˆ°å…¨å±€æœ€å¤§å€¼è€Œä¸æ˜¯å±€éƒ¨æœ€å¤§å€¼\n\nCodekmeans_spec &lt;-tidyclust::k_means(num_clusters = 3) %&gt;%\n  set_mode(\"partition\") %&gt;%\n  set_engine(\"stats\") %&gt;%\n  set_args(nstart = 20)\n\nkmeans_spec\n#&gt; K Means Cluster Specification (partition)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   num_clusters = 3\n#&gt; \n#&gt; Engine-Specific Arguments:\n#&gt;   nstart = 20\n#&gt; \n#&gt; Computational engine: stats\n\n\nK-means algorithm starts with random initialization\n\nCodeset.seed(100)\nkmeans_fit &lt;- kmeans_spec %&gt;%\n  fit(~., data = x_df)\n\nkmeans_fit$fit\n#&gt; K-means clustering with 3 clusters of sizes 14, 12, 24\n#&gt; \n#&gt; Cluster means:\n#&gt;           V1         V2\n#&gt; 2 -0.4354713 -0.8929796\n#&gt; 1 -0.0594887  0.8269786\n#&gt; 3  2.6977371 -3.9171729\n#&gt; \n#&gt; Clustering vector:\n#&gt;  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n#&gt;  1  1  2  2  2  2  1  2  1  2  1  1  1  2  2  2  2  1  2  1  2  1  1  1  1  3 \n#&gt; 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 \n#&gt;  3  3  3  3  3  3  3  3  3  3  3  1  3  3  3  3  3  3  3  3  3  3  3  3 \n#&gt; \n#&gt; Within cluster sum of squares by cluster:\n#&gt; [1] 19.593523  9.502891 32.730828\n#&gt;  (between_SS / total_SS =  83.4 %)\n#&gt; \n#&gt; Available components:\n#&gt; \n#&gt; [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n#&gt; [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"\n\nextract_centroids(kmeans_fit)\n#&gt; # A tibble: 3 Ã— 3\n#&gt;   .cluster       V1     V2\n#&gt;   &lt;fct&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 Cluster_1 -0.435  -0.893\n#&gt; 2 Cluster_2 -0.0595  0.827\n#&gt; 3 Cluster_3  2.70   -3.92\nkmeans_fit$fit$centers\n#&gt;           V1         V2\n#&gt; 2 -0.4354713 -0.8929796\n#&gt; 1 -0.0594887  0.8269786\n#&gt; 3  2.6977371 -3.9171729\n\nkmeans_fit$fit$cluster\n#&gt;  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n#&gt;  1  1  2  2  2  2  1  2  1  2  1  1  1  2  2  2  2  1  2  1  2  1  1  1  1  3 \n#&gt; 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 \n#&gt;  3  3  3  3  3  3  3  3  3  3  3  1  3  3  3  3  3  3  3  3  3  3  3  3\n\n\n\nCodepredict(kmeans_fit, new_data = x_df)\n#&gt; # A tibble: 50 Ã— 1\n#&gt;    .pred_cluster\n#&gt;    &lt;fct&gt;        \n#&gt;  1 Cluster_1    \n#&gt;  2 Cluster_1    \n#&gt;  3 Cluster_2    \n#&gt;  4 Cluster_2    \n#&gt;  5 Cluster_2    \n#&gt;  6 Cluster_2    \n#&gt;  7 Cluster_1    \n#&gt;  8 Cluster_2    \n#&gt;  9 Cluster_1    \n#&gt; 10 Cluster_2    \n#&gt; # â„¹ 40 more rows\naugment(kmeans_fit, new_data = x_df)\n#&gt; # A tibble: 50 Ã— 3\n#&gt;         V1     V2 .pred_cluster\n#&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;        \n#&gt;  1  0.0187 -0.401 Cluster_1    \n#&gt;  2 -0.184  -0.335 Cluster_1    \n#&gt;  3 -1.37    1.37  Cluster_2    \n#&gt;  4 -0.599   2.14  Cluster_2    \n#&gt;  5  0.295   0.506 Cluster_2    \n#&gt;  6  0.390   0.786 Cluster_2    \n#&gt;  7 -1.21   -0.902 Cluster_1    \n#&gt;  8 -0.364   0.533 Cluster_2    \n#&gt;  9 -1.63   -0.646 Cluster_1    \n#&gt; 10 -0.256   0.291 Cluster_2    \n#&gt; # â„¹ 40 more rows\n\n\n\nCodeaugment(kmeans_fit, new_data = x_df) %&gt;%\n  ggplot(aes(V1, V2, color = .pred_cluster)) +\n  geom_point()\n\n\n\n\n\n\n\ntune_cluster()æ‰¾åˆ°æœ€é€‚åˆçš„ç±»çš„æ•°ç›®\n\nCodekmeans_spec_tuned &lt;- kmeans_spec %&gt;% \n  set_args(num_clusters = tune())\n\nkmeans_wf &lt;- workflow() %&gt;%\n  add_model(kmeans_spec_tuned) %&gt;%\n  add_formula(~.)\n\n\n\nCodeset.seed(1000)\nx_boots &lt;- bootstraps(x_df, times = 10)\n\nnum_clusters_grid &lt;- tibble(num_clusters = seq(1, 10))\n\ntune_res &lt;- tune_cluster(\n  object = kmeans_wf,\n  resamples = x_boots,\n  grid = num_clusters_grid\n)\n\n\n\nCodetune_res %&gt;%\n  collect_metrics()\n#&gt; # A tibble: 20 Ã— 7\n#&gt;    num_clusters .metric          .estimator   mean     n std_err .config        \n#&gt;           &lt;int&gt; &lt;chr&gt;            &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;          \n#&gt;  1            1 sse_total        standard   381.      10  10.4   Preprocessor1_â€¦\n#&gt;  2            1 sse_within_total standard   381.      10  10.4   Preprocessor1_â€¦\n#&gt;  3            2 sse_total        standard   381.      10  10.4   Preprocessor1_â€¦\n#&gt;  4            2 sse_within_total standard    81.4     10   4.36  Preprocessor1_â€¦\n#&gt;  5            3 sse_total        standard   381.      10  10.4   Preprocessor1_â€¦\n#&gt;  6            3 sse_within_total standard    56.8     10   3.34  Preprocessor1_â€¦\n#&gt;  7            4 sse_total        standard   381.      10  10.4   Preprocessor1_â€¦\n#&gt;  8            4 sse_within_total standard    40.5     10   2.33  Preprocessor1_â€¦\n#&gt;  9            5 sse_total        standard   381.      10  10.4   Preprocessor1_â€¦\n#&gt; 10            5 sse_within_total standard    29.8     10   1.71  Preprocessor1_â€¦\n#&gt; 11            6 sse_total        standard   381.      10  10.4   Preprocessor1_â€¦\n#&gt; 12            6 sse_within_total standard    21.8     10   1.43  Preprocessor1_â€¦\n#&gt; 13            7 sse_total        standard   381.      10  10.4   Preprocessor1_â€¦\n#&gt; 14            7 sse_within_total standard    17.0     10   1.04  Preprocessor1_â€¦\n#&gt; 15            8 sse_total        standard   381.      10  10.4   Preprocessor1_â€¦\n#&gt; 16            8 sse_within_total standard    13.6     10   0.842 Preprocessor1_â€¦\n#&gt; 17            9 sse_total        standard   381.      10  10.4   Preprocessor1_â€¦\n#&gt; 18            9 sse_within_total standard    11.0     10   0.669 Preprocessor1_â€¦\n#&gt; 19           10 sse_total        standard   381.      10  10.4   Preprocessor1_â€¦\n#&gt; 20           10 sse_within_total standard     8.82    10   0.595 Preprocessor1_â€¦\n\n\nelbow method æ‰¾åˆ°æœ€ç†æƒ³çš„ç±»çš„ä¸ªæ•°ã€‚\n\nCodetune_res %&gt;%\n  autoplot()\n\n\n\n\n\n\n\nè°ƒæ•´åçš„èšç±»\n\nCodefinal_kmeans &lt;- kmeans_wf %&gt;%\n  update_model(kmeans_spec %&gt;% set_args(num_clusters = 2)) %&gt;%\n  fit(x_df)\n\n\n\nCodeaugment(final_kmeans, new_data = x_df) %&gt;%\n  ggplot(aes(V1, V2, color = .pred_cluster)) +\n  geom_point()",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "æ— ç›‘ç£å­¦ä¹ ï¼ˆUnsupervised Learningï¼‰",
      "èšç±»"
    ]
  },
  {
    "objectID": "Clustering.html#åˆ†å±‚èšç±»å°æ ·æœ¬hierarchical-clustering",
    "href": "Clustering.html#åˆ†å±‚èšç±»å°æ ·æœ¬hierarchical-clustering",
    "title": "",
    "section": "\n1.2 åˆ†å±‚èšç±»(å°æ ·æœ¬)Hierarchical Clustering",
    "text": "1.2 åˆ†å±‚èšç±»(å°æ ·æœ¬)Hierarchical Clustering\nç®—æ³•\n\nå®šä¹‰æ¯ä¸ªè§‚æµ‹ä¸ºä¸€ç±»\nè®¡ç®—æ¯ç±»ä¸å…¶ä»–å„ç±»çš„è·ç¦»\næŠŠè·ç¦»æœ€çŸ­çš„ä¸¤ç±»åˆå¹¶æˆæ–°çš„ä¸€ç±»,æ€»çš„ç±»çš„ä¸ªæ•°å‡ä¸€\né‡å¤2,3æ­¥éª¤,ç›´åˆ°æ‰€æœ‰çš„ç±»èšæˆå•ä¸ªç±»ä¸ºæ­¢\n\n\n1.2.1 hclust specification\n\nCoderes_hclust_complete &lt;- tidyclust::hier_clust(linkage_method = \"complete\") %&gt;%\n  fit(~., data = x_df)\n\nres_hclust_average &lt;- hier_clust(linkage_method = \"average\") %&gt;%\n  fit(~., data = x_df)\n\nres_hclust_single &lt;- hier_clust(linkage_method = \"single\") %&gt;%\n  fit(~., data = x_df)\n\n\nfactoextra package æå–æ¨¡å‹ä¿¡æ¯å’Œå¯è§†åŒ–\n\nCodelibrary(factoextra)\nres_hclust_complete %&gt;%\n  extract_fit_engine() %&gt;%\n  fviz_dend(main = \"complete\", k = 2)\n#&gt; Warning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\n#&gt; of ggplot2 3.3.4.\n#&gt; â„¹ The deprecated feature was likely used in the factoextra package.\n#&gt;   Please report the issue at &lt;https://github.com/kassambara/factoextra/issues&gt;.\n\n\n\n\n\n\n\n\nCoderes_hclust_average %&gt;%\n  extract_fit_engine() %&gt;%\n  fviz_dend(main = \"average\", k = 2)\n\n\n\n\n\n\n\n\nCoderes_hclust_single %&gt;%\n  extract_fit_engine() %&gt;%\n  fviz_dend(main = \"single\", k = 2)\n\n\n\n\n\n\n\n\nCodehier_rec &lt;- recipe(~., data = x_df) %&gt;%\n  step_normalize(all_numeric_predictors()) # æ ‡å‡†åŒ–\n\nhier_wf &lt;- workflow() %&gt;%\n  add_recipe(hier_rec) %&gt;%\n  add_model(hier_clust(linkage_method = \"complete\"))\n\nhier_fit &lt;- hier_wf %&gt;%\n  fit(data = x_df) \n\nhier_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  fviz_dend(k = 2)",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "æ— ç›‘ç£å­¦ä¹ ï¼ˆUnsupervised Learningï¼‰",
      "èšç±»"
    ]
  },
  {
    "objectID": "descriptive_models.html",
    "href": "descriptive_models.html",
    "title": "",
    "section": "",
    "text": "Code",
    "crumbs": [
      "æè¿°æ€§æ¨¡å‹",
      "**æè¿°æ€§æ¨¡å‹**"
    ]
  },
  {
    "objectID": "descriptive_models.html#footnotes",
    "href": "descriptive_models.html#footnotes",
    "title": "",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCleveland, W. 1979.Â â€œRobust Locally Weighted Regression and Smoothing Scatterplots.â€Â Journal of the American Statistical AssociationÂ 74 (368): 829â€“36.â†©ï¸\nBolstad, B. 2004.Â Low-Level Analysis of High-Density Oligonucleotide Array Data: Background, Normalization and Summarization. University of California, Berkeley.â†©ï¸",
    "crumbs": [
      "æè¿°æ€§æ¨¡å‹",
      "**æè¿°æ€§æ¨¡å‹**"
    ]
  },
  {
    "objectID": "inferential_models.html",
    "href": "inferential_models.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\næ¨æ–­æ€§æ¨¡å‹\næ¨æ–­æ€§æ¨¡å‹ï¼ˆInferential Modelsï¼‰æ˜¯ç”¨äºä»æ ·æœ¬æ•°æ®ä¸­æ¨æ–­æ€»ä½“ç‰¹å¾æˆ–ä½œå‡ºå…³äºæ€»ä½“çš„ç»“è®ºçš„ç»Ÿè®¡å·¥å…·å’Œæ–¹æ³•ã€‚è¿™äº›æ¨¡å‹ä¸ä»…ä»…æ˜¯æè¿°æ•°æ®ç‰¹å¾ï¼Œè€Œæ˜¯åˆ©ç”¨æ•°æ®æ¥è¿›è¡Œæ¨æ–­ã€é¢„æµ‹æˆ–æµ‹è¯•å‡è®¾ã€‚æ¨æ–­æ€§æ¨¡å‹é€šå¸¸å…³æ³¨äºç¡®å®šæ•°æ®ä¸­å˜é‡ä¹‹é—´çš„å…³ç³»ã€è¯„ä¼°å› æœå…³ç³»ã€åšå‡ºé¢„æµ‹å’Œè¿›è¡Œç»Ÿè®¡æ¨æ–­ã€‚\næ¨æ–­æ€§æ¨¡å‹é€šè¿‡å‡è®¾æ£€éªŒã€ç½®ä¿¡åŒºé—´ä¼°è®¡ã€å›å½’åˆ†æç­‰æ–¹æ³•ï¼Œå¯¹æ€»ä½“åšå‡ºç»“è®ºæˆ–é¢„æµ‹ï¼š\n\nçº¿æ€§å›å½’æ¨¡å‹\nå¹¿ä¹‰çº¿æ€§æ¨¡å‹ï¼Œå¦‚ t æ£€éªŒï¼Œæ–¹å·®åˆ†æï¼ˆANOVAï¼‰\nå¡æ–¹æ£€éªŒ\nç”Ÿå­˜åˆ†æ\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "æ¨æ–­æ€§æ¨¡å‹",
      "**æ¨æ–­æ€§æ¨¡å‹**"
    ]
  },
  {
    "objectID": "machine_learning.html",
    "href": "machine_learning.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All CodeView Source",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "æœºå™¨å­¦ä¹ "
    ]
  },
  {
    "objectID": "machine_learning.html#æ¢ç´¢æ€§æ•°æ®åˆ†æ",
    "href": "machine_learning.html#æ¢ç´¢æ€§æ•°æ®åˆ†æ",
    "title": "",
    "section": "\n1.1 æ¢ç´¢æ€§æ•°æ®åˆ†æ",
    "text": "1.1 æ¢ç´¢æ€§æ•°æ®åˆ†æ\n\nCodelibrary(tidymodels)\n#&gt; â”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels 1.2.0 â”€â”€\n#&gt; âœ” broom        1.0.6     âœ” rsample      1.2.1\n#&gt; âœ” dials        1.2.1     âœ” tune         1.2.1\n#&gt; âœ” infer        1.0.7     âœ” workflows    1.1.4\n#&gt; âœ” modeldata    1.4.0     âœ” workflowsets 1.1.0\n#&gt; âœ” parsnip      1.2.1     âœ” yardstick    1.3.1\n#&gt; âœ” recipes      1.1.0\ndata(ames)\n\nggplot(ames, aes(x = Sale_Price)) + \n  geom_histogram(bins = 50, col= \"white\")\n\n\n\n\n\n\nCode\n\n\n\names &lt;- ames |&gt;mutate(Sale_Price = log10(Sale_Price))\n\nggplot(ames, aes(x = Sale_Price)) + \n  geom_histogram(bins = 50, col= \"white\")+\n    geom_vline(xintercept =quantile(ames$Sale_Price),lty=5 )",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "æœºå™¨å­¦ä¹ "
    ]
  },
  {
    "objectID": "machine_learning.html#æ‹†åˆ†è®­ç»ƒé›†éªŒè¯é›†å’Œæµ‹è¯•é›†-rsample",
    "href": "machine_learning.html#æ‹†åˆ†è®­ç»ƒé›†éªŒè¯é›†å’Œæµ‹è¯•é›†-rsample",
    "title": "",
    "section": "\n1.2 æ‹†åˆ†è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›† rsample\n",
    "text": "1.2 æ‹†åˆ†è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›† rsample\n\n\n1.2.1 ç®€å•æŠ½æ ·\n\nCodeset.seed(10)\names_split &lt;- initial_split(ames, prop = c(0.8))\names_split\n\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\ndim(ames_train)\n\n\n\n1.2.2 åˆ†å±‚æŠ½æ ·\n\nCodeset.seed(100)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\ndim(ames_train)\n#&gt; [1] 2342   74\n\n\n\n1.2.3 éªŒè¯é›†\n\nCodeset.seed(101)\n\n# To put 60% into training, 20% in validation, and remaining 20% in testing:\names_split &lt;- initial_validation_split(ames, prop = c(0.6, 0.2),\n                                       strata = Sale_Price)\names_split\n\names_train &lt;- training(ames_split)\names_test &lt;- testing(ames_split)\names_valid &lt;- validation(ames_split)",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "æœºå™¨å­¦ä¹ "
    ]
  },
  {
    "objectID": "machine_learning.html#æ¨¡å‹é€‰æ‹©-parsnip",
    "href": "machine_learning.html#æ¨¡å‹é€‰æ‹©-parsnip",
    "title": "",
    "section": "\n1.3 æ¨¡å‹é€‰æ‹© parsnip\n",
    "text": "1.3 æ¨¡å‹é€‰æ‹© parsnip\n\n\nCodeparsnip_addin()\n\n\n\nCodeshow_engines('linear_reg')\n#&gt; # A tibble: 7 Ã— 2\n#&gt;   engine mode      \n#&gt;   &lt;chr&gt;  &lt;chr&gt;     \n#&gt; 1 lm     regression\n#&gt; 2 glm    regression\n#&gt; 3 glmnet regression\n#&gt; 4 stan   regression\n#&gt; 5 spark  regression\n#&gt; 6 keras  regression\n#&gt; 7 brulee regression\nshow_engines(\"logistic_reg\")\n#&gt; # A tibble: 7 Ã— 2\n#&gt;   engine    mode          \n#&gt;   &lt;chr&gt;     &lt;chr&gt;         \n#&gt; 1 glm       classification\n#&gt; 2 glmnet    classification\n#&gt; 3 LiblineaR classification\n#&gt; 4 spark     classification\n#&gt; 5 keras     classification\n#&gt; 6 stan      classification\n#&gt; 7 brulee    classification\n\n\n\nCodelm_model &lt;- linear_reg() |&gt; \n    set_engine(\"lm\") |&gt; \n    set_mode(mode = \"regression\")\nlm_model\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lm\n\nlogistic_reg() |&gt;\n  set_mode(\"classification\") |&gt;\n  set_engine(\"glm\") \n#&gt; Logistic Regression Model Specification (classification)\n#&gt; \n#&gt; Computational engine: glm\n\nrand_forest(trees = 1000, min_n = 5) |&gt;\n  set_engine(\"ranger\", verbose = TRUE) |&gt;\n  set_mode(\"regression\") \n#&gt; Random Forest Model Specification (regression)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   trees = 1000\n#&gt;   min_n = 5\n#&gt; \n#&gt; Engine-Specific Arguments:\n#&gt;   verbose = TRUE\n#&gt; \n#&gt; Computational engine: ranger\n\ndecision_tree(min_n = 2) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n#&gt; Decision Tree Model Specification (regression)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   min_n = 2\n#&gt; \n#&gt; Computational engine: rpart",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "æœºå™¨å­¦ä¹ "
    ]
  },
  {
    "objectID": "machine_learning.html#æ¨¡å‹å·¥ä½œæµ-workflows",
    "href": "machine_learning.html#æ¨¡å‹å·¥ä½œæµ-workflows",
    "title": "",
    "section": "\n1.4 æ¨¡å‹å·¥ä½œæµ workflows\n",
    "text": "1.4 æ¨¡å‹å·¥ä½œæµ workflows\n\n\n1.4.1 çº¿æ€§æ¨¡å‹\né¢„å¤„ç† Preprocessor\n\nCode\n#  Preprocessor\n# None\n\nlm_wflow &lt;- \n  workflow() %&gt;% \n  add_model(lm_model)\n\nlm_wflow\n#&gt; â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: None\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lm\n\n\n# Formula\nlm_wflow &lt;- lm_wflow |&gt; \n    add_formula(Sale_Price ~ Longitude + Latitude)\n\nlm_wflow\n#&gt; â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Sale_Price ~ Longitude + Latitude\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lm\n\n#  æ‹Ÿåˆ\nlm_fit &lt;- fit(lm_wflow, ames_train)\nlm_fit\n#&gt; â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Sale_Price ~ Longitude + Latitude\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ..y ~ ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)    Longitude     Latitude  \n#&gt;    -314.938       -2.138        2.853\n\n\n# æ›´æ¢å…¬å¼ å†æ‹Ÿåˆ\nlm_wflow %&gt;% update_formula(Sale_Price ~ Longitude) |&gt; fit(ames_train)\n#&gt; â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Sale_Price ~ Longitude\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ..y ~ ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)    Longitude  \n#&gt;    -191.309       -2.099\n\n\n\n# Variables:outcomes ~ predictors\nlm_wflow &lt;- \n  lm_wflow %&gt;% \n  remove_formula() %&gt;% \n  add_variables(outcomes  = Sale_Price, predictors = c(Longitude, Latitude))  # c(ends_with(\"tude\"))\n\nlm_wflow |&gt; fit(ames_train)\n#&gt; â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Variables\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Outcomes: Sale_Price\n#&gt; Predictors: c(Longitude, Latitude)\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ..y ~ ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)    Longitude     Latitude  \n#&gt;    -314.938       -2.138        2.853\n\n\n# recepe\n\n\n\n1.4.2 é¢„æµ‹\n\nCode\n# å›å½’ \"numeric\" , \"conf_int\",\"pred_int\",\"raw\".\n#  censored regression   \"time\"ï¼Œ\"hazard\",\"survival\"\n# åˆ†ç±»  \"class\", \"prob\",\n# \"quantile\"\n\n# When NULL, predict() will choose an appropriate value based on the model's mode.\npredict(lm_fit, ames_test)  # \"numeric\"\n#&gt; # A tibble: 588 Ã— 1\n#&gt;    .pred\n#&gt;    &lt;dbl&gt;\n#&gt;  1  5.23\n#&gt;  2  5.29\n#&gt;  3  5.28\n#&gt;  4  5.27\n#&gt;  5  5.26\n#&gt;  6  5.24\n#&gt;  7  5.24\n#&gt;  8  5.24\n#&gt;  9  5.24\n#&gt; 10  5.30\n#&gt; # â„¹ 578 more rows\n\n\n\n1.4.3 æ··åˆæ•ˆåº”æ¨¡å‹ multilevelmod\n\n\nCodelibrary(multilevelmod)\nmultilevel_spec &lt;- linear_reg() %&gt;% set_engine(\"lmer\")\nmultilevel_spec\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lmer\n\ndf &lt;-  read_delim(\"data/lme_anova.txt\",) |&gt; pivot_longer(cols = 3:7,names_to = \"time\",values_to = \"BP\") |&gt; \n    mutate_at(1:3,as.factor)\n#&gt; Rows: 15 Columns: 7\n#&gt; â”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Delimiter: \"\\t\"\n#&gt; chr (1): induced_method\n#&gt; dbl (6): subject, t0, t1, t2, t3, t4\n#&gt; \n#&gt; â„¹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; â„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\ndf\n#&gt; # A tibble: 75 Ã— 4\n#&gt;    subject induced_method time     BP\n#&gt;    &lt;fct&gt;   &lt;fct&gt;          &lt;fct&gt; &lt;dbl&gt;\n#&gt;  1 1       A              t0      120\n#&gt;  2 1       A              t1      108\n#&gt;  3 1       A              t2      112\n#&gt;  4 1       A              t3      120\n#&gt;  5 1       A              t4      117\n#&gt;  6 2       A              t0      118\n#&gt;  7 2       A              t1      109\n#&gt;  8 2       A              t2      115\n#&gt;  9 2       A              t3      126\n#&gt; 10 2       A              t4      123\n#&gt; # â„¹ 65 more rows\nmultilevel_workflow &lt;- \n  workflow() %&gt;% \n  add_variables(outcome = BP, predictors = c(induced_method,time,subject)) %&gt;% \n  add_model(multilevel_spec, \n            # This formula is given to the model\n            formula = BP ~ induced_method+time + ( 1| subject))\n\nmultilevel_workflow\n#&gt; â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Variables\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Outcomes: BP\n#&gt; Predictors: c(induced_method, time, subject)\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lmer\nmultilevel_workflow |&gt;  fit(data =df) \n#&gt; â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Variables\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Outcomes: BP\n#&gt; Predictors: c(induced_method, time, subject)\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Linear mixed model fit by REML ['lmerMod']\n#&gt; Formula: BP ~ induced_method + time + (1 | subject)\n#&gt;    Data: data\n#&gt; REML criterion at convergence: 431.0591\n#&gt; Random effects:\n#&gt;  Groups   Name        Std.Dev.\n#&gt;  subject  (Intercept) 3.441   \n#&gt;  Residual             4.434   \n#&gt; Number of obs: 75, groups:  subject, 15\n#&gt; Fixed Effects:\n#&gt;     (Intercept)  induced_methodB  induced_methodC           timet1  \n#&gt;         118.360            4.800            8.520           -4.400  \n#&gt;          timet2           timet3           timet4  \n#&gt;          -4.467            9.400            6.067\n\n\n\n1.4.4 ç”Ÿå­˜æ¨¡å‹ censored\n\ntype = \"time\"  type = \"survival\"   type = \"linear_pred\"   type = \"quantile\"   type = \"hazard\"\n\nCodelibrary(censored)\n#&gt; Loading required package: survival\n\nparametric_spec &lt;- survival_reg()\n\nparametric_workflow &lt;- \n  workflow() %&gt;% \n  add_variables(outcome = c(fustat, futime), predictors = c(age, rx)) %&gt;% \n  add_model(parametric_spec, \n            formula = Surv(futime, fustat) ~ age + strata(rx))\n\nparametric_fit &lt;- fit(parametric_workflow, data = ovarian)\nparametric_fit\n#&gt; â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Variables\n#&gt; Model: survival_reg()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Outcomes: c(fustat, futime)\n#&gt; Predictors: c(age, rx)\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Call:\n#&gt; survival::survreg(formula = Surv(futime, fustat) ~ age + strata(rx), \n#&gt;     data = data, model = TRUE)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)         age \n#&gt;  12.8734120  -0.1033569 \n#&gt; \n#&gt; Scale:\n#&gt;      rx=1      rx=2 \n#&gt; 0.7695509 0.4703602 \n#&gt; \n#&gt; Loglik(model)= -89.4   Loglik(intercept only)= -97.1\n#&gt;  Chisq= 15.36 on 1 degrees of freedom, p= 8.88e-05 \n#&gt; n= 26\n\n\n\n1.4.5 å·¥ä½œæµé›† workflowsets\n\n\nCodelocation &lt;- list(\n  longitude = Sale_Price ~ Longitude,\n  latitude = Sale_Price ~ Latitude,\n  coords = Sale_Price ~ Longitude + Latitude,\n  neighborhood = Sale_Price ~ Neighborhood\n)\n\nlibrary(workflowsets)\nlocation_models &lt;- workflow_set(preproc = location, models = list(lm = lm_model))\nlocation_models\n#&gt; # A workflow set/tibble: 4 Ã— 4\n#&gt;   wflow_id        info             option    result    \n#&gt;   &lt;chr&gt;           &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n#&gt; 1 longitude_lm    &lt;tibble [1 Ã— 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 2 latitude_lm     &lt;tibble [1 Ã— 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 3 coords_lm       &lt;tibble [1 Ã— 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 4 neighborhood_lm &lt;tibble [1 Ã— 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\nlocation_models$info[[1]]\n#&gt; # A tibble: 1 Ã— 4\n#&gt;   workflow   preproc model      comment\n#&gt;   &lt;list&gt;     &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;  \n#&gt; 1 &lt;workflow&gt; formula linear_reg \"\"\nextract_workflow(location_models, id = \"coords_lm\")\n#&gt; â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Sale_Price ~ Longitude + Latitude\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lm\n\n\nlocation_models &lt;-\n   location_models %&gt;%\n   mutate(fit = map(info, ~ fit(.x$workflow[[1]], ames_train)))\nlocation_models\n#&gt; # A workflow set/tibble: 4 Ã— 5\n#&gt;   wflow_id        info             option    result     fit       \n#&gt;   &lt;chr&gt;           &lt;list&gt;           &lt;list&gt;    &lt;list&gt;     &lt;list&gt;    \n#&gt; 1 longitude_lm    &lt;tibble [1 Ã— 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; &lt;workflow&gt;\n#&gt; 2 latitude_lm     &lt;tibble [1 Ã— 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; &lt;workflow&gt;\n#&gt; 3 coords_lm       &lt;tibble [1 Ã— 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; &lt;workflow&gt;\n#&gt; 4 neighborhood_lm &lt;tibble [1 Ã— 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; &lt;workflow&gt;\n\nlocation_models$fit[[1]]\n#&gt; â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Formula\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Sale_Price ~ Longitude\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ..y ~ ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)    Longitude  \n#&gt;    -191.309       -2.099\n\n\n\nCodefinal_lm_res &lt;- last_fit(lm_wflow, ames_split)\nfinal_lm_res\n#&gt; # Resampling results\n#&gt; # Manual resampling \n#&gt; # A tibble: 1 Ã— 6\n#&gt;   splits             id               .metrics .notes   .predictions .workflow \n#&gt;   &lt;list&gt;             &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n#&gt; 1 &lt;split [2342/588]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\nextract_workflow(final_lm_res)\n#&gt; â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Variables\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Outcomes: Sale_Price\n#&gt; Predictors: c(Longitude, Latitude)\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ..y ~ ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)    Longitude     Latitude  \n#&gt;    -314.938       -2.138        2.853\n\ncollect_metrics(final_lm_res)\n#&gt; # A tibble: 2 Ã— 4\n#&gt;   .metric .estimator .estimate .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 rmse    standard       0.157 Preprocessor1_Model1\n#&gt; 2 rsq     standard       0.152 Preprocessor1_Model1\ncollect_predictions(final_lm_res) %&gt;% slice(1:5)\n#&gt; # A tibble: 5 Ã— 5\n#&gt;   .pred id                .row Sale_Price .config             \n#&gt;   &lt;dbl&gt; &lt;chr&gt;            &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1  5.23 train/test split     1       5.33 Preprocessor1_Model1\n#&gt; 2  5.29 train/test split     6       5.29 Preprocessor1_Model1\n#&gt; 3  5.28 train/test split    13       5.26 Preprocessor1_Model1\n#&gt; 4  5.27 train/test split    14       5.23 Preprocessor1_Model1\n#&gt; 5  5.26 train/test split    16       5.73 Preprocessor1_Model1",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "æœºå™¨å­¦ä¹ "
    ]
  },
  {
    "objectID": "machine_learning.html#ç‰¹å¾å·¥ç¨‹-recipes",
    "href": "machine_learning.html#ç‰¹å¾å·¥ç¨‹-recipes",
    "title": "",
    "section": "\n1.5 ç‰¹å¾å·¥ç¨‹ recipes\n",
    "text": "1.5 ç‰¹å¾å·¥ç¨‹ recipes\n\n\n1.5.1 è™šæ‹Ÿå˜é‡\n\nCodesimple_ames &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,\n         data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_dummy(all_nominal_predictors())# å› å­æˆ–å­—ç¬¦å˜é‡ï¼Œåä¹‰nominal\n# all_numeric_predictors()  all_numeric()  all_predictors()  all_outcomes()\n\n# è·¨æ¨¡å‹å¾ªç¯ä½¿ç”¨\nsimple_ames\n#&gt; \n#&gt; â”€â”€ Recipe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; â”€â”€ Inputs\n#&gt; Number of variables by role\n#&gt; outcome:   1\n#&gt; predictor: 4\n#&gt; \n#&gt; â”€â”€ Operations\n#&gt; â€¢ Log transformation on: Gr_Liv_Area\n#&gt; â€¢ Dummy variables from: all_nominal_predictors()\n\n\n\nCodelm_wflow &lt;- \n  lm_wflow %&gt;% \n    #ä¸€æ¬¡åªèƒ½æœ‰ä¸€ç§é¢„å¤„ç†æ–¹æ³•ï¼Œéœ€è¦åœ¨æ·»åŠ é…æ–¹ä¹‹å‰åˆ é™¤ç°æœ‰çš„é¢„å¤„ç†å™¨\n  remove_variables() %&gt;% \n  add_recipe(simple_ames)\n\nlm_wflow\n#&gt; â•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Recipe\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; 2 Recipe Steps\n#&gt; \n#&gt; â€¢ step_log()\n#&gt; â€¢ step_dummy()\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lm\n\nlm_fit &lt;- fit(lm_wflow, ames_train)\npredict(lm_fit, ames_test)\n#&gt; Warning in predict.lm(object = object$fit, newdata = new_data, type =\n#&gt; \"response\", : prediction from rank-deficient fit; consider predict(.,\n#&gt; rankdeficient=\"NA\")\n#&gt; # A tibble: 588 Ã— 1\n#&gt;    .pred\n#&gt;    &lt;dbl&gt;\n#&gt;  1  5.24\n#&gt;  2  5.27\n#&gt;  3  5.24\n#&gt;  4  5.20\n#&gt;  5  5.66\n#&gt;  6  5.12\n#&gt;  7  5.08\n#&gt;  8  5.22\n#&gt;  9  5.00\n#&gt; 10  5.37\n#&gt; # â„¹ 578 more rows\n\n# æå–æ¨¡å‹ä¿¡æ¯\nlm_fit %&gt;% \n  extract_recipe(estimated = TRUE)\n#&gt; \n#&gt; â”€â”€ Recipe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; â”€â”€ Inputs\n#&gt; Number of variables by role\n#&gt; outcome:   1\n#&gt; predictor: 4\n#&gt; \n#&gt; â”€â”€ Training information\n#&gt; Training data contained 2342 data points and no incomplete rows.\n#&gt; \n#&gt; â”€â”€ Operations\n#&gt; â€¢ Log transformation on: Gr_Liv_Area | Trained\n#&gt; â€¢ Dummy variables from: Neighborhood and Bldg_Type | Trained\n\nlm_fit %&gt;% \n  extract_fit_parsnip() %&gt;% \n  tidy()\n#&gt; # A tibble: 35 Ã— 5\n#&gt;    term                            estimate std.error statistic   p.value\n#&gt;    &lt;chr&gt;                              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1 (Intercept)                     -1.10     0.234       -4.70  2.72e-  6\n#&gt;  2 Gr_Liv_Area                      0.646    0.0143      45.0   1.88e-318\n#&gt;  3 Year_Built                       0.00217  0.000118    18.5   3.21e- 71\n#&gt;  4 Neighborhood_College_Creek       0.00897  0.00841      1.07  2.87e-  1\n#&gt;  5 Neighborhood_Old_Town           -0.0178   0.00846     -2.11  3.51e-  2\n#&gt;  6 Neighborhood_Edwards            -0.0413   0.00769     -5.37  8.59e-  8\n#&gt;  7 Neighborhood_Somerset            0.0433   0.00992      4.37  1.30e-  5\n#&gt;  8 Neighborhood_Northridge_Heights  0.129    0.0102      12.7   6.96e- 36\n#&gt;  9 Neighborhood_Gilbert            -0.0437   0.00939     -4.65  3.43e-  6\n#&gt; 10 Neighborhood_Sawyer             -0.00785  0.00841     -0.933 3.51e-  1\n#&gt; # â„¹ 25 more rows\n\n\n\nCode\nsimple_ames &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,\n         data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_other(Neighborhood, threshold = 0.01) %&gt;% \n  step_dummy(all_nominal_predictors())\n\n\n\nCodeggplot(ames_train, aes(x = Gr_Liv_Area, y = 10^Sale_Price)) + \n  geom_point(alpha = .2) + \n  facet_wrap(~ Bldg_Type) + \n  geom_smooth(method = lm, formula = y ~ x, se = FALSE, color = \"blue\") + \n  scale_x_log10() + \n  scale_y_log10() + \n  labs(x = \"Gross Living Area\", y = \"Sale Price (USD)\")\n\n\n\n\n\n\n\n\n1.5.2 äº¤äº’é¡¹\nstep_interact(~ interaction terms) , +åˆ†éš”ä¸åŒäº¤äº’æ•ˆåº”\n\nCodesimple_ames &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,\n         data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_other(Neighborhood, threshold = 0.01) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  # Gr_Liv_Area is on the log scale from a previous step\n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") )\n\nsimple_ames \n#&gt; \n#&gt; â”€â”€ Recipe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; â”€â”€ Inputs\n#&gt; Number of variables by role\n#&gt; outcome:   1\n#&gt; predictor: 4\n#&gt; \n#&gt; â”€â”€ Operations\n#&gt; â€¢ Log transformation on: Gr_Liv_Area\n#&gt; â€¢ Collapsing factor levels for: Neighborhood\n#&gt; â€¢ Dummy variables from: all_nominal_predictors()\n#&gt; â€¢ Interactions with: Gr_Liv_Area:starts_with(\"Bldg_Type_\")\n\n\n\n1.5.3 æ ·æ¡å‡½æ•°\næ·»åŠ éçº¿æ€§ç‰¹å¾\n\nCodelibrary(patchwork)\nlibrary(splines)\nplot_smoother &lt;- function(deg_free) {\n  ggplot(ames_train, aes(x = Latitude, y = 10^Sale_Price)) + \n    geom_point(alpha = .2) + \n    scale_y_log10() +\n    geom_smooth(\n      method = lm,\n      formula = y ~ ns(x, df = deg_free),# natural splines.\n      color = \"lightblue\",\n      se = FALSE\n    ) +\n    labs(title = paste(deg_free, \"Spline Terms\"),\n         y = \"Sale Price (USD)\")\n}\n\n( plot_smoother(1) + plot_smoother(5) ) / ( plot_smoother(20) + plot_smoother(100) )\n\n\n\n\n\n\n\n\nCoderecipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + Latitude,\n         data = ames_train) %&gt;%\n  step_ns(Latitude, deg_free = 20)\n#&gt; \n#&gt; â”€â”€ Recipe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; â”€â”€ Inputs\n#&gt; Number of variables by role\n#&gt; outcome:   1\n#&gt; predictor: 5\n#&gt; \n#&gt; â”€â”€ Operations\n#&gt; â€¢ Natural splines on: Latitude\n\n\n\n1.5.4 ç‰¹å¾æå–\nPCA,\n\nCode  # Use a regular expression to capture house size predictors: \nrecipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + Latitude+Total_Bsmt_SF+First_Flr_SF+Gr_Liv_Area,\n         data = ames_train) %&gt;%\n    step_pca(matches(\"(SF$)|(Gr_Liv)\"))\n#&gt; \n#&gt; â”€â”€ Recipe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; â”€â”€ Inputs\n#&gt; Number of variables by role\n#&gt; outcome:   1\n#&gt; predictor: 7\n#&gt; \n#&gt; â”€â”€ Operations\n#&gt; â€¢ PCA extraction with: matches(\"(SF$)|(Gr_Liv)\")\n\n\n\n1.5.5 è¡Œé‡‡æ ·\nDownsamplingï¼ŒUpsamplingï¼ŒHybrid\nstep_filter() step_sample() step_slice() step_arrange() skip TRUE\n\nCodelibrary(themis)\nstep_downsample(outcome_column_name)\n\n\n\n1.5.6 ä¸€èˆ¬è½¬æ¢\nstep_mutate() æ¯”ï¼ŒBedroom_AbvGr / Full_Bath\n\n1.5.7 tidy()\n\n\nCodeames_rec &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_other(Neighborhood, threshold = 0.01) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %&gt;% \n  step_ns(Latitude, Longitude, deg_free = 20)\ntidy(ames_rec)\n#&gt; # A tibble: 5 Ã— 6\n#&gt;   number operation type     trained skip  id            \n#&gt;    &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;    &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;         \n#&gt; 1      1 step      log      FALSE   FALSE log_qeD8W     \n#&gt; 2      2 step      other    FALSE   FALSE other_Hirvj   \n#&gt; 3      3 step      dummy    FALSE   FALSE dummy_HJDrx   \n#&gt; 4      4 step      interact FALSE   FALSE interact_B7kcu\n#&gt; 5      5 step      ns       FALSE   FALSE ns_yBiqt\n\n\n\nCodeames_rec &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_other(Neighborhood, threshold = 0.01, id = \"my_id\") %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %&gt;% \n  step_ns(Latitude, Longitude, deg_free = 20)\n\nlm_wflow &lt;- \n  workflow() %&gt;% \n  add_model(lm_model) %&gt;% \n  add_recipe(ames_rec)\n\nlm_fit &lt;- fit(lm_wflow, ames_train)\n\nestimated_recipe &lt;- \n  lm_fit %&gt;% \n  extract_recipe(estimated = TRUE)\n\ntidy(estimated_recipe, id = \"my_id\")\n#&gt; # A tibble: 21 Ã— 3\n#&gt;    terms        retained           id   \n#&gt;    &lt;chr&gt;        &lt;chr&gt;              &lt;chr&gt;\n#&gt;  1 Neighborhood North_Ames         my_id\n#&gt;  2 Neighborhood College_Creek      my_id\n#&gt;  3 Neighborhood Old_Town           my_id\n#&gt;  4 Neighborhood Edwards            my_id\n#&gt;  5 Neighborhood Somerset           my_id\n#&gt;  6 Neighborhood Northridge_Heights my_id\n#&gt;  7 Neighborhood Gilbert            my_id\n#&gt;  8 Neighborhood Sawyer             my_id\n#&gt;  9 Neighborhood Northwest_Ames     my_id\n#&gt; 10 Neighborhood Sawyer_West        my_id\n#&gt; # â„¹ 11 more rows\ntidy(estimated_recipe, number = 2)\n#&gt; # A tibble: 21 Ã— 3\n#&gt;    terms        retained           id   \n#&gt;    &lt;chr&gt;        &lt;chr&gt;              &lt;chr&gt;\n#&gt;  1 Neighborhood North_Ames         my_id\n#&gt;  2 Neighborhood College_Creek      my_id\n#&gt;  3 Neighborhood Old_Town           my_id\n#&gt;  4 Neighborhood Edwards            my_id\n#&gt;  5 Neighborhood Somerset           my_id\n#&gt;  6 Neighborhood Northridge_Heights my_id\n#&gt;  7 Neighborhood Gilbert            my_id\n#&gt;  8 Neighborhood Sawyer             my_id\n#&gt;  9 Neighborhood Northwest_Ames     my_id\n#&gt; 10 Neighborhood Sawyer_West        my_id\n#&gt; # â„¹ 11 more rows\n\n\n\n1.5.8 åˆ—è§’è‰²\n\nCodeames_rec %&gt;% update_role(Year_Built, new_role = \"Street\")\n#&gt; \n#&gt; â”€â”€ Recipe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; â”€â”€ Inputs\n#&gt; Number of variables by role\n#&gt; outcome:   1\n#&gt; predictor: 5\n#&gt; Street:    1\n#&gt; \n#&gt; â”€â”€ Operations\n#&gt; â€¢ Log transformation on: Gr_Liv_Area\n#&gt; â€¢ Collapsing factor levels for: Neighborhood\n#&gt; â€¢ Dummy variables from: all_nominal_predictors()\n#&gt; â€¢ Interactions with: Gr_Liv_Area:starts_with(\"Bldg_Type_\")\n#&gt; â€¢ Natural splines on: Latitude and Longitude\n\n\n\n1.5.9 è‡ªç„¶è¯­è¨€å¤„ç†\ntextrecipes",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "æœºå™¨å­¦ä¹ "
    ]
  },
  {
    "objectID": "machine_learning.html#æ¨¡å‹è¯„ä¼°-yardstick",
    "href": "machine_learning.html#æ¨¡å‹è¯„ä¼°-yardstick",
    "title": "",
    "section": "\n1.6 æ¨¡å‹è¯„ä¼° yardstick\n",
    "text": "1.6 æ¨¡å‹è¯„ä¼° yardstick\n\n\n1.6.1 å›å½’æŒ‡æ ‡\n\nCodeames_test_res &lt;- predict(lm_fit, new_data = ames_test %&gt;% select(-Sale_Price))\names_test_res\n#&gt; # A tibble: 588 Ã— 1\n#&gt;    .pred\n#&gt;    &lt;dbl&gt;\n#&gt;  1  5.24\n#&gt;  2  5.27\n#&gt;  3  5.24\n#&gt;  4  5.21\n#&gt;  5  5.65\n#&gt;  6  5.13\n#&gt;  7  5.04\n#&gt;  8  5.22\n#&gt;  9  5.00\n#&gt; 10  5.32\n#&gt; # â„¹ 578 more rows\n\names_test_res &lt;- bind_cols(ames_test_res, ames_test %&gt;% select(Sale_Price))\names_test_res\n#&gt; # A tibble: 588 Ã— 2\n#&gt;    .pred Sale_Price\n#&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n#&gt;  1  5.24       5.33\n#&gt;  2  5.27       5.29\n#&gt;  3  5.24       5.26\n#&gt;  4  5.21       5.23\n#&gt;  5  5.65       5.73\n#&gt;  6  5.13       5.17\n#&gt;  7  5.04       5.06\n#&gt;  8  5.22       5.26\n#&gt;  9  5.00       5.02\n#&gt; 10  5.32       5.24\n#&gt; # â„¹ 578 more rows\n\n\n\nCodeggplot(ames_test_res, aes(x = Sale_Price, y = .pred)) + \n  # Create a diagonal line:\n  geom_abline(lty = 2) + \n  geom_point(alpha = 0.5) + \n  labs(y = \"Predicted Sale Price (log10)\", x = \"Sale Price (log10)\") +\n  # Scale and size the x- and y-axis uniformly:\n  coord_obs_pred()\n\n\n\n\n\n\n\n\n1.6.1.1 å‡æ–¹æ ¹è¯¯å·®RMSE\n\nCodermse(ames_test_res, truth = Sale_Price, estimate = .pred)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 rmse    standard      0.0772\n\n\n\n1.6.1.2 å†³å®šç³»æ•°R2ï¼Œå¹³å‡ç»å¯¹è¯¯å·®MAE\n\nCodeames_metrics &lt;- metric_set(rmse, rsq, mae)\names_metrics(ames_test_res, truth = Sale_Price, estimate = .pred)\n#&gt; # A tibble: 3 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 rmse    standard      0.0772\n#&gt; 2 rsq     standard      0.795 \n#&gt; 3 mae     standard      0.0550\n\n\n\n1.6.2 äºŒåˆ†ç±»æŒ‡æ ‡\n\nCodedata(two_class_example)\ntibble(two_class_example)\n#&gt; # A tibble: 500 Ã— 4\n#&gt;    truth   Class1   Class2 predicted\n#&gt;    &lt;fct&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;    \n#&gt;  1 Class2 0.00359 0.996    Class2   \n#&gt;  2 Class1 0.679   0.321    Class1   \n#&gt;  3 Class2 0.111   0.889    Class2   \n#&gt;  4 Class1 0.735   0.265    Class1   \n#&gt;  5 Class2 0.0162  0.984    Class2   \n#&gt;  6 Class1 0.999   0.000725 Class1   \n#&gt;  7 Class1 0.999   0.000799 Class1   \n#&gt;  8 Class1 0.812   0.188    Class1   \n#&gt;  9 Class2 0.457   0.543    Class2   \n#&gt; 10 Class2 0.0976  0.902    Class2   \n#&gt; # â„¹ 490 more rows\n\n\n\nCode# æ··æ·†çŸ©é˜µ\n# A confusion matrix: \nconf_mat(two_class_example, truth = truth, estimate = predicted)\n#&gt;           Truth\n#&gt; Prediction Class1 Class2\n#&gt;     Class1    227     50\n#&gt;     Class2     31    192\n\n\n\nCode# Accuracy:\naccuracy(two_class_example, truth, predicted)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.838\n\n\n\nCode# Matthews correlation coefficient:\nmcc(two_class_example, truth, predicted)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 mcc     binary         0.677\n\n\n\nCode# F1 metric:\nf_meas(two_class_example, truth, predicted)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 f_meas  binary         0.849\n\n\n\nCode# Combining these three classification metrics together\nclassification_metrics &lt;- metric_set(accuracy, mcc, f_meas)\nclassification_metrics(two_class_example, truth = truth, estimate = predicted)\n#&gt; # A tibble: 3 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.838\n#&gt; 2 mcc      binary         0.677\n#&gt; 3 f_meas   binary         0.849\n\n\næ„Ÿå…´è¶£ äº‹ä»¶æ°´å¹³\nç¬¬äºŒçº§é€»è¾‘å°†ç»“æœç¼–ç ä¸º0/1ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç¬¬äºŒä¸ªå€¼æ˜¯äº‹ä»¶ï¼‰\n\nCodef_meas(two_class_example, truth, predicted, event_level = \"second\")\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 f_meas  binary         0.826\n\n\n\n1.6.2.1 ROCï¼ŒAUC\nä¸ä½¿ç”¨é¢„æµ‹ç±»åˆ—,å¯¹äºä¸¤ç±»é—®é¢˜ï¼Œæ„Ÿå…´è¶£äº‹ä»¶çš„æ¦‚ç‡åˆ—å°†ä¼ é€’åˆ°å‡½æ•°ä¸­\n\nCodetwo_class_curve &lt;- roc_curve(two_class_example, truth, Class1)\ntwo_class_curve\n#&gt; # A tibble: 502 Ã— 3\n#&gt;    .threshold specificity sensitivity\n#&gt;         &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n#&gt;  1 -Inf           0                 1\n#&gt;  2    1.79e-7     0                 1\n#&gt;  3    4.50e-6     0.00413           1\n#&gt;  4    5.81e-6     0.00826           1\n#&gt;  5    5.92e-6     0.0124            1\n#&gt;  6    1.22e-5     0.0165            1\n#&gt;  7    1.40e-5     0.0207            1\n#&gt;  8    1.43e-5     0.0248            1\n#&gt;  9    2.38e-5     0.0289            1\n#&gt; 10    3.30e-5     0.0331            1\n#&gt; # â„¹ 492 more rows\nroc_auc(two_class_example, truth, Class1)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 roc_auc binary         0.939\n\n\n\nCodeautoplot(two_class_curve)+\n    annotate(\"text\",x=0.5,y=0.25,label=\"AUC=0.939\")\n\n\n\n\n\n\n\n\n1.6.3 å¤šåˆ†ç±»æŒ‡æ ‡\n\nCodedata(hpc_cv)\ntibble(hpc_cv)\n#&gt; # A tibble: 3,467 Ã— 7\n#&gt;    obs   pred     VF      F       M          L Resample\n#&gt;    &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;   \n#&gt;  1 VF    VF    0.914 0.0779 0.00848 0.0000199  Fold01  \n#&gt;  2 VF    VF    0.938 0.0571 0.00482 0.0000101  Fold01  \n#&gt;  3 VF    VF    0.947 0.0495 0.00316 0.00000500 Fold01  \n#&gt;  4 VF    VF    0.929 0.0653 0.00579 0.0000156  Fold01  \n#&gt;  5 VF    VF    0.942 0.0543 0.00381 0.00000729 Fold01  \n#&gt;  6 VF    VF    0.951 0.0462 0.00272 0.00000384 Fold01  \n#&gt;  7 VF    VF    0.914 0.0782 0.00767 0.0000354  Fold01  \n#&gt;  8 VF    VF    0.918 0.0744 0.00726 0.0000157  Fold01  \n#&gt;  9 VF    VF    0.843 0.128  0.0296  0.000192   Fold01  \n#&gt; 10 VF    VF    0.920 0.0728 0.00703 0.0000147  Fold01  \n#&gt; # â„¹ 3,457 more rows\n\n\n\nCodeaccuracy(hpc_cv, obs, pred)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy multiclass     0.709\nmcc(hpc_cv, obs, pred)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 mcc     multiclass     0.515\n\n\näºŒåˆ†ç±»å¯æ‹“å±•åˆ°å¤šåˆ†ç±»\n\nCodesensitivity(hpc_cv, obs, pred, estimator = \"macro\")\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 sensitivity macro          0.560\nsensitivity(hpc_cv, obs, pred, estimator = \"macro_weighted\")\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator     .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;              &lt;dbl&gt;\n#&gt; 1 sensitivity macro_weighted     0.709\nsensitivity(hpc_cv, obs, pred, estimator = \"micro\")\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 sensitivity micro          0.709\n\n\nå¤šåˆ†ç±»\n\nCoderoc_auc(hpc_cv, obs, VF, F, M, L)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 roc_auc hand_till      0.829\nroc_auc(hpc_cv, obs, VF, F, M, L, estimator = \"macro_weighted\")\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric .estimator     .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;\n#&gt; 1 roc_auc macro_weighted     0.868\n\n\n\nCodehpc_cv %&gt;% \n  group_by(Resample) %&gt;% \n  accuracy(obs, pred)\n#&gt; # A tibble: 10 Ã— 4\n#&gt;    Resample .metric  .estimator .estimate\n#&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt;  1 Fold01   accuracy multiclass     0.726\n#&gt;  2 Fold02   accuracy multiclass     0.712\n#&gt;  3 Fold03   accuracy multiclass     0.758\n#&gt;  4 Fold04   accuracy multiclass     0.712\n#&gt;  5 Fold05   accuracy multiclass     0.712\n#&gt;  6 Fold06   accuracy multiclass     0.697\n#&gt;  7 Fold07   accuracy multiclass     0.675\n#&gt;  8 Fold08   accuracy multiclass     0.721\n#&gt;  9 Fold09   accuracy multiclass     0.673\n#&gt; 10 Fold10   accuracy multiclass     0.699\n\n# Four 1-vs-all ROC curves for each fold\nhpc_cv %&gt;% \n  group_by(Resample) %&gt;% \n  roc_curve(obs, VF, F, M, L) %&gt;% \n  autoplot()",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "æœºå™¨å­¦ä¹ "
    ]
  },
  {
    "objectID": "machine_learning.html#é‡é‡‡æ ·-rsample",
    "href": "machine_learning.html#é‡é‡‡æ ·-rsample",
    "title": "",
    "section": "\n1.7 é‡é‡‡æ · rsample\n",
    "text": "1.7 é‡é‡‡æ · rsample\n\n\nCodeames_rec\n#&gt; \n#&gt; â”€â”€ Recipe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; â”€â”€ Inputs\n#&gt; Number of variables by role\n#&gt; outcome:   1\n#&gt; predictor: 6\n#&gt; \n#&gt; â”€â”€ Operations\n#&gt; â€¢ Log transformation on: Gr_Liv_Area\n#&gt; â€¢ Collapsing factor levels for: Neighborhood\n#&gt; â€¢ Dummy variables from: all_nominal_predictors()\n#&gt; â€¢ Interactions with: Gr_Liv_Area:starts_with(\"Bldg_Type_\")\n#&gt; â€¢ Natural splines on: Latitude and Longitude\n\n\n\nCoderf_model &lt;- \n  rand_forest(trees = 1000) %&gt;% \n  set_engine(\"ranger\") %&gt;% \n  set_mode(\"regression\")\n\nrf_wflow &lt;- \n  workflow() %&gt;% \n  add_formula(\n    Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n      Latitude + Longitude) %&gt;% \n  add_model(rf_model) \n\nrf_fit &lt;- rf_wflow %&gt;% fit(data = ames_train)\n\n\n\nCodeestimate_perf &lt;- function(model, dat) {\n  # Capture the names of the `model` and `dat` objects\n  cl &lt;- match.call()\n  obj_name &lt;- as.character(cl$model)\n  data_name &lt;- as.character(cl$dat)\n  data_name &lt;- gsub(\"ames_\", \"\", data_name)\n  \n  # Estimate these metrics:\n  reg_metrics &lt;- metric_set(rmse, rsq)\n  \n  model %&gt;%\n    predict(dat) %&gt;%\n    bind_cols(dat %&gt;% select(Sale_Price)) %&gt;%\n    reg_metrics(Sale_Price, .pred) %&gt;%\n    select(-.estimator) %&gt;%\n    mutate(object = obj_name, data = data_name)\n}\n\nestimate_perf(rf_fit, ames_train)\n#&gt; # A tibble: 2 Ã— 4\n#&gt;   .metric .estimate object data \n#&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n#&gt; 1 rmse       0.0364 rf_fit train\n#&gt; 2 rsq        0.962  rf_fit train\nestimate_perf(lm_fit, ames_train)\n#&gt; # A tibble: 2 Ã— 4\n#&gt;   .metric .estimate object data \n#&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n#&gt; 1 rmse       0.0749 lm_fit train\n#&gt; 2 rsq        0.824  lm_fit train\n\n\n\nCodeestimate_perf(rf_fit, ames_test)\n#&gt; # A tibble: 2 Ã— 4\n#&gt;   .metric .estimate object data \n#&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n#&gt; 1 rmse       0.0677 rf_fit test \n#&gt; 2 rsq        0.843  rf_fit test\n\n\n\n\n\n\n\n1.7.1 äº¤å‰éªŒè¯\nV-fold cross-validationï¼Œæ•°æ®è¢«éšæœºåˆ’åˆ†ä¸ºæ ·æœ¬é‡å¤§è‡´ç›¸ç­‰çš„Vç»„ï¼ˆç§°ä¸ºæŠ˜å ï¼‰ï¼Œä¾‹å¦‚10é‡äº¤å‰éªŒè¯\n\nCodeset.seed(1001)\n# 10-fold cross-validation\names_folds &lt;- vfold_cv(ames_train, v = 10)\n\n# åˆ†æé›†/è¯„ä¼°é›†\names_folds\n#&gt; #  10-fold cross-validation \n#&gt; # A tibble: 10 Ã— 2\n#&gt;    splits             id    \n#&gt;    &lt;list&gt;             &lt;chr&gt; \n#&gt;  1 &lt;split [2107/235]&gt; Fold01\n#&gt;  2 &lt;split [2107/235]&gt; Fold02\n#&gt;  3 &lt;split [2108/234]&gt; Fold03\n#&gt;  4 &lt;split [2108/234]&gt; Fold04\n#&gt;  5 &lt;split [2108/234]&gt; Fold05\n#&gt;  6 &lt;split [2108/234]&gt; Fold06\n#&gt;  7 &lt;split [2108/234]&gt; Fold07\n#&gt;  8 &lt;split [2108/234]&gt; Fold08\n#&gt;  9 &lt;split [2108/234]&gt; Fold09\n#&gt; 10 &lt;split [2108/234]&gt; Fold10\n\n\n# æ£€ç´¢åˆ†åŒºæ•°æ®\names_folds$splits[[1]] %&gt;% analysis() |&gt; dim()\n#&gt; [1] 2107   74\n\n\nmodel_spec %&gt;% fit_resamples(formula, resamples, ...)\nmodel_spec %&gt;% fit_resamples(recipe, resamples, ...)\nworkflow %&gt;% fit_resamples( resamples, ...)\n\nCodekeep_pred &lt;- control_resamples(save_pred = TRUE, save_workflow = TRUE)\n\nset.seed(1003)\nrf_res &lt;- \n  rf_wflow %&gt;% \n  fit_resamples(resamples = ames_folds, control = keep_pred)\nrf_res\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation \n#&gt; # A tibble: 10 Ã— 5\n#&gt;    splits             id     .metrics         .notes           .predictions\n#&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n#&gt;  1 &lt;split [2107/235]&gt; Fold01 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  2 &lt;split [2107/235]&gt; Fold02 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  3 &lt;split [2108/234]&gt; Fold03 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  4 &lt;split [2108/234]&gt; Fold04 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  5 &lt;split [2108/234]&gt; Fold05 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  6 &lt;split [2108/234]&gt; Fold06 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  7 &lt;split [2108/234]&gt; Fold07 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  8 &lt;split [2108/234]&gt; Fold08 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt;  9 &lt;split [2108/234]&gt; Fold09 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n#&gt; 10 &lt;split [2108/234]&gt; Fold10 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;\ncollect_metrics(rf_res)\n#&gt; # A tibble: 2 Ã— 6\n#&gt;   .metric .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 rmse    standard   0.0726    10 0.00258 Preprocessor1_Model1\n#&gt; 2 rsq     standard   0.834     10 0.0116  Preprocessor1_Model1\ncollect_metrics(rf_res,summarize = F)\n#&gt; # A tibble: 20 Ã— 5\n#&gt;    id     .metric .estimator .estimate .config             \n#&gt;    &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n#&gt;  1 Fold01 rmse    standard      0.0647 Preprocessor1_Model1\n#&gt;  2 Fold01 rsq     standard      0.861  Preprocessor1_Model1\n#&gt;  3 Fold02 rmse    standard      0.0662 Preprocessor1_Model1\n#&gt;  4 Fold02 rsq     standard      0.860  Preprocessor1_Model1\n#&gt;  5 Fold03 rmse    standard      0.0793 Preprocessor1_Model1\n#&gt;  6 Fold03 rsq     standard      0.808  Preprocessor1_Model1\n#&gt;  7 Fold04 rmse    standard      0.0861 Preprocessor1_Model1\n#&gt;  8 Fold04 rsq     standard      0.761  Preprocessor1_Model1\n#&gt;  9 Fold05 rmse    standard      0.0738 Preprocessor1_Model1\n#&gt; 10 Fold05 rsq     standard      0.857  Preprocessor1_Model1\n#&gt; 11 Fold06 rmse    standard      0.0678 Preprocessor1_Model1\n#&gt; 12 Fold06 rsq     standard      0.860  Preprocessor1_Model1\n#&gt; 13 Fold07 rmse    standard      0.0644 Preprocessor1_Model1\n#&gt; 14 Fold07 rsq     standard      0.873  Preprocessor1_Model1\n#&gt; 15 Fold08 rmse    standard      0.0673 Preprocessor1_Model1\n#&gt; 16 Fold08 rsq     standard      0.829  Preprocessor1_Model1\n#&gt; 17 Fold09 rmse    standard      0.0719 Preprocessor1_Model1\n#&gt; 18 Fold09 rsq     standard      0.840  Preprocessor1_Model1\n#&gt; 19 Fold10 rmse    standard      0.0848 Preprocessor1_Model1\n#&gt; 20 Fold10 rsq     standard      0.790  Preprocessor1_Model1\nassess_res &lt;- collect_predictions(rf_res)\nassess_res\n#&gt; # A tibble: 2,342 Ã— 5\n#&gt;    .pred id      .row Sale_Price .config             \n#&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;               \n#&gt;  1  5.17 Fold01    10       5.05 Preprocessor1_Model1\n#&gt;  2  5.04 Fold01    27       5.06 Preprocessor1_Model1\n#&gt;  3  5.11 Fold01    47       5.10 Preprocessor1_Model1\n#&gt;  4  5.14 Fold01    52       5.11 Preprocessor1_Model1\n#&gt;  5  5.09 Fold01    59       5.05 Preprocessor1_Model1\n#&gt;  6  4.88 Fold01    63       4.77 Preprocessor1_Model1\n#&gt;  7  5.01 Fold01    65       5.10 Preprocessor1_Model1\n#&gt;  8  5.16 Fold01    66       5    Preprocessor1_Model1\n#&gt;  9  4.96 Fold01    67       5.10 Preprocessor1_Model1\n#&gt; 10  4.84 Fold01    68       4.91 Preprocessor1_Model1\n#&gt; # â„¹ 2,332 more rows\n\n\n\nCodeassess_res %&gt;% \n  ggplot(aes(x = Sale_Price, y = .pred)) + \n  geom_point(alpha = .15) +\n  geom_abline(color = \"red\") + \n  coord_obs_pred() + \n  ylab(\"Predicted\")\n\n\n\n\n\n\n\n\nCode# æ‰¾åˆ°æ®‹å·®æœ€å¤§çš„2ä¸ª\nover_predicted &lt;- \n  assess_res %&gt;% \n  mutate(residual = Sale_Price - .pred) %&gt;% \n  arrange(desc(abs(residual))) %&gt;% \n  slice(1:2)\nover_predicted\n#&gt; # A tibble: 2 Ã— 6\n#&gt;   .pred id      .row Sale_Price .config              residual\n#&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;                   &lt;dbl&gt;\n#&gt; 1  4.97 Fold10    33       4.11 Preprocessor1_Model1   -0.858\n#&gt; 2  4.93 Fold04   323       4.12 Preprocessor1_Model1   -0.814\n\names_train %&gt;% \n  slice(over_predicted$.row) %&gt;% \n  select(Gr_Liv_Area, Neighborhood, Year_Built, Bedroom_AbvGr, Full_Bath)\n#&gt; # A tibble: 2 Ã— 5\n#&gt;   Gr_Liv_Area Neighborhood           Year_Built Bedroom_AbvGr Full_Bath\n#&gt;         &lt;int&gt; &lt;fct&gt;                       &lt;int&gt;         &lt;int&gt;     &lt;int&gt;\n#&gt; 1         832 Old_Town                     1923             2         1\n#&gt; 2         733 Iowa_DOT_and_Rail_Road       1952             2         1\n\n\né‡å¤äº¤å‰éªŒè¯\n\nCode# 10-fold cross-validation repeated 5 times \nvfold_cv(ames_train, v = 10, repeats = 5)\n#&gt; #  10-fold cross-validation repeated 5 times \n#&gt; # A tibble: 50 Ã— 3\n#&gt;    splits             id      id2   \n#&gt;    &lt;list&gt;             &lt;chr&gt;   &lt;chr&gt; \n#&gt;  1 &lt;split [2107/235]&gt; Repeat1 Fold01\n#&gt;  2 &lt;split [2107/235]&gt; Repeat1 Fold02\n#&gt;  3 &lt;split [2108/234]&gt; Repeat1 Fold03\n#&gt;  4 &lt;split [2108/234]&gt; Repeat1 Fold04\n#&gt;  5 &lt;split [2108/234]&gt; Repeat1 Fold05\n#&gt;  6 &lt;split [2108/234]&gt; Repeat1 Fold06\n#&gt;  7 &lt;split [2108/234]&gt; Repeat1 Fold07\n#&gt;  8 &lt;split [2108/234]&gt; Repeat1 Fold08\n#&gt;  9 &lt;split [2108/234]&gt; Repeat1 Fold09\n#&gt; 10 &lt;split [2108/234]&gt; Repeat1 Fold10\n#&gt; # â„¹ 40 more rows\n\n\nç•™ä¸€äº¤å‰éªŒè¯\nleave-one-out (LOO) cross-validation\nloo_cv()\nè’™ç‰¹å¡ç½—äº¤å‰éªŒè¯\nMonte Carlo cross-validationï¼ŒMCCVï¼Œå°†å›ºå®šæ¯”ä¾‹çš„æ•°æ®åˆ†é…ç»™åˆ†æé›†å’Œè¯„ä¼°é›†ã€‚è¯¥æ¯”ä¾‹çš„æ•°æ®æ¯æ¬¡éƒ½æ˜¯éšæœºé€‰æ‹©çš„ï¼Œå¯¼è‡´è¯„ä¼°é›†ä¸ç›¸äº’æ’æ–¥\n\nCodemc_cv(ames_train, prop = 9/10, times = 20)\n#&gt; # Monte Carlo cross-validation (0.9/0.1) with 20 resamples  \n#&gt; # A tibble: 20 Ã— 2\n#&gt;    splits             id        \n#&gt;    &lt;list&gt;             &lt;chr&gt;     \n#&gt;  1 &lt;split [2107/235]&gt; Resample01\n#&gt;  2 &lt;split [2107/235]&gt; Resample02\n#&gt;  3 &lt;split [2107/235]&gt; Resample03\n#&gt;  4 &lt;split [2107/235]&gt; Resample04\n#&gt;  5 &lt;split [2107/235]&gt; Resample05\n#&gt;  6 &lt;split [2107/235]&gt; Resample06\n#&gt;  7 &lt;split [2107/235]&gt; Resample07\n#&gt;  8 &lt;split [2107/235]&gt; Resample08\n#&gt;  9 &lt;split [2107/235]&gt; Resample09\n#&gt; 10 &lt;split [2107/235]&gt; Resample10\n#&gt; 11 &lt;split [2107/235]&gt; Resample11\n#&gt; 12 &lt;split [2107/235]&gt; Resample12\n#&gt; 13 &lt;split [2107/235]&gt; Resample13\n#&gt; 14 &lt;split [2107/235]&gt; Resample14\n#&gt; 15 &lt;split [2107/235]&gt; Resample15\n#&gt; 16 &lt;split [2107/235]&gt; Resample16\n#&gt; 17 &lt;split [2107/235]&gt; Resample17\n#&gt; 18 &lt;split [2107/235]&gt; Resample18\n#&gt; 19 &lt;split [2107/235]&gt; Resample19\n#&gt; 20 &lt;split [2107/235]&gt; Resample20\n\n\n\n1.7.2 éªŒè¯é›†\n\nCodeset.seed(101)\n\n# To put 60% into training, 20% in validation, and remaining 20% in testing:\names_validation_split &lt;- initial_validation_split(ames, prop = c(0.6, 0.2))\names_validation_split\n#&gt; &lt;Training/Validation/Testing/Total&gt;\n#&gt; &lt;1758/586/586/2930&gt;\n\n\n# Object used for resampling: \nval_set &lt;- validation_set(ames_validation_split)\nval_set\n#&gt; # A tibble: 1 Ã— 2\n#&gt;   splits             id        \n#&gt;   &lt;list&gt;             &lt;chr&gt;     \n#&gt; 1 &lt;split [1758/586]&gt; validation\n\n\n\nCodeval_res &lt;- rf_wflow %&gt;% fit_resamples(resamples = val_set)\nval_res\n#&gt; # Resampling results\n#&gt; # Validation Set (0.75/0.25) \n#&gt; # A tibble: 1 Ã— 4\n#&gt;   splits             id         .metrics         .notes          \n#&gt;   &lt;list&gt;             &lt;chr&gt;      &lt;list&gt;           &lt;list&gt;          \n#&gt; 1 &lt;split [1758/586]&gt; validation &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt;\ncollect_metrics(val_res)\n#&gt; # A tibble: 2 Ã— 6\n#&gt;   .metric .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 rmse    standard   0.0809     1      NA Preprocessor1_Model1\n#&gt; 2 rsq     standard   0.818      1      NA Preprocessor1_Model1\n\n\n\n1.7.3 è‡ªåŠ©æ³•\nBootstrap resampling\nreplacement\nout-of-bag sample\n\nCodebootstraps(ames_train, times = 5)\n#&gt; # Bootstrap sampling \n#&gt; # A tibble: 5 Ã— 2\n#&gt;   splits             id        \n#&gt;   &lt;list&gt;             &lt;chr&gt;     \n#&gt; 1 &lt;split [2342/882]&gt; Bootstrap1\n#&gt; 2 &lt;split [2342/849]&gt; Bootstrap2\n#&gt; 3 &lt;split [2342/870]&gt; Bootstrap3\n#&gt; 4 &lt;split [2342/875]&gt; Bootstrap4\n#&gt; 5 &lt;split [2342/849]&gt; Bootstrap5\n\n\n\n1.7.4 Rolling forecast origin resampling\næ»šåŠ¨é¢„æµ‹åŸç‚¹é‡é‡‡æ ·\næ—¶é—´åºåˆ—æ•°æ®\n\nCodetime_slices &lt;- \n  tibble(x = 1:365) %&gt;% \n  rolling_origin(initial = 6 * 30, assess = 30, skip = 29, cumulative = FALSE)\n\ndata_range &lt;- function(x) {\n  summarize(x, first = min(x), last = max(x))\n}\n\nmap_dfr(time_slices$splits, ~   analysis(.x) %&gt;% data_range())\n#&gt; # A tibble: 6 Ã— 2\n#&gt;   first  last\n#&gt;   &lt;int&gt; &lt;int&gt;\n#&gt; 1     1   180\n#&gt; 2    31   210\n#&gt; 3    61   240\n#&gt; 4    91   270\n#&gt; 5   121   300\n#&gt; 6   151   330\n\nmap_dfr(time_slices$splits, ~ assessment(.x) %&gt;% data_range())\n#&gt; # A tibble: 6 Ã— 2\n#&gt;   first  last\n#&gt;   &lt;int&gt; &lt;int&gt;\n#&gt; 1   181   210\n#&gt; 2   211   240\n#&gt; 3   241   270\n#&gt; 4   271   300\n#&gt; 5   301   330\n#&gt; 6   331   360\n\n\n\n1.7.5 å¹¶è¡Œè®¡ç®—\n\nCodeparallel::detectCores(logical = FALSE)\n#&gt; [1] 4\nparallel::detectCores(logical = TRUE)\n#&gt; [1] 8\n\n\n\n1.7.6 ä¿å­˜é‡é‡‡æ ·å¯¹è±¡\n\nCodeames_rec &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %&gt;%\n  step_other(Neighborhood, threshold = 0.01) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %&gt;% \n  step_ns(Latitude, Longitude, deg_free = 20)\n\nlm_wflow &lt;-  \n  workflow() %&gt;% \n  add_recipe(ames_rec) %&gt;% \n  add_model(linear_reg() %&gt;% set_engine(\"lm\")) \n\nlm_fit &lt;- lm_wflow %&gt;% fit(data = ames_train)\n\n# Select the recipe: \nextract_recipe(lm_fit, estimated = TRUE)\n#&gt; \n#&gt; â”€â”€ Recipe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; \n#&gt; â”€â”€ Inputs\n#&gt; Number of variables by role\n#&gt; outcome:   1\n#&gt; predictor: 6\n#&gt; \n#&gt; â”€â”€ Training information\n#&gt; Training data contained 2342 data points and no incomplete rows.\n#&gt; \n#&gt; â”€â”€ Operations\n#&gt; â€¢ Collapsing factor levels for: Neighborhood | Trained\n#&gt; â€¢ Dummy variables from: Neighborhood and Bldg_Type | Trained\n#&gt; â€¢ Interactions with: Gr_Liv_Area:(Bldg_Type_TwoFmCon + Bldg_Type_Duplex +\n#&gt;   Bldg_Type_Twnhs + Bldg_Type_TwnhsE) | Trained\n#&gt; â€¢ Natural splines on: Latitude and Longitude | Trained\n\n\n\nCodeget_model &lt;- function(x) {\n  extract_fit_parsnip(x) %&gt;% tidy()\n}\n\nget_model(lm_fit)\n#&gt; # A tibble: 72 Ã— 5\n#&gt;    term                             estimate  std.error statistic   p.value\n#&gt;    &lt;chr&gt;                               &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1 (Intercept)                      0.895    0.303          2.95  3.17e-  3\n#&gt;  2 Gr_Liv_Area                      0.000177 0.00000444    39.8   4.16e-263\n#&gt;  3 Year_Built                       0.00204  0.000139      14.7   1.51e- 46\n#&gt;  4 Neighborhood_College_Creek      -0.0283   0.0334        -0.849 3.96e-  1\n#&gt;  5 Neighborhood_Old_Town           -0.0489   0.0125        -3.91  9.48e-  5\n#&gt;  6 Neighborhood_Edwards            -0.0823   0.0273        -3.01  2.62e-  3\n#&gt;  7 Neighborhood_Somerset            0.0735   0.0192         3.83  1.33e-  4\n#&gt;  8 Neighborhood_Northridge_Heights  0.148    0.0278         5.33  1.07e-  7\n#&gt;  9 Neighborhood_Gilbert             0.0306   0.0218         1.41  1.60e-  1\n#&gt; 10 Neighborhood_Sawyer             -0.111    0.0257        -4.33  1.56e-  5\n#&gt; # â„¹ 62 more rows\n\n\n\nCodectrl &lt;- control_resamples(extract = get_model)\n\nlm_res &lt;- lm_wflow %&gt;%  fit_resamples(resamples = ames_folds, control = ctrl)\nlm_res\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation \n#&gt; # A tibble: 10 Ã— 5\n#&gt;    splits             id     .metrics         .notes           .extracts       \n#&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;          \n#&gt;  1 &lt;split [2107/235]&gt; Fold01 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [1 Ã— 2]&gt;\n#&gt;  2 &lt;split [2107/235]&gt; Fold02 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [1 Ã— 2]&gt;\n#&gt;  3 &lt;split [2108/234]&gt; Fold03 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [1 Ã— 2]&gt;\n#&gt;  4 &lt;split [2108/234]&gt; Fold04 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [1 Ã— 2]&gt;\n#&gt;  5 &lt;split [2108/234]&gt; Fold05 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [1 Ã— 2]&gt;\n#&gt;  6 &lt;split [2108/234]&gt; Fold06 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [1 Ã— 2]&gt;\n#&gt;  7 &lt;split [2108/234]&gt; Fold07 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [1 Ã— 2]&gt;\n#&gt;  8 &lt;split [2108/234]&gt; Fold08 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [1 Ã— 2]&gt;\n#&gt;  9 &lt;split [2108/234]&gt; Fold09 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [1 Ã— 2]&gt;\n#&gt; 10 &lt;split [2108/234]&gt; Fold10 &lt;tibble [2 Ã— 4]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble [1 Ã— 2]&gt;\n\n\n\nCodelm_res$.extracts[[1]]\n#&gt; # A tibble: 1 Ã— 2\n#&gt;   .extracts         .config             \n#&gt;   &lt;list&gt;            &lt;chr&gt;               \n#&gt; 1 &lt;tibble [72 Ã— 5]&gt; Preprocessor1_Model1\n\nlm_res$.extracts[[1]][[1]]\n#&gt; [[1]]\n#&gt; # A tibble: 72 Ã— 5\n#&gt;    term                             estimate  std.error statistic   p.value\n#&gt;    &lt;chr&gt;                               &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1 (Intercept)                      0.950    0.324          2.93  3.42e-  3\n#&gt;  2 Gr_Liv_Area                      0.000175 0.00000481    36.3   4.09e-223\n#&gt;  3 Year_Built                       0.00201  0.000149      13.5   1.08e- 39\n#&gt;  4 Neighborhood_College_Creek      -0.0273   0.0359        -0.762 4.46e-  1\n#&gt;  5 Neighborhood_Old_Town           -0.0543   0.0134        -4.04  5.51e-  5\n#&gt;  6 Neighborhood_Edwards            -0.0825   0.0293        -2.81  4.95e-  3\n#&gt;  7 Neighborhood_Somerset            0.0706   0.0207         3.40  6.76e-  4\n#&gt;  8 Neighborhood_Northridge_Heights  0.139    0.0298         4.65  3.55e-  6\n#&gt;  9 Neighborhood_Gilbert             0.0197   0.0235         0.835 4.04e-  1\n#&gt; 10 Neighborhood_Sawyer             -0.117    0.0273        -4.30  1.77e-  5\n#&gt; # â„¹ 62 more rows\n\n\n\nCodeall_coef &lt;- map_dfr(lm_res$.extracts, ~ .x[[1]][[1]])\n# Show the replicates for a single predictor:\nfilter(all_coef, term == \"Year_Built\")\n#&gt; # A tibble: 10 Ã— 5\n#&gt;    term       estimate std.error statistic  p.value\n#&gt;    &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1 Year_Built  0.00201  0.000149      13.5 1.08e-39\n#&gt;  2 Year_Built  0.00203  0.000152      13.3 6.61e-39\n#&gt;  3 Year_Built  0.00189  0.000147      12.9 1.85e-36\n#&gt;  4 Year_Built  0.00210  0.000145      14.5 1.98e-45\n#&gt;  5 Year_Built  0.00211  0.000148      14.3 2.19e-44\n#&gt;  6 Year_Built  0.00204  0.000147      13.9 4.52e-42\n#&gt;  7 Year_Built  0.00215  0.000150      14.3 1.97e-44\n#&gt;  8 Year_Built  0.00213  0.000144      14.7 9.43e-47\n#&gt;  9 Year_Built  0.00208  0.000151      13.8 1.77e-41\n#&gt; 10 Year_Built  0.00212  0.000148      14.3 4.48e-44",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "æœºå™¨å­¦ä¹ "
    ]
  },
  {
    "objectID": "predictive_models.html",
    "href": "predictive_models.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\né¢„æµ‹æ€§æ¨¡å‹\né¢„æµ‹æ€§æ¨¡å‹ï¼ˆPredictive Modelsï¼‰æ˜¯æŒ‡åˆ©ç”¨ç»Ÿè®¡å­¦ã€æ•°å­¦ã€è®¡ç®—æœºç§‘å­¦ç­‰æ–¹æ³•ï¼Œå¯¹å·²çŸ¥æ•°æ®è¿›è¡Œåˆ†æå’Œå»ºæ¨¡ï¼Œä»è€Œå¯¹æœªæ¥æˆ–æœªçŸ¥æ•°æ®è¿›è¡Œé¢„æµ‹çš„æ¨¡å‹ã€‚\né¢„æµ‹æ€§æ¨¡å‹å¯ä»¥æ ¹æ®å…¶æ„å»ºæ–¹æ³•å’Œåº”ç”¨é¢†åŸŸè¿›è¡Œåˆ†ç±»ï¼š\n\nåŸºäºç»Ÿè®¡å­¦çš„æ–¹æ³•ï¼š\n\nå›å½’åˆ†æï¼šå¦‚çº¿æ€§å›å½’ã€éçº¿æ€§å›å½’ã€é€»è¾‘å›å½’ç­‰ï¼Œç”¨äºå»ºæ¨¡å’Œé¢„æµ‹å˜é‡ä¹‹é—´çš„å…³ç³»ã€‚\næ—¶é—´åºåˆ—åˆ†æï¼šå¦‚ARIMAæ¨¡å‹ã€æŒ‡æ•°å¹³æ»‘æ³•ï¼Œç”¨äºåˆ†æå’Œé¢„æµ‹æ—¶é—´åºåˆ—æ•°æ®ã€‚\n\nåŸºäºç»éªŒé©±åŠ¨çš„æ¨¡å‹ï¼ˆEmpirically Driven Modelsï¼‰ï¼Œå¦‚æœºå™¨å­¦ä¹ \n\nç›‘ç£å­¦ä¹ ï¼šå¦‚æ”¯æŒå‘é‡æœºã€ç¥ç»ç½‘ç»œã€K-æœ€è¿‘é‚»ã€å†³ç­–æ ‘ã€éšæœºæ£®æ—ç­‰ï¼Œé€šè¿‡è®­ç»ƒæ•°æ®å»ºç«‹é¢„æµ‹æ¨¡å‹ã€‚\næ— ç›‘ç£å­¦ä¹ ï¼šå¦‚èšç±»åˆ†æï¼Œç”¨äºå‘ç°æ•°æ®ä¸­çš„æ½œåœ¨æ¨¡å¼å’Œç»“æ„ã€‚\næ·±åº¦å­¦ä¹ ï¼šå¦‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ç­‰ï¼Œç”¨äºå¤„ç†å¤æ‚çš„é«˜ç»´æ•°æ®å’Œéçº¿æ€§å…³ç³»ã€‚\n\nåŸºäºç¬¬ä¸€æ€§åŸç†ï¼ˆFirst Principlesï¼‰çš„æœºç†æ¨¡å‹ï¼ˆMechanistic Modelsï¼‰ï¼š\n\né‡å­åŠ›å­¦è®¡ç®—ï¼šå¦‚å¯†åº¦æ³›å‡½ç†è®ºï¼ˆDFTï¼‰ã€åˆ†å­åŠ¨åŠ›å­¦ï¼ˆMDï¼‰ç­‰ï¼Œä»åŸºæœ¬ç‰©ç†å®šå¾‹å‡ºå‘ï¼Œé¢„æµ‹åˆ†å­å’Œææ–™çš„æ€§è´¨å’Œè¡Œä¸ºã€‚\nå¤šå°ºåº¦å»ºæ¨¡ï¼šç»“åˆä¸åŒå°ºåº¦çš„æ¨¡å‹ï¼ˆå¦‚åŸå­å°ºåº¦ã€åˆ†å­å°ºåº¦ã€å®è§‚å°ºåº¦ï¼‰ï¼Œå®ç°ä»å¾®è§‚åˆ°å®è§‚çš„ç»¼åˆé¢„æµ‹ã€‚\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "**é¢„æµ‹æ€§æ¨¡å‹**"
    ]
  },
  {
    "objectID": "survival.html",
    "href": "survival.html",
    "title": "",
    "section": "",
    "text": "ç›‘ç£å­¦ä¹ ï¼ˆSupervised Learningï¼‰ç”Ÿå­˜æ¨¡å‹ CodeShow All CodeHide All CodeView Source\n0.1 ç”Ÿå­˜æ¨¡å‹\nhttps://censored.tidymodels.org/articles/examples.html\ntype = \"time\"  type = \"survival\"   type = \"linear_pred\"   type = \"quantile\"   type = \"hazard\"\n\nCodelibrary(tidymodels)\nlibrary(censored)\n\nparametric_spec &lt;- survival_reg()\n\nparametric_workflow &lt;- \n  workflow() %&gt;% \n  add_variables(outcome = c(fustat, futime), predictors = c(age, rx)) %&gt;% \n  add_model(parametric_spec, \n            formula = Surv(futime, fustat) ~ age + strata(rx))\n\nparametric_fit &lt;- fit(parametric_workflow, data = ovarian)\nparametric_fit\n#&gt; â•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#&gt; Preprocessor: Variables\n#&gt; Model: survival_reg()\n#&gt; \n#&gt; â”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Outcomes: c(fustat, futime)\n#&gt; Predictors: c(age, rx)\n#&gt; \n#&gt; â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Call:\n#&gt; survival::survreg(formula = Surv(futime, fustat) ~ age + strata(rx), \n#&gt;     data = data, model = TRUE)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)         age \n#&gt;  12.8734120  -0.1033569 \n#&gt; \n#&gt; Scale:\n#&gt;      rx=1      rx=2 \n#&gt; 0.7695509 0.4703602 \n#&gt; \n#&gt; Loglik(model)= -89.4   Loglik(intercept only)= -97.1\n#&gt;  Chisq= 15.36 on 1 degrees of freedom, p= 8.88e-05 \n#&gt; n= 26\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "ç›‘ç£å­¦ä¹ ï¼ˆSupervised Learningï¼‰",
      "ç”Ÿå­˜æ¨¡å‹"
    ]
  },
  {
    "objectID": "tree-based_models.html",
    "href": "tree-based_models.html",
    "title": "",
    "section": "",
    "text": "ç›‘ç£å­¦ä¹ ï¼ˆSupervised Learningï¼‰åŸºäºæ ‘çš„æ¨¡å‹ CodeShow All CodeHide All CodeView Source",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "ç›‘ç£å­¦ä¹ ï¼ˆSupervised Learningï¼‰",
      "åŸºäºæ ‘çš„æ¨¡å‹"
    ]
  },
  {
    "objectID": "tree-based_models.html#å†³ç­–æ ‘-åˆ†ç±»",
    "href": "tree-based_models.html#å†³ç­–æ ‘-åˆ†ç±»",
    "title": "",
    "section": "\n1.1 å†³ç­–æ ‘ ï¼ˆåˆ†ç±»ï¼‰",
    "text": "1.1 å†³ç­–æ ‘ ï¼ˆåˆ†ç±»ï¼‰\n\nCodedf &lt;- read_csv(\"data/breast-cancer-wisconsin.data\",col_names = F,na = c(\"\",\"NA\",\"?\"))\n#&gt; Rows: 699 Columns: 11\n#&gt; â”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#&gt; Delimiter: \",\"\n#&gt; dbl (11): X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11\n#&gt; \n#&gt; â„¹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; â„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncolnames(df) &lt;- c(\"id\",\"è‚¿å—åšåº¦\",\"ç»†èƒå¤§å°å‡åŒ€æ€§\",\"ç»†èƒå½¢çŠ¶å‡åŒ€æ€§\",\"è¾¹é™…é™„ç€åŠ›\",\"å•ä¸ªä¸Šçš®ç»†èƒå¤§å°\",\n               \"è£¸æ ¸\",\"bland_chromatin\",\"æ­£å¸¸æ ¸\",\"æœ‰ä¸åˆ†è£‚\",\"class\")\n\ndf &lt;- df |&gt;\n    select(-1) |&gt;\n    mutate(class = factor(class, levels = c(2, 4), labels = c(\"è‰¯æ€§\", \"æ¶æ€§\"))) |&gt;\n    drop_na()\nglimpse(df)\n#&gt; Rows: 683\n#&gt; Columns: 10\n#&gt; $ è‚¿å—åšåº¦         &lt;dbl&gt; 5, 5, 3, 6, 4, 8, 1, 2, 2, 4, 1, 2, 5, 1, 8, 7, 4, 4,â€¦\n#&gt; $ ç»†èƒå¤§å°å‡åŒ€æ€§   &lt;dbl&gt; 1, 4, 1, 8, 1, 10, 1, 1, 1, 2, 1, 1, 3, 1, 7, 4, 1, 1â€¦\n#&gt; $ ç»†èƒå½¢çŠ¶å‡åŒ€æ€§   &lt;dbl&gt; 1, 4, 1, 8, 1, 10, 1, 2, 1, 1, 1, 1, 3, 1, 5, 6, 1, 1â€¦\n#&gt; $ è¾¹é™…é™„ç€åŠ›       &lt;dbl&gt; 1, 5, 1, 1, 3, 8, 1, 1, 1, 1, 1, 1, 3, 1, 10, 4, 1, 1â€¦\n#&gt; $ å•ä¸ªä¸Šçš®ç»†èƒå¤§å° &lt;dbl&gt; 2, 7, 2, 3, 2, 7, 2, 2, 2, 2, 1, 2, 2, 2, 7, 6, 2, 2,â€¦\n#&gt; $ è£¸æ ¸             &lt;dbl&gt; 1, 10, 2, 4, 1, 10, 10, 1, 1, 1, 1, 1, 3, 3, 9, 1, 1,â€¦\n#&gt; $ bland_chromatin  &lt;dbl&gt; 3, 3, 3, 3, 3, 9, 3, 3, 1, 2, 3, 2, 4, 3, 5, 4, 2, 3,â€¦\n#&gt; $ æ­£å¸¸æ ¸           &lt;dbl&gt; 1, 2, 1, 7, 1, 7, 1, 1, 1, 1, 1, 1, 4, 1, 5, 3, 1, 1,â€¦\n#&gt; $ æœ‰ä¸åˆ†è£‚         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 4, 1, 1, 1,â€¦\n#&gt; $ class            &lt;fct&gt; è‰¯æ€§, è‰¯æ€§, è‰¯æ€§, è‰¯æ€§, è‰¯æ€§, æ¶æ€§, è‰¯æ€§, è‰¯æ€§, è‰¯æ€§,â€¦\n\n\n\nCode# 2=è‰¯æ€§ 4=æ¶æ€§\ntable(df$class)\n#&gt; \n#&gt; è‰¯æ€§ æ¶æ€§ \n#&gt;  444  239\n# æ‹†åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†         ####\nset.seed(100)\nsplit &lt;- initial_split(df, prop = 0.70, strata = class)\n\nsplit\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;477/206/683&gt;\ntrain &lt;- training(split)\ntest  &lt;-  testing(split)\n\ntable(train$class)\n#&gt; \n#&gt; è‰¯æ€§ æ¶æ€§ \n#&gt;  310  167\ntable(test$class)\n#&gt; \n#&gt; è‰¯æ€§ æ¶æ€§ \n#&gt;  134   72\n\n\n\nCodeclass_tree_spec &lt;- decision_tree() %&gt;%\n    set_engine(\"rpart\") %&gt;%\n    set_mode(\"classification\") \n\ncdtree &lt;- class_tree_spec |&gt; fit(class ~ . ,data = train)\ncdtree\n#&gt; parsnip model object\n#&gt; \n#&gt; n= 477 \n#&gt; \n#&gt; node), split, n, loss, yval, (yprob)\n#&gt;       * denotes terminal node\n#&gt; \n#&gt;  1) root 477 167 è‰¯æ€§ (0.64989518 0.35010482)  \n#&gt;    2) ç»†èƒå¤§å°å‡åŒ€æ€§&lt; 2.5 293  11 è‰¯æ€§ (0.96245734 0.03754266)  \n#&gt;      4) è£¸æ ¸&lt; 5.5 285   4 è‰¯æ€§ (0.98596491 0.01403509) *\n#&gt;      5) è£¸æ ¸&gt;=5.5 8   1 æ¶æ€§ (0.12500000 0.87500000) *\n#&gt;    3) ç»†èƒå¤§å°å‡åŒ€æ€§&gt;=2.5 184  28 æ¶æ€§ (0.15217391 0.84782609)  \n#&gt;      6) ç»†èƒå½¢çŠ¶å‡åŒ€æ€§&lt; 2.5 16   3 è‰¯æ€§ (0.81250000 0.18750000) *\n#&gt;      7) ç»†èƒå½¢çŠ¶å‡åŒ€æ€§&gt;=2.5 168  15 æ¶æ€§ (0.08928571 0.91071429)  \n#&gt;       14) ç»†èƒå¤§å°å‡åŒ€æ€§&lt; 4.5 46  12 æ¶æ€§ (0.26086957 0.73913043)  \n#&gt;         28) è£¸æ ¸&lt; 2.5 10   3 è‰¯æ€§ (0.70000000 0.30000000) *\n#&gt;         29) è£¸æ ¸&gt;=2.5 36   5 æ¶æ€§ (0.13888889 0.86111111) *\n#&gt;       15) ç»†èƒå¤§å°å‡åŒ€æ€§&gt;=4.5 122   3 æ¶æ€§ (0.02459016 0.97540984) *\n\n\n\n1.1.1 æ¨¡å‹å¯è§†åŒ–\n\nCoderpart::plotcp(cdtree$fit)\n\n\n\n\n\n\nCodecdtree %&gt;%\n    extract_fit_engine() %&gt;%\n    rpart.plot::rpart.plot(roundint = F)\n\n\n\n\n\n\n\n\n1.1.2 æ¨¡å‹æ€§èƒ½è¯„ä¼°\n\nCodeaugment(cdtree, new_data = test) %&gt;%\n    accuracy(truth = class, estimate = .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.971\n\naugment(cdtree, new_data = test) %&gt;%\n    conf_mat(truth = class, estimate = .pred_class)\n#&gt;           Truth\n#&gt; Prediction è‰¯æ€§ æ¶æ€§\n#&gt;       è‰¯æ€§  132    4\n#&gt;       æ¶æ€§    2   68",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "ç›‘ç£å­¦ä¹ ï¼ˆSupervised Learningï¼‰",
      "åŸºäºæ ‘çš„æ¨¡å‹"
    ]
  },
  {
    "objectID": "tree-based_models.html#éšæœºæ£®æ—-åˆ†ç±»",
    "href": "tree-based_models.html#éšæœºæ£®æ—-åˆ†ç±»",
    "title": "",
    "section": "\n1.2 éšæœºæ£®æ— ï¼ˆåˆ†ç±»ï¼‰",
    "text": "1.2 éšæœºæ£®æ— ï¼ˆåˆ†ç±»ï¼‰\néšæœºæ£®æ—æ˜¯è£…è¢‹æ³•çš„ä¸€ç§æ‰©å±•ï¼Œå®ƒä¸ä»…å¯¹æ•°æ®è¿›è¡ŒéšæœºæŠ½æ ·ï¼Œè¿˜å¯¹ç‰¹å¾è¿›è¡ŒéšæœºæŠ½æ ·ï¼Œä»¥æ­¤å¢åŠ æ¨¡å‹çš„å¤šæ ·æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚éšæœºæ£®æ—ç”±å¤§é‡å†³ç­–æ ‘ç»„æˆï¼Œæ¯æ£µæ ‘éƒ½æ˜¯åœ¨ä¸€ä¸ªéšæœºæŠ½å–çš„æ ·æœ¬å’Œç‰¹å¾å­é›†ä¸Šè®­ç»ƒçš„ã€‚\n\nCodecf_spec_class &lt;-\n    rand_forest(#mtry = .cols(), \n        trees = 500 ,min_n = 1) %&gt;%\n    set_engine('randomForest', importance = TRUE) %&gt;%\n    set_mode('classification')\n\nclass_rf_fit &lt;- cf_spec_class |&gt; \n    fit(class ~ . , data = train)\nclass_rf_fit\n#&gt; parsnip model object\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt;  randomForest(x = maybe_data_frame(x), y = y, ntree = ~500, nodesize = min_rows(~1,      x), importance = ~TRUE) \n#&gt;                Type of random forest: classification\n#&gt;                      Number of trees: 500\n#&gt; No. of variables tried at each split: 3\n#&gt; \n#&gt;         OOB estimate of  error rate: 3.77%\n#&gt; Confusion matrix:\n#&gt;      è‰¯æ€§ æ¶æ€§ class.error\n#&gt; è‰¯æ€§  300   10  0.03225806\n#&gt; æ¶æ€§    8  159  0.04790419\n\n\n\n1.2.1 ç‰¹å¾é‡è¦æ€§ï¼šåŸºäºGiniç³»æ•°çš„å‡å°‘\nOOB ï¼Œout of bag è¢‹å¤–é¢„æµ‹è¯¯å·®\nè¿™ç§æ–¹æ³•ä¸»è¦ç”¨äºåˆ†ç±»ä»»åŠ¡ã€‚\nMean Decrease in Gini (MDG) è¿™ç§æ–¹æ³•é€šè¿‡è¡¡é‡æŸä¸ªç‰¹å¾å¯¹åˆ†ç±»çº¯åº¦çš„è´¡çŒ®æ¥è®¡ç®—å…¶é‡è¦æ€§ã€‚å…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š\n\nè®­ç»ƒæ¨¡å‹ï¼šä½¿ç”¨æ‰€æœ‰ç‰¹å¾è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹ã€‚\nè®¡ç®—Giniç³»æ•°ï¼šåœ¨å†³ç­–æ ‘ä¸­ï¼Œæ¯æ¬¡èŠ‚ç‚¹åˆ†è£‚éƒ½ä¼šè®¡ç®—Giniç³»æ•°å‡å°‘é‡ã€‚Giniç³»æ•°ç”¨äºè¡¡é‡æ•°æ®é›†çš„çº¯åº¦ï¼Œè¶Šä½è¡¨ç¤ºè¶Šçº¯ã€‚\nç´¯åŠ Giniå‡å°‘é‡ï¼šåœ¨æ¯æ£µæ ‘ä¸­ï¼Œè®¡ç®—æ¯ä¸ªç‰¹å¾åœ¨åˆ†è£‚è¿‡ç¨‹ä¸­å¸¦æ¥çš„Giniå‡å°‘é‡ï¼Œå¹¶å°†è¿™äº›å‡å°‘é‡ç´¯åŠ èµ·æ¥ã€‚\nè®¡ç®—å¹³å‡å€¼ï¼šå¯¹æ‰€æœ‰æ ‘çš„ç´¯åŠ å€¼å–å¹³å‡å€¼ï¼Œä½œä¸ºè¯¥ç‰¹å¾çš„é‡è¦æ€§å¾—åˆ†ã€‚\n\n\nCodeclass_rf_fit %&gt;% vip::vi()\n#&gt; # A tibble: 9 Ã— 2\n#&gt;   Variable         Importance\n#&gt;   &lt;chr&gt;                 &lt;dbl&gt;\n#&gt; 1 è£¸æ ¸                  25.6 \n#&gt; 2 è‚¿å—åšåº¦              21.0 \n#&gt; 3 bland_chromatin       19.2 \n#&gt; 4 ç»†èƒå¤§å°å‡åŒ€æ€§        18.4 \n#&gt; 5 ç»†èƒå½¢çŠ¶å‡åŒ€æ€§        17.9 \n#&gt; 6 è¾¹é™…é™„ç€åŠ›            14.0 \n#&gt; 7 æ­£å¸¸æ ¸                13.9 \n#&gt; 8 å•ä¸ªä¸Šçš®ç»†èƒå¤§å°      10.6 \n#&gt; 9 æœ‰ä¸åˆ†è£‚               5.88\nclass_rf_fit %&gt;% vip::vip()\n\n\n\n\n\n\n\n\nCodeclass_rf_fit$fit$importance\n#&gt;                         è‰¯æ€§        æ¶æ€§ MeanDecreaseAccuracy MeanDecreaseGini\n#&gt; è‚¿å—åšåº¦         0.045985557 0.041538273          0.044359259        12.465397\n#&gt; ç»†èƒå¤§å°å‡åŒ€æ€§   0.049172066 0.076704399          0.058509661        47.927558\n#&gt; ç»†èƒå½¢çŠ¶å‡åŒ€æ€§   0.009876762 0.096802558          0.040209397        41.331213\n#&gt; è¾¹é™…é™„ç€åŠ›       0.016551632 0.035342949          0.023040888         6.945234\n#&gt; å•ä¸ªä¸Šçš®ç»†èƒå¤§å° 0.012280597 0.010654858          0.011723405        16.171318\n#&gt; è£¸æ ¸             0.061713383 0.066225145          0.063075655        40.588065\n#&gt; bland_chromatin  0.017570370 0.058096398          0.031839179        31.183405\n#&gt; æ­£å¸¸æ ¸           0.027576658 0.021034748          0.025191390        17.830550\n#&gt; æœ‰ä¸åˆ†è£‚         0.003095108 0.001130822          0.002400936         1.946241\n\n\n\nCode\naugment(class_rf_fit, new_data = test) %&gt;%\n    accuracy(truth = class, estimate = .pred_class)\n#&gt; # A tibble: 1 Ã— 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.990\n\naugment(class_rf_fit, new_data = test) %&gt;%\n    conf_mat(truth = class, estimate = .pred_class)\n#&gt;           Truth\n#&gt; Prediction è‰¯æ€§ æ¶æ€§\n#&gt;       è‰¯æ€§  132    0\n#&gt;       æ¶æ€§    2   72\n\n\n\n1.2.2 åŸºäºè¡¨è¾¾æ•°æ®çš„åº”ç”¨\n\nCodedf &lt;- dendextend::khan\n\ndf$train.classes\n#&gt;  [1] EWS    EWS    EWS    EWS    EWS    EWS    EWS    EWS    EWS    EWS   \n#&gt; [11] EWS    EWS    EWS    EWS    EWS    EWS    EWS    EWS    EWS    EWS   \n#&gt; [21] EWS    EWS    EWS    BL-NHL BL-NHL BL-NHL BL-NHL BL-NHL BL-NHL BL-NHL\n#&gt; [31] BL-NHL NB     NB     NB     NB     NB     NB     NB     NB     NB    \n#&gt; [41] NB     NB     NB     RMS    RMS    RMS    RMS    RMS    RMS    RMS   \n#&gt; [51] RMS    RMS    RMS    RMS    RMS    RMS    RMS    RMS    RMS    RMS   \n#&gt; [61] RMS    RMS    RMS    RMS   \n#&gt; Levels: EWS BL-NHL NB RMS\ntrain &lt;- t(df$train) |&gt; bind_cols(tibble(class=df$train.classes)) |&gt; \n    relocate(class, .before = 1) |&gt; \n    mutate(\n        class=factor(class,levels = c(\"EWS\", \"BL-NHL\", \"NB\",\"RMS\"))\n    )\nstr(train$class)\n#&gt;  Factor w/ 4 levels \"EWS\",\"BL-NHL\",..: 1 1 1 1 1 1 1 1 1 1 ...\ntable(train$class)\n#&gt; \n#&gt;    EWS BL-NHL     NB    RMS \n#&gt;     23      8     12     21\n\n\n\nCodedf$test.classes\n#&gt;  [1] Normal Normal Normal NB     RMS    Normal Normal NB     EWS    RMS   \n#&gt; [11] BL-NHL EWS    RMS    EWS    EWS    EWS    RMS    BL-NHL RMS    NB    \n#&gt; [21] NB     NB     NB     BL-NHL EWS   \n#&gt; Levels: EWS BL-NHL NB RMS Normal\n\ntest &lt;- t(df$test) |&gt; bind_cols(tibble(class=df$test.classes)) |&gt; \n    relocate(class, .before = 1) |&gt; \n    mutate(\n        class=factor(class,levels = c(\"EWS\", \"BL-NHL\", \"NB\",\"RMS\",\"Normal\"))\n    )\nstr(test$class)\n#&gt;  Factor w/ 5 levels \"EWS\",\"BL-NHL\",..: 5 5 5 3 4 5 5 3 1 4 ...\ntable(test$class)\n#&gt; \n#&gt;    EWS BL-NHL     NB    RMS Normal \n#&gt;      6      3      6      5      5\n\n\n\nCode#\ndt &lt;- class_tree_spec |&gt; fit(class ~ . ,data = train)\nrpart::plotcp(dt$fit)\n\n\n\n\n\n\nCode\ndt%&gt;%\n    extract_fit_engine() %&gt;%\n    rpart.plot::rpart.plot(roundint = F)\n\n\n\n\n\n\n\n\nCode#\nrf_fit_eg &lt;- cf_spec_class |&gt; \n    fit(class ~ . , data = train)\nrf_fit_eg\n#&gt; parsnip model object\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt;  randomForest(x = maybe_data_frame(x), y = y, ntree = ~500, nodesize = min_rows(~1,      x), importance = ~TRUE) \n#&gt;                Type of random forest: classification\n#&gt;                      Number of trees: 500\n#&gt; No. of variables tried at each split: 17\n#&gt; \n#&gt;         OOB estimate of  error rate: 0%\n#&gt; Confusion matrix:\n#&gt;        EWS BL-NHL NB RMS class.error\n#&gt; EWS     23      0  0   0           0\n#&gt; BL-NHL   0      8  0   0           0\n#&gt; NB       0      0 12   0           0\n#&gt; RMS      0      0  0  21           0",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "ç›‘ç£å­¦ä¹ ï¼ˆSupervised Learningï¼‰",
      "åŸºäºæ ‘çš„æ¨¡å‹"
    ]
  },
  {
    "objectID": "tree-based_models.html#éšæœºæ£®æ—-å›å½’",
    "href": "tree-based_models.html#éšæœºæ£®æ—-å›å½’",
    "title": "",
    "section": "\n1.3 éšæœºæ£®æ— ï¼ˆå›å½’ï¼‰",
    "text": "1.3 éšæœºæ£®æ— ï¼ˆå›å½’ï¼‰\n\nCodelibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidymodels)\ndata &lt;- haven::read_sav(\"data/æŠ‘éƒéšæœºæ£®æ—æ¨¡å‹å˜é‡é‡è¦æ€§.sav\") \n\n\n# glimpse(data)\n# \n# attributes(data$Gender)\n# attributes(data$ZD)\ndata &lt;- data %&gt;% \n    mutate(\n        Gender = factor(Gender),\n        XK = factor(XK),\n        DQ = factor(DQ),\n        SYDLX = factor(SYDLX),\n        \n    )\n\nset.seed(123)\nsplit &lt;- initial_split(data, prop = 0.7)\ntrain_data &lt;- training(split)\ntest_data &lt;- testing(split)\n\n\n\nCode# æ„å»ºéšæœºæ£®æ—æ¨¡å‹\nrf_sepc_reg &lt;- rand_forest(mtry = 5, trees = 500) %&gt;%\n    set_engine(\"randomForest\" , importance = TRUE\n               ) %&gt;%\n    set_mode(\"regression\")\n\n# åˆ›å»ºé…æ–¹\nrf_recipe &lt;- recipe(ZD ~ ., data = train_data) \n\n# å·¥ä½œæµç¨‹\nrf_workflow &lt;- workflow() %&gt;%\n    add_recipe(rf_recipe) %&gt;%\n    add_model(rf_sepc_reg)\n\n# è®­ç»ƒæ¨¡å‹\nreg_rf_fit &lt;- rf_workflow %&gt;%\n    fit(data = train_data)\nreg_rf_fit %&gt;% extract_fit_parsnip()\n#&gt; parsnip model object\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt;  randomForest(x = maybe_data_frame(x), y = y, ntree = ~500, mtry = min_cols(~5,      x), importance = ~TRUE) \n#&gt;                Type of random forest: regression\n#&gt;                      Number of trees: 500\n#&gt; No. of variables tried at each split: 5\n#&gt; \n#&gt;           Mean of squared residuals: 48.45618\n#&gt;                     % Var explained: 87.48\n\n\n\n1.3.1 ç‰¹å¾é‡è¦æ€§ï¼šåŸºäºå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰çš„å‡å°‘\nè¿™ç§æ–¹æ³•ä¸»è¦ç”¨äºå›å½’ä»»åŠ¡ã€‚\nMean Decrease in Accuracy (MDA) è¿™ç§æ–¹æ³•é€šè¿‡è¡¡é‡æŸä¸ªç‰¹å¾å¯¹æ•´ä½“æ¨¡å‹é¢„æµ‹å‡†ç¡®æ€§çš„è´¡çŒ®æ¥è®¡ç®—å…¶é‡è¦æ€§ã€‚å…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š\n\nè®­ç»ƒæ¨¡å‹ï¼šä½¿ç”¨æ‰€æœ‰ç‰¹å¾è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹ã€‚\nè®¡ç®—åŸºçº¿è¯¯å·®ï¼šä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šè®¡ç®—åŸºçº¿è¯¯å·®ï¼ˆä¾‹å¦‚ï¼Œå‡æ–¹è¯¯å·®ï¼‰ã€‚\næ‰°åŠ¨ç‰¹å¾å€¼ï¼šå¯¹äºæ¯ä¸ªç‰¹å¾ï¼Œéšæœºæ‰“ä¹±éªŒè¯é›†ä¸­è¯¥ç‰¹å¾çš„å€¼ï¼Œä»è€Œç ´åå…¶ä¸ç›®æ ‡å˜é‡çš„å…³ç³»ã€‚ é‡æ–°è®¡ç®—è¯¯å·®ï¼šä½¿ç”¨æ‰°åŠ¨åçš„æ•°æ®å†æ¬¡è®¡ç®—æ¨¡å‹è¯¯å·®ã€‚\nè®¡ç®—é‡è¦æ€§ï¼šç‰¹å¾é‡è¦æ€§å¾—åˆ†ç­‰äºæ‰°åŠ¨åçš„è¯¯å·®ä¸åŸºçº¿è¯¯å·®ä¹‹å·®ã€‚è¯¯å·®å¢åŠ è¶Šå¤šï¼Œè¯´æ˜è¯¥ç‰¹å¾å¯¹æ¨¡å‹é¢„æµ‹çš„è´¡çŒ®è¶Šå¤§ã€‚\n\n\nCode# æŸ¥çœ‹ç‰¹å¾é‡è¦æ€§\nimportance &lt;- reg_rf_fit %&gt;%\n    extract_fit_parsnip() %&gt;%\n    vip::vi()\nvip::vi(reg_rf_fit$fit$fit) \n#&gt; # A tibble: 26 Ã— 2\n#&gt;    Variable Importance\n#&gt;    &lt;chr&gt;         &lt;dbl&gt;\n#&gt;  1 HL             34.9\n#&gt;  2 NE_A           29.4\n#&gt;  3 Sport          21.9\n#&gt;  4 SA             14.6\n#&gt;  5 I              14.1\n#&gt;  6 SMML           13.6\n#&gt;  7 FC             13.5\n#&gt;  8 BECKNC         13.2\n#&gt;  9 YS             12.8\n#&gt; 10 MVS            12.6\n#&gt; # â„¹ 16 more rows\n\n\n# å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§\nggplot(importance, aes(x = reorder(Variable, Importance), y = Importance)) + \n    geom_col(fill = \"skyblue\") +\n    coord_flip() +\n    labs(title = \"ç‰¹å¾é‡è¦æ€§\", x = \"ç‰¹å¾\", y = \"é‡è¦æ€§å¾—åˆ†\")",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "ç›‘ç£å­¦ä¹ ï¼ˆSupervised Learningï¼‰",
      "åŸºäºæ ‘çš„æ¨¡å‹"
    ]
  },
  {
    "objectID": "tree-based_models.html#è£…è¢‹æ³•",
    "href": "tree-based_models.html#è£…è¢‹æ³•",
    "title": "",
    "section": "\n1.4 è£…è¢‹æ³•",
    "text": "1.4 è£…è¢‹æ³•\nè£…è¢‹æ³•ï¼ˆBaggingï¼‰æˆ–ç§°è‡ªåŠ©èšåˆï¼ˆBootstrap Aggregationï¼‰æ˜¯åŸºäºæ ‘çš„æ¨¡å‹çš„ä¸€ç§é›†æˆå­¦ä¹ æŠ€æœ¯ã€‚\nè£…è¢‹æ³•æ˜¯ä¸€ç§é›†æˆå­¦ä¹ æŠ€æœ¯ï¼Œå®ƒé€šè¿‡æ„å»ºå¤šä¸ªæ¨¡å‹ï¼ˆé€šå¸¸æ˜¯å†³ç­–æ ‘ï¼‰å¹¶å°†å…¶é¢„æµ‹ç»“æœè¿›è¡Œå¹³å‡ï¼ˆå¯¹äºå›å½’ä»»åŠ¡ï¼‰æˆ–æŠ•ç¥¨ï¼ˆå¯¹äºåˆ†ç±»ä»»åŠ¡ï¼‰æ¥æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚å…¶ä¸»è¦æ­¥éª¤å¦‚ä¸‹ï¼š\n\næ•°æ®æŠ½æ ·ï¼šä»åŸå§‹è®­ç»ƒæ•°æ®é›†ä¸­é€šè¿‡è‡ªåŠ©æ³•ï¼ˆBootstrapï¼‰éšæœºæœ‰æ”¾å›åœ°æŠ½å–å¤šä¸ªå­é›†ã€‚æ¯ä¸ªå­é›†çš„å¤§å°ä¸åŸå§‹æ•°æ®é›†ç›¸åŒï¼Œä½†ç”±äºæ˜¯æœ‰æ”¾å›åœ°æŠ½æ ·ï¼Œå› æ­¤æ¯ä¸ªå­é›†ä¸­å¯èƒ½åŒ…å«é‡å¤çš„æ ·æœ¬ã€‚\næ¨¡å‹è®­ç»ƒï¼šå¯¹æ¯ä¸ªå­é›†è®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼ˆé€šå¸¸æ˜¯å†³ç­–æ ‘æ¨¡å‹ï¼‰ã€‚ç”±äºæ¯ä¸ªå­é›†çš„æ ·æœ¬å¯èƒ½ä¸åŒï¼Œè®­ç»ƒå¾—åˆ°çš„æ¯ä¸ªæ¨¡å‹ä¹Ÿå¯èƒ½ä¸åŒã€‚\næ¨¡å‹é›†æˆï¼šåœ¨è¿›è¡Œé¢„æµ‹æ—¶ï¼Œå°†æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹ç»“æœè¿›è¡Œæ•´åˆã€‚å¯¹äºåˆ†ç±»ä»»åŠ¡ï¼Œé‡‡ç”¨å¤šæ•°æŠ•ç¥¨æ³•ï¼Œå³é€‰æ‹©å‡ºç°æ¬¡æ•°æœ€å¤šçš„ç±»åˆ«ä½œä¸ºæœ€ç»ˆé¢„æµ‹ç»“æœï¼›å¯¹äºå›å½’ä»»åŠ¡ï¼Œåˆ™é‡‡ç”¨å¹³å‡æ³•ï¼Œå³å–å„æ¨¡å‹é¢„æµ‹å€¼çš„å¹³å‡å€¼ä½œä¸ºæœ€ç»ˆé¢„æµ‹ç»“æœã€‚",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "ç›‘ç£å­¦ä¹ ï¼ˆSupervised Learningï¼‰",
      "åŸºäºæ ‘çš„æ¨¡å‹"
    ]
  },
  {
    "objectID": "tree-based_models.html#æ¢¯åº¦æå‡æ ‘",
    "href": "tree-based_models.html#æ¢¯åº¦æå‡æ ‘",
    "title": "",
    "section": "\n1.5 æ¢¯åº¦æå‡æ ‘",
    "text": "1.5 æ¢¯åº¦æå‡æ ‘\næ¢¯åº¦æå‡æ ‘ï¼ˆGradient Boosting Trees, GBTï¼‰æ˜¯ä¸€ç§å¼ºå¤§çš„é›†æˆå­¦ä¹ æ–¹æ³•ï¼Œå®ƒé€šè¿‡é€æ­¥æ„å»ºå¤šä¸ªå†³ç­–æ ‘ï¼Œå¹¶å°†å®ƒä»¬çš„é¢„æµ‹ç»“æœè¿›è¡ŒåŠ æƒç»„åˆæ¥æé«˜æ¨¡å‹çš„é¢„æµ‹æ€§èƒ½ã€‚ä¸è£…è¢‹æ³•ï¼ˆBaggingï¼‰ä¸åŒï¼Œæ¢¯åº¦æå‡æ ‘æ˜¯ä¸€ä¸ªè¿­ä»£çš„è¿‡ç¨‹ï¼Œåœ¨æ¯ä¸€æ­¥ä¸­éƒ½è¯•å›¾çº æ­£å‰ä¸€æ­¥æ¨¡å‹çš„é”™è¯¯ã€‚\næ¢¯åº¦æå‡æ ‘çš„åŸºæœ¬æ€æƒ³æ˜¯é€šè¿‡é€æ­¥æ„å»ºä¸€ç³»åˆ—çš„å¼±å­¦ä¹ å™¨ï¼ˆé€šå¸¸æ˜¯å†³ç­–æ ‘ï¼‰æ¥é€¼è¿‘ç›®æ ‡å‡½æ•°ã€‚æ¯ä¸€ä¸ªæ–°çš„æ ‘éƒ½åœ¨å…ˆå‰æ ‘çš„åŸºç¡€ä¸Šè¿›è¡Œæ”¹è¿›ï¼Œä½¿æ•´ä½“æ¨¡å‹çš„é¢„æµ‹è¯¯å·®é€æ­¥å‡å°ã€‚å…¶ä¸»è¦æ­¥éª¤å¦‚ä¸‹ï¼š\n\nåˆå§‹åŒ–æ¨¡å‹ï¼šé¦–å…ˆç”¨ä¸€ä¸ªç®€å•çš„æ¨¡å‹ï¼ˆå¦‚å¸¸æ•°å€¼æ¨¡å‹ï¼‰åˆå§‹åŒ–é¢„æµ‹å€¼ã€‚\nè®¡ç®—æ®‹å·®ï¼šè®¡ç®—åˆå§‹æ¨¡å‹çš„é¢„æµ‹å€¼ä¸å®é™…å€¼ä¹‹é—´çš„å·®å€¼ï¼ˆæ®‹å·®ï¼‰ï¼Œè¿™äº›æ®‹å·®ä»£è¡¨äº†å½“å‰æ¨¡å‹çš„è¯¯å·®ã€‚\nè®­ç»ƒæ–°æ ‘ï¼šåŸºäºæ®‹å·®è®­ç»ƒä¸€ä¸ªæ–°çš„å†³ç­–æ ‘ï¼Œç›®çš„æ˜¯å­¦ä¹ å¦‚ä½•çº æ­£å½“å‰æ¨¡å‹çš„è¯¯å·®ã€‚\næ›´æ–°æ¨¡å‹ï¼šå°†æ–°æ ‘çš„é¢„æµ‹ç»“æœåŠ æƒåŠ å…¥åˆ°å½“å‰æ¨¡å‹ä¸­ï¼Œä»è€Œæ›´æ–°æ•´ä½“æ¨¡å‹ã€‚æ›´æ–°å…¬å¼é€šå¸¸ä¸ºï¼š\n\n\\[\nF_m(x)=F_{m-1}(x)+Î·h_m(x)F_{m}(x)\n\\] å…¶ä¸­ï¼Œ\\(F_{m}(x)\\) æ˜¯ç¬¬ m æ¬¡è¿­ä»£çš„æ¨¡å‹ï¼Œ\\(\\eta\\) æ˜¯å­¦ä¹ ç‡ï¼ˆé€šå¸¸åœ¨0å’Œ1ä¹‹é—´ï¼‰ï¼Œ\\(h_m(x)\\) æ˜¯ç¬¬ m æ£µæ ‘çš„é¢„æµ‹å€¼ã€‚\n\n\né‡å¤è¿­ä»£ï¼šé‡å¤æ­¥éª¤2-4ï¼Œç›´åˆ°è¾¾åˆ°é¢„å®šçš„æ ‘çš„æ•°é‡æˆ–è¯¯å·®æ”¶æ•›ã€‚",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "ç›‘ç£å­¦ä¹ ï¼ˆSupervised Learningï¼‰",
      "åŸºäºæ ‘çš„æ¨¡å‹"
    ]
  },
  {
    "objectID": "tree-based_models.html#è´å¶æ–¯ç›¸åŠ å›å½’æ ‘",
    "href": "tree-based_models.html#è´å¶æ–¯ç›¸åŠ å›å½’æ ‘",
    "title": "",
    "section": "\n1.6 è´å¶æ–¯ç›¸åŠ å›å½’æ ‘",
    "text": "1.6 è´å¶æ–¯ç›¸åŠ å›å½’æ ‘",
    "crumbs": [
      "é¢„æµ‹æ€§æ¨¡å‹",
      "ç›‘ç£å­¦ä¹ ï¼ˆSupervised Learningï¼‰",
      "åŸºäºæ ‘çš„æ¨¡å‹"
    ]
  }
]