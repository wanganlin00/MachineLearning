[
  {
    "objectID": "UMAP.html",
    "href": "UMAP.html",
    "title": "",
    "section": "",
    "text": "无监督学习（Unsupervised Learning）UMAP CodeShow All CodeHide All CodeView Source\n1 UMAP\nUniform Manifold Approximation and Projection (UMAP)\nhttps://github.com/jlmelville/uwot\nhttps://github.com/tkonopka/umap\n2018年McInnes提出了算法，UMAP（Uniform Manifold Approximation and Projection for Dimension Reduction，一致的流形逼近和投影以进行降维）。 一致的流形近似和投影（UMAP）是一种降维技术，类似于t-SNE，可用于可视化，但也可用于一般的非线性降维。 该算法基于关于数据的三个假设：\n\n数据均匀分布在黎曼流形上（Riemannian manifold）；\n黎曼度量是局部恒定的（或可以这样近似）；\n流形是局部连接的。\n\nhttps://jlmelville.github.io/uwot/index.html\n\nCodelibrary(uwot)\nhead(iris)\n#&gt;   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#&gt; 1          5.1         3.5          1.4         0.2  setosa\n#&gt; 2          4.9         3.0          1.4         0.2  setosa\n#&gt; 3          4.7         3.2          1.3         0.2  setosa\n#&gt; 4          4.6         3.1          1.5         0.2  setosa\n#&gt; 5          5.0         3.6          1.4         0.2  setosa\n#&gt; 6          5.4         3.9          1.7         0.4  setosa\ncolors = rainbow(length(unique(iris$Species)))\nnames(colors) = unique(iris$Species)\n# umap2 is a version of the umap() function with better defaults\niris_umap2 &lt;- umap2(iris[1:4]) |&gt; as_tibble()\n#&gt; Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n#&gt; `.name_repair` is omitted as of tibble 2.0.0.\n#&gt; ℹ Using compatibility `.name_repair`.\n\nggplot(iris_umap2,aes(V1,V2))+\n    geom_text(aes(label=iris$Species),color=colors[iris$Species])\n\n\n\n\n\n\n\n\nCode# but you can still use the umap function (which most of the existing \n# documentation does)\niris_umap &lt;- umap(iris[1:4]) |&gt; as_tibble()\n\nggplot(iris_umap,aes(V1,V2))+\n    geom_text(aes(label=iris$Species),color=colors[iris$Species])\n\n\n\n\n\n\n\n\nCodelibrary(uwot)\n\nset.seed(42) # 为了结果可重复\nuwot_result &lt;- umap(iris[1:4])\n\n# 将结果转换为数据框\nuwot_df &lt;- as.data.frame(uwot_result)\ncolnames(uwot_df) &lt;- c(\"UMAP1\", \"UMAP2\")\nuwot_df$Species &lt;- iris$Species\n\n# 可视化\nggplot(uwot_df, aes(x = UMAP1, y = UMAP2, color = Species)) +\n  geom_point(size = 2) +\n  labs(title = \"UMAP of Iris Dataset (uwot)\",\n       x = \"UMAP Dimension 1\",\n       y = \"UMAP Dimension 2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "预测性模型",
      "无监督学习（Unsupervised Learning）",
      "UMAP"
    ]
  },
  {
    "objectID": "t-SNE.html",
    "href": "t-SNE.html",
    "title": "",
    "section": "",
    "text": "无监督学习（Unsupervised Learning）t-SNE CodeShow All CodeHide All CodeView Source\n1 t-SNE\nt-Distributed Stochastic Neighbor Embedding (t-SNE)是一种降维技术，用于在二维或三维的低维空间中表示高维数据集，从而使其可视化。与其他降维算法(如PCA)相比，t-SNE创建了一个缩小的特征空间，相似的样本由附近的点建模，不相似的样本由高概率的远点建模。\nt-Distributed Stochastic Neighbor Embedding (t-SNE)\n\nCodelibrary(tsne)\n\nhead(iris)\n#&gt;   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#&gt; 1          5.1         3.5          1.4         0.2  setosa\n#&gt; 2          4.9         3.0          1.4         0.2  setosa\n#&gt; 3          4.7         3.2          1.3         0.2  setosa\n#&gt; 4          4.6         3.1          1.5         0.2  setosa\n#&gt; 5          5.0         3.6          1.4         0.2  setosa\n#&gt; 6          5.4         3.9          1.7         0.4  setosa\n\ncolors = rainbow(length(unique(iris$Species)))\nnames(colors) = unique(iris$Species)\necb = function(x, y) {\n    plot(x, t = 'n')\n    text(x, labels = iris$Species, col = colors[iris$Species])\n}\ntsne_iris = tsne(iris[,1:4], epoch_callback = ecb, perplexity=50)\n#&gt; sigma summary: Min. : 0.565012665854053 |1st Qu. : 0.681985646004023 |Median : 0.713004330336136 |Mean : 0.716213420895748 |3rd Qu. : 0.74581655363904 |Max. : 0.874979764925049 |\n#&gt; Epoch: Iteration #100 error is: 12.0521968962333\n#&gt; Epoch: Iteration #200 error is: 0.278293775495887\n\n\n\n\n\n\n#&gt; Epoch: Iteration #300 error is: 0.277972566238466\n\n\n\n\n\n\n#&gt; Epoch: Iteration #400 error is: 0.277972360425316\n\n\n\n\n\n\n#&gt; Epoch: Iteration #500 error is: 0.277972360400364\n\n\n\n\n\n\n#&gt; Epoch: Iteration #600 error is: 0.277972360400287\n\n\n\n\n\n\n#&gt; Epoch: Iteration #700 error is: 0.277972360400287\n\n\n\n\n\n\n#&gt; Epoch: Iteration #800 error is: 0.277972360400287\n\n\n\n\n\n\n#&gt; Epoch: Iteration #900 error is: 0.277972360400287\n\n\n\n\n\n\n#&gt; Epoch: Iteration #1000 error is: 0.277972360400287\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# compare to PCA\ndev.new()\npca_iris = princomp(iris[,1:4])$scores[,1:2]\nplot(pca_iris, t='n')\ntext(pca_iris, labels=iris$Species,col=colors[iris$Species])\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "预测性模型",
      "无监督学习（Unsupervised Learning）",
      "t-SNE"
    ]
  },
  {
    "objectID": "Regression.html",
    "href": "Regression.html",
    "title": "",
    "section": "",
    "text": "监督学习（Supervised Learning）回归 Code\n\n\n\n\n\n1 回归\n用于预测连续型数值。\n\n线性回归（Linear Regression）：预测因变量与一个或多个自变量之间的线性关系。\n岭回归（Ridge Regression）：线性回归的正则化版本，减少过拟合。\nLasso回归（Lasso Regression）：另一种正则化的线性回归，通过L1正则化进行特征选择。\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "预测性模型",
      "监督学习（Supervised Learning）",
      "回归"
    ]
  },
  {
    "objectID": "PCA.html",
    "href": "PCA.html",
    "title": "",
    "section": "",
    "text": "无监督学习（Unsupervised Learning）主成分分析 CodeShow All CodeHide All CodeView Source",
    "crumbs": [
      "预测性模型",
      "无监督学习（Unsupervised Learning）",
      "主成分分析"
    ]
  },
  {
    "objectID": "PCA.html#自定义-pca-步骤",
    "href": "PCA.html#自定义-pca-步骤",
    "title": "",
    "section": "\n1.1 自定义 PCA 步骤",
    "text": "1.1 自定义 PCA 步骤\n\n1.1.1 数据标准化\nR中数据框 n个观测，p个变量\n\\[\nX=\n\\begin{bmatrix}\n  x_{11}& x_{12} & ... & x_{1p}\\\\\nx_{21} & x_{22} & ... & x_{2p}\\\\\n\\vdots  &  \\vdots &   & \\vdots \\\\\n  x_{n1}& x_{n2} & ... & x_{np}\n\\end{bmatrix}\n=(X_1,X_2,...X_p)\n\\]\n对原始数据矩阵标准化，消除量纲和数量级的影响。\n数据标准化确保变量在相同的尺度上，这对于PCA非常重要。\n使用R语言内置的 USArrests 数据集：\n\nCodedf &lt;- as_tibble(USArrests, rownames = \"state\") |&gt; column_to_rownames(\"state\")\nhead(df)\n#&gt;            Murder Assault UrbanPop Rape\n#&gt; Alabama      13.2     236       58 21.2\n#&gt; Alaska       10.0     263       48 44.5\n#&gt; Arizona       8.1     294       80 31.0\n#&gt; Arkansas      8.8     190       50 19.5\n#&gt; California    9.0     276       91 40.6\n#&gt; Colorado      7.9     204       78 38.7\n\ndf_center &lt;- scale(df,center = T,scale = T)\n\n\n\n1.1.2 计算协方差矩阵\n当\\(\\Sigma\\) 未知时，其用其估计值样本协方差矩阵Sp×p 代替\n\\[\nS=\\frac{AA^T}{n-1}\n\\]\n\n\n\\(A_{ p×n}\\) 显示来自每个变量值与其均值的偏差 \\(X_i-\\bar X\\)；\n\n\\((AA^T)_{ii}\\) 显示偏差平方和 （样本方差 \\(s_i^2\\) ）；\n\n\\((AA^T)_{ij}，i\\ne j\\) 显示样本协方差 \\(s_{ij} = (A 的行 i) · (A 的行 j)\\)。\n\n\nCode# 手动计算协方差矩阵\nA &lt;- as.matrix(t((df_center)))\nAA_T &lt;- A %*% t(A)\nS &lt;- AA_T / (nrow(df_center) - 1)\nS\n#&gt;              Murder   Assault   UrbanPop      Rape\n#&gt; Murder   1.00000000 0.8018733 0.06957262 0.5635788\n#&gt; Assault  0.80187331 1.0000000 0.25887170 0.6652412\n#&gt; UrbanPop 0.06957262 0.2588717 1.00000000 0.4113412\n#&gt; Rape     0.56357883 0.6652412 0.41134124 1.0000000\n# 内置协方差矩阵函数\ncov(df_center)\n#&gt;              Murder   Assault   UrbanPop      Rape\n#&gt; Murder   1.00000000 0.8018733 0.06957262 0.5635788\n#&gt; Assault  0.80187331 1.0000000 0.25887170 0.6652412\n#&gt; UrbanPop 0.06957262 0.2588717 1.00000000 0.4113412\n#&gt; Rape     0.56357883 0.6652412 0.41134124 1.0000000\n\n\n\n1.1.3 计算相关系数矩阵\n相关系数矩阵 \\(R=(r_{ij})\\) 的公式为：\n\\[\nr_{ij}=\\frac {S_{ij}}{\\sqrt{S_{ii}×S_{jj}}}\n\\]\n\nCode# 定义自定义函数计算相关系数矩阵\nr &lt;- function(df){\n    df &lt;- as.data.frame(df)\n    n=length(df)\n    names &lt;- colnames(df)\n    df &lt;- scale(df)\n    S &lt;- cov(df)\n    r &lt;- matrix(data = NA,n,n,dimnames = list(names,names))\n    for(i in 1:n){\n        for(j in 1:n){\n            r[i,j]=S[i,j]/sqrt(S[i,i]*S[j,j])\n        }\n    }\n    return(r)\n}\nr(df)\n#&gt;              Murder   Assault   UrbanPop      Rape\n#&gt; Murder   1.00000000 0.8018733 0.06957262 0.5635788\n#&gt; Assault  0.80187331 1.0000000 0.25887170 0.6652412\n#&gt; UrbanPop 0.06957262 0.2588717 1.00000000 0.4113412\n#&gt; Rape     0.56357883 0.6652412 0.41134124 1.0000000\n\n# 使用内置的相关系数矩阵函数\ncor(df_center)\n#&gt;              Murder   Assault   UrbanPop      Rape\n#&gt; Murder   1.00000000 0.8018733 0.06957262 0.5635788\n#&gt; Assault  0.80187331 1.0000000 0.25887170 0.6652412\n#&gt; UrbanPop 0.06957262 0.2588717 1.00000000 0.4113412\n#&gt; Rape     0.56357883 0.6652412 0.41134124 1.0000000\n\n\n\n1.1.4 总方差\n总方差T=所有特征值的总和=样本方差的总和=协方差矩阵的迹(对角线的总和)\nsum(eigen(AA_T)$values)=sum(diag(AA_T))= sum(svd$d^2)\n\nCode# AA^T的特征值\ny &lt;- eigen(AA_T)\ny$values\n#&gt; [1] 121.531837  48.498492  17.471596   8.498074\ny$vectors\n#&gt;            [,1]       [,2]       [,3]        [,4]\n#&gt; [1,] -0.5358995  0.4181809 -0.3412327  0.64922780\n#&gt; [2,] -0.5831836  0.1879856 -0.2681484 -0.74340748\n#&gt; [3,] -0.2781909 -0.8728062 -0.3780158  0.13387773\n#&gt; [4,] -0.5434321 -0.1673186  0.8177779  0.08902432\n\n# 特征值的和\nsum(y$values)\n#&gt; [1] 196\n\n# 迹\nsum(diag(AA_T))\n#&gt; [1] 196\n\n\n\n1.1.5 奇异值分解与主成分推导\nSVD公式：\n\\[\nA_{p×n}=U\\Sigma V^T\n\\]\nA 是中心化后的数据矩阵， U 是左奇异矩阵， \\(\\Sigma\\) 是奇异值对角矩阵， V 是右奇异矩阵（（也是主成分方向））\n主成分推导：\n\\[ PC=A\\cdot V=U \\Sigma  \\]\n在这个公式中：\n\n\\(U\\) 是包含左奇异向量的矩阵，表示样本在新坐标系中的坐标。\n\\(\\Sigma\\) 是包含奇异值的对角矩阵，这些奇异值与特征值相关，表示每个主成分的方差大小。\n\n\nCode\n# 进行奇异值分解\n\nsvd &lt;- svd(df_center)\nsvd$d\n#&gt; [1] 11.024148  6.964086  4.179904  2.915146\nsvd$u\n#&gt;               [,1]        [,2]         [,3]          [,4]\n#&gt;  [1,] -0.088502119 -0.16111249  0.105218608  0.0530665011\n#&gt;  [2,] -0.175119011 -0.15255799 -0.483145153 -0.1489378244\n#&gt;  [3,] -0.158329049  0.10603826 -0.012974042 -0.2834384049\n#&gt;  [4,]  0.012699298 -0.15917987 -0.027135114 -0.0620804497\n#&gt;  [5,] -0.226649068  0.21932910 -0.141759482 -0.1161380178\n#&gt;  [6,] -0.136005136  0.14038162 -0.259336498  0.0004974586\n#&gt;  [7,]  0.122004202  0.15479183  0.152346210 -0.0402308322\n#&gt;  [8,] -0.004284214  0.04624999  0.170197773 -0.2995093258\n#&gt;  [9,] -0.270566006 -0.00557636  0.136613685 -0.0326971796\n#&gt; [10,] -0.147204794 -0.18180252  0.081106695  0.3656676469\n#&gt; [11,]  0.081955040  0.22324195 -0.012026954  0.3065826885\n#&gt; [12,]  0.147251202 -0.02998994 -0.061530174 -0.1694899353\n#&gt; [13,] -0.123823808  0.09692418  0.160455000 -0.0414370084\n#&gt; [14,]  0.045389560  0.02154472 -0.054011475  0.1442115222\n#&gt; [15,]  0.202373535  0.01479136 -0.038974667  0.0059617844\n#&gt; [16,]  0.071558552  0.03840409 -0.006051930  0.0701237800\n#&gt; [17,]  0.067425851 -0.13624293  0.006718884  0.2277132300\n#&gt; [18,] -0.140517959 -0.12382100  0.185555941  0.1544203417\n#&gt; [19,]  0.215231159 -0.05350432  0.015555919 -0.1122203023\n#&gt; [20,] -0.158347534 -0.06079147  0.037242408 -0.1898534932\n#&gt; [21,]  0.043656895  0.20960067  0.144350623 -0.0609897144\n#&gt; [22,] -0.189334383  0.02208976 -0.091150533  0.0347643443\n#&gt; [23,]  0.151999911  0.08987636 -0.036252509  0.0228600295\n#&gt; [24,] -0.089483486 -0.34027971  0.175449706  0.0731840095\n#&gt; [25,] -0.062570302  0.03743606 -0.089392087  0.0766873549\n#&gt; [26,]  0.106451539 -0.07631705 -0.058472149  0.0420214180\n#&gt; [27,]  0.113651981  0.02757065 -0.041582129  0.0053970395\n#&gt; [28,] -0.258115678  0.11025209 -0.275529769  0.1068057898\n#&gt; [29,]  0.214071497  0.00257041 -0.008728664 -0.0112530536\n#&gt; [30,] -0.016304324  0.20604821  0.181049718  0.0826499280\n#&gt; [31,] -0.177802722 -0.02030605 -0.043504824 -0.1153016522\n#&gt; [32,] -0.151092550  0.11701618  0.152302994 -0.0045791345\n#&gt; [33,] -0.100877464 -0.31671218  0.204524432 -0.3240968906\n#&gt; [34,]  0.268696706 -0.08516514 -0.071353148 -0.0862511360\n#&gt; [35,]  0.020291306  0.10550966  0.007374849  0.1609363198\n#&gt; [36,]  0.027997563  0.04091867  0.003625902  0.0035087357\n#&gt; [37,] -0.005309061  0.07696200 -0.222585787 -0.0807475502\n#&gt; [38,]  0.079778211  0.08118230  0.094883089  0.1219329726\n#&gt; [39,]  0.077565243  0.21208574  0.324451737 -0.2083610270\n#&gt; [40,] -0.118598723 -0.27483477  0.071178009 -0.0446445538\n#&gt; [41,]  0.178498756 -0.11703880 -0.092198470 -0.0372092940\n#&gt; [42,] -0.089775081 -0.12228530 -0.044544714  0.2217051038\n#&gt; [43,] -0.121689077  0.05863443  0.116539361  0.2184216921\n#&gt; [44,]  0.049439812  0.20917537 -0.069565218 -0.0279528910\n#&gt; [45,]  0.251561949 -0.19933619 -0.199240942 -0.0492029259\n#&gt; [46,]  0.008650709 -0.02839251 -0.002773945  0.0717790646\n#&gt; [47,]  0.019477550  0.13790380 -0.147991604 -0.0749973365\n#&gt; [48,]  0.189347337 -0.20254292 -0.024814359  0.0447947015\n#&gt; [49,]  0.186754750  0.08689225  0.032888157  0.0625194853\n#&gt; [50,]  0.056521430 -0.04563221  0.056996643 -0.0565930091\nsvd$v\n#&gt;            [,1]       [,2]       [,3]        [,4]\n#&gt; [1,] -0.5358995 -0.4181809  0.3412327  0.64922780\n#&gt; [2,] -0.5831836 -0.1879856  0.2681484 -0.74340748\n#&gt; [3,] -0.2781909  0.8728062  0.3780158  0.13387773\n#&gt; [4,] -0.5434321  0.1673186 -0.8177779  0.08902432\n# 奇异值的平方和\nsum(svd$d^2)\n#&gt; [1] 196\n\n# 奇异值的对角矩阵\nD &lt;- diag(svd$d)\n\n#  df_center  X = U D V'\nX &lt;- svd$u %*% D %*% t(svd$v) \n\n#  D = U' X V\nt(svd$u) %*% X %*% svd$v\n#&gt;               [,1]         [,2]          [,3]          [,4]\n#&gt; [1,]  1.102415e+01 1.110223e-15  8.881784e-16  3.219647e-15\n#&gt; [2,]  2.220446e-16 6.964086e+00  6.661338e-16  1.318390e-15\n#&gt; [3,] -2.220446e-16 4.440892e-16  4.179904e+00 -8.881784e-16\n#&gt; [4,] -4.440892e-16 1.436351e-15 -9.436896e-16  2.915146e+00\n\n\n\n1.1.6 结果解释\n\n1.1.6.1 主成分荷载系数\n\nCode# 主成分荷载系数\nsvd$v\n#&gt;            [,1]       [,2]       [,3]        [,4]\n#&gt; [1,] -0.5358995 -0.4181809  0.3412327  0.64922780\n#&gt; [2,] -0.5831836 -0.1879856  0.2681484 -0.74340748\n#&gt; [3,] -0.2781909  0.8728062  0.3780158  0.13387773\n#&gt; [4,] -0.5434321  0.1673186 -0.8177779  0.08902432\n\n\n在PCA中，右奇异向量矩阵𝑉 的列向量代表数据在新的正交基上的方向，这些基是按数据中方差最大化的方向排列的。每个向量就是一个主成分方向。具体来说，矩阵 svd$v 中的每一列都是一个主成分， 且这些列向量可以看作是原始变量在新主成分空间中的线性组合系数。\n因此，svd$v 中的元素表示的是每个原始变量在对应主成分上的贡献，即主成分荷载系数（loadings）。\n例如，如果 V 的第j 个列向量为 \\([v_{1j},v_{2j},...,v_{pj}]\\) ，这意味着第j 个主成分可以表示为原始变量的线性组合：\n\\[\nPC_j=v_{1j}\\cdot x_1+v_{2j}\\cdot x_2+...+v_{pj}\\cdot x_p\n\\]\n其中， \\(x_1,x_2,...,x_p\\) 是原始变量， \\(v_{1j},v_{2j},...,v_{pj}\\) 是它们在第j 个主成分上的荷载系数。\n\n1.1.6.2 主成分得分\n主成分得分 (principal component scores) 代表了原始数据在新主成分轴上的坐标。\n具体来说，主成分得分可以表示为：\n\\[\nScores = U\\Sigma\n\\]\n\nCode# 得分\nsvd$u %*% D\n#&gt;              [,1]        [,2]        [,3]         [,4]\n#&gt;  [1,] -0.97566045 -1.12200121  0.43980366  0.154696581\n#&gt;  [2,] -1.93053788 -1.06242692 -2.01950027 -0.434175454\n#&gt;  [3,] -1.74544285  0.73845954 -0.05423025 -0.826264240\n#&gt;  [4,]  0.13999894 -1.10854226 -0.11342217 -0.180973554\n#&gt;  [5,] -2.49861285  1.52742672 -0.59254100 -0.338559240\n#&gt;  [6,] -1.49934074  0.97762966 -1.08400162  0.001450164\n#&gt;  [7,]  1.34499236  1.07798362  0.63679250 -0.117278736\n#&gt;  [8,] -0.04722981  0.32208890  0.71141032 -0.873113315\n#&gt;  [9,] -2.98275967 -0.03883425  0.57103206 -0.095317042\n#&gt; [10,] -1.62280742 -1.26608838  0.33901818  1.065974459\n#&gt; [11,]  0.90348448  1.55467609 -0.05027151  0.893733198\n#&gt; [12,]  1.62331903 -0.20885253 -0.25719021 -0.494087852\n#&gt; [13,] -1.36505197  0.67498834  0.67068647 -0.120794916\n#&gt; [14,]  0.50038122  0.15003926 -0.22576277  0.420397595\n#&gt; [15,]  2.23099579  0.10300828 -0.16291036  0.017379470\n#&gt; [16,]  0.78887206  0.26744941 -0.02529648  0.204421034\n#&gt; [17,]  0.74331256 -0.94880748  0.02808429  0.663817237\n#&gt; [18,] -1.54909076 -0.86230011  0.77560598  0.450157791\n#&gt; [19,]  2.37274014 -0.37260865  0.06502225 -0.327138529\n#&gt; [20,] -1.74564663 -0.42335704  0.15566968 -0.553450589\n#&gt; [21,]  0.48128007  1.45967706  0.60337172 -0.177793902\n#&gt; [22,] -2.08725025  0.15383500 -0.38100046  0.101343128\n#&gt; [23,]  1.67566951  0.62590670 -0.15153200  0.066640316\n#&gt; [24,] -0.98647919 -2.36973712  0.73336290  0.213342049\n#&gt; [25,] -0.68978426  0.26070794 -0.37365033  0.223554811\n#&gt; [26,]  1.17353751 -0.53147851 -0.24440796  0.122498555\n#&gt; [27,]  1.25291625  0.19200440 -0.17380930  0.015733156\n#&gt; [28,] -2.84550542  0.76780502 -1.15168793  0.311354436\n#&gt; [29,]  2.35995585  0.01790055 -0.03648498 -0.032804291\n#&gt; [30,] -0.17974128  1.43493745  0.75677041  0.240936580\n#&gt; [31,] -1.96012351 -0.14141308 -0.18184598 -0.336121113\n#&gt; [32,] -1.66566662  0.81491072  0.63661186 -0.013348844\n#&gt; [33,] -1.11208808 -2.20561081  0.85489245 -0.944789648\n#&gt; [34,]  2.96215223 -0.59309738 -0.29824930 -0.251434626\n#&gt; [35,]  0.22369436  0.73477837  0.03082616  0.469152817\n#&gt; [36,]  0.30864928  0.28496113  0.01515592  0.010228476\n#&gt; [37,] -0.05852787  0.53596999 -0.93038718 -0.235390872\n#&gt; [38,]  0.87948680  0.56536050  0.39660218  0.355452378\n#&gt; [39,]  0.85509072  1.47698328  1.35617705 -0.607402746\n#&gt; [40,] -1.30744986 -1.91397297  0.29751723 -0.130145378\n#&gt; [41,]  1.96779669 -0.81506822 -0.38538073 -0.108470512\n#&gt; [42,] -0.98969377 -0.85160534 -0.18619262  0.646302674\n#&gt; [43,] -1.34151838  0.40833518  0.48712332  0.636731051\n#&gt; [44,]  0.54503180  1.45671524 -0.29077592 -0.081486749\n#&gt; [45,]  2.77325613 -1.38819435 -0.83280797 -0.143433697\n#&gt; [46,]  0.09536670 -0.19772785 -0.01159482  0.209246429\n#&gt; [47,]  0.21472339  0.96037394 -0.61859067 -0.218628161\n#&gt; [48,]  2.08739306 -1.41052627 -0.10372163  0.130583080\n#&gt; [49,]  2.05881199  0.60512507  0.13746933  0.182253407\n#&gt; [50,]  0.62310061 -0.31778662  0.23824049 -0.164976866\n\n\n\n1.1.6.3 主成分标准差\n\n\\[\nStandard \\ Deviation \\ of \\ PC_i =\\frac{\\sigma_i}{\\sqrt{n-1}}\n\\]\n其中，n 是样本量，σ 是奇异值。\n\nCodesvd$d /sqrt(nrow(df_center)-1)\n#&gt; [1] 1.5748783 0.9948694 0.5971291 0.4164494\n\n\n\n1.1.6.4 方差贡献百分比\n在主成分分析 (PCA) 中，方差贡献百分比（variance explained ratio）是用来衡量每个主成分解释了数据总方差的比例。这个比例可以通过奇异值分解 (SVD) 的奇异值来计算。\n具体来说，方差贡献百分比的计算过程如下：\n\n计算每个主成分的方差。奇异值的平方 \\(\\sigma_i^2\\) 表示主成分 \\(𝑖\\)的方差。\n计算总方差，即所有奇异值的平方和。\n每个主成分的方差贡献百分比可以通过将每个奇异值的平方除以总方差来计算。\n\n\nCode# 方差贡献百分比\npct &lt;- svd$d^2/sum(svd$d^2)\npct\n#&gt; [1] 0.62006039 0.24744129 0.08914080 0.04335752\n\n# 累计方差贡献百分比\ncumsum(pct)\n#&gt; [1] 0.6200604 0.8675017 0.9566425 1.0000000",
    "crumbs": [
      "预测性模型",
      "无监督学习（Unsupervised Learning）",
      "主成分分析"
    ]
  },
  {
    "objectID": "PCA.html#内置pca",
    "href": "PCA.html#内置pca",
    "title": "",
    "section": "\n1.2 内置PCA",
    "text": "1.2 内置PCA\n\nCodepca &lt;- prcomp(df_center)\npca\n#&gt; Standard deviations (1, .., p=4):\n#&gt; [1] 1.5748783 0.9948694 0.5971291 0.4164494\n#&gt; \n#&gt; Rotation (n x k) = (4 x 4):\n#&gt;                 PC1        PC2        PC3         PC4\n#&gt; Murder   -0.5358995 -0.4181809  0.3412327  0.64922780\n#&gt; Assault  -0.5831836 -0.1879856  0.2681484 -0.74340748\n#&gt; UrbanPop -0.2781909  0.8728062  0.3780158  0.13387773\n#&gt; Rape     -0.5434321  0.1673186 -0.8177779  0.08902432\nsummary(pca)\n#&gt; Importance of components:\n#&gt;                           PC1    PC2     PC3     PC4\n#&gt; Standard deviation     1.5749 0.9949 0.59713 0.41645\n#&gt; Proportion of Variance 0.6201 0.2474 0.08914 0.04336\n#&gt; Cumulative Proportion  0.6201 0.8675 0.95664 1.00000",
    "crumbs": [
      "预测性模型",
      "无监督学习（Unsupervised Learning）",
      "主成分分析"
    ]
  },
  {
    "objectID": "PCA.html#可视化分析",
    "href": "PCA.html#可视化分析",
    "title": "",
    "section": "\n1.3 可视化分析",
    "text": "1.3 可视化分析\n\n1.3.1 判断主成分的个数\n\nCattell碎石图 图形变化最大处，即拐角处\nKaiser-Harris准则 特征值大于1，直线y=1以上\n平行分析 基于真实数据的特征值大于一组随机数据矩阵相应的特征值（虚线）\n\n1.3.2 碎石图\n\nCode# Create Scree Plot\nscreeplot(pca, type = \"lines\", main = \"Scree Plot\")\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\nexplained_variance &lt;- pca$sdev^2 / sum(pca$sdev^2)\nexplained_variance_df &lt;- data.frame(\n  Principal_Component = paste0(\"PC\", 1:length(explained_variance)),\n  Explained_Variance = explained_variance\n)\n\nggplot(explained_variance_df, aes(x = Principal_Component, y = Explained_Variance)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  geom_line(aes(group = 1), color = \"blue\") +\n  geom_point(color = \"red\") +\n  labs(title = \"Scree Plot\", x = \"Principal Component\", y = \"Explained Variance\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n1.3.2.1 平行分析\n\nCodefa_parallel &lt;- psych::fa.parallel(df_center, fa = \"pc\", n.iter = 100)\n\n\n\n\n\n\n#&gt; Parallel analysis suggests that the number of factors =  NA  and the number of components =  1\n\n\nCodesvd$v\n#&gt;            [,1]       [,2]       [,3]        [,4]\n#&gt; [1,] -0.5358995 -0.4181809  0.3412327  0.64922780\n#&gt; [2,] -0.5831836 -0.1879856  0.2681484 -0.74340748\n#&gt; [3,] -0.2781909  0.8728062  0.3780158  0.13387773\n#&gt; [4,] -0.5434321  0.1673186 -0.8177779  0.08902432\ntibble(x = 1:4, pc1 = svd$v[, 1]) %&gt;%\n    ggplot(aes(x, pc1)) +\n    geom_point() +\n    theme_classic() +\n    theme(panel.border = element_rect(\n        color = \"black\",\n        fill = NA,\n    ), # 添加四周框线\n    )\n\n\n\n\n\n\nCode\nplot(svd$v[,1],ylab = \"1st PC\")\n\n\n\n\n\n\nCodeplot(svd$v[,1],svd$v[,2],xlab=\"lst PC\",ylab=\"2nd PC\")",
    "crumbs": [
      "预测性模型",
      "无监督学习（Unsupervised Learning）",
      "主成分分析"
    ]
  },
  {
    "objectID": "PCA.html#tidy-主成分分析",
    "href": "PCA.html#tidy-主成分分析",
    "title": "",
    "section": "\n1.4 tidy 主成分分析",
    "text": "1.4 tidy 主成分分析\n\nCodelibrary(tidymodels)\n\n\n\nCodedf &lt;- as_tibble(USArrests, rownames = \"state\")\ndf\n#&gt; # A tibble: 50 × 5\n#&gt;    state       Murder Assault UrbanPop  Rape\n#&gt;    &lt;chr&gt;        &lt;dbl&gt;   &lt;int&gt;    &lt;int&gt; &lt;dbl&gt;\n#&gt;  1 Alabama       13.2     236       58  21.2\n#&gt;  2 Alaska        10       263       48  44.5\n#&gt;  3 Arizona        8.1     294       80  31  \n#&gt;  4 Arkansas       8.8     190       50  19.5\n#&gt;  5 California     9       276       91  40.6\n#&gt;  6 Colorado       7.9     204       78  38.7\n#&gt;  7 Connecticut    3.3     110       77  11.1\n#&gt;  8 Delaware       5.9     238       72  15.8\n#&gt;  9 Florida       15.4     335       80  31.9\n#&gt; 10 Georgia       17.4     211       60  25.8\n#&gt; # ℹ 40 more rows\n\ndf |&gt;\n  select(-state) |&gt;\n  map_dfr(mean)  #apply(.,2,mean)\n#&gt; # A tibble: 1 × 4\n#&gt;   Murder Assault UrbanPop  Rape\n#&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1   7.79    171.     65.5  21.2\n\n\n\nCodedf_pca &lt;- df |&gt;\n  select(-state) |&gt;\n  stats::prcomp(scale = TRUE)\n\n\n主成分得分，表示主成分与原有观测的相关系数\n\nCodedf_pca$x\n#&gt;               PC1         PC2         PC3          PC4\n#&gt;  [1,] -0.97566045 -1.12200121  0.43980366  0.154696581\n#&gt;  [2,] -1.93053788 -1.06242692 -2.01950027 -0.434175454\n#&gt;  [3,] -1.74544285  0.73845954 -0.05423025 -0.826264240\n#&gt;  [4,]  0.13999894 -1.10854226 -0.11342217 -0.180973554\n#&gt;  [5,] -2.49861285  1.52742672 -0.59254100 -0.338559240\n#&gt;  [6,] -1.49934074  0.97762966 -1.08400162  0.001450164\n#&gt;  [7,]  1.34499236  1.07798362  0.63679250 -0.117278736\n#&gt;  [8,] -0.04722981  0.32208890  0.71141032 -0.873113315\n#&gt;  [9,] -2.98275967 -0.03883425  0.57103206 -0.095317042\n#&gt; [10,] -1.62280742 -1.26608838  0.33901818  1.065974459\n#&gt; [11,]  0.90348448  1.55467609 -0.05027151  0.893733198\n#&gt; [12,]  1.62331903 -0.20885253 -0.25719021 -0.494087852\n#&gt; [13,] -1.36505197  0.67498834  0.67068647 -0.120794916\n#&gt; [14,]  0.50038122  0.15003926 -0.22576277  0.420397595\n#&gt; [15,]  2.23099579  0.10300828 -0.16291036  0.017379470\n#&gt; [16,]  0.78887206  0.26744941 -0.02529648  0.204421034\n#&gt; [17,]  0.74331256 -0.94880748  0.02808429  0.663817237\n#&gt; [18,] -1.54909076 -0.86230011  0.77560598  0.450157791\n#&gt; [19,]  2.37274014 -0.37260865  0.06502225 -0.327138529\n#&gt; [20,] -1.74564663 -0.42335704  0.15566968 -0.553450589\n#&gt; [21,]  0.48128007  1.45967706  0.60337172 -0.177793902\n#&gt; [22,] -2.08725025  0.15383500 -0.38100046  0.101343128\n#&gt; [23,]  1.67566951  0.62590670 -0.15153200  0.066640316\n#&gt; [24,] -0.98647919 -2.36973712  0.73336290  0.213342049\n#&gt; [25,] -0.68978426  0.26070794 -0.37365033  0.223554811\n#&gt; [26,]  1.17353751 -0.53147851 -0.24440796  0.122498555\n#&gt; [27,]  1.25291625  0.19200440 -0.17380930  0.015733156\n#&gt; [28,] -2.84550542  0.76780502 -1.15168793  0.311354436\n#&gt; [29,]  2.35995585  0.01790055 -0.03648498 -0.032804291\n#&gt; [30,] -0.17974128  1.43493745  0.75677041  0.240936580\n#&gt; [31,] -1.96012351 -0.14141308 -0.18184598 -0.336121113\n#&gt; [32,] -1.66566662  0.81491072  0.63661186 -0.013348844\n#&gt; [33,] -1.11208808 -2.20561081  0.85489245 -0.944789648\n#&gt; [34,]  2.96215223 -0.59309738 -0.29824930 -0.251434626\n#&gt; [35,]  0.22369436  0.73477837  0.03082616  0.469152817\n#&gt; [36,]  0.30864928  0.28496113  0.01515592  0.010228476\n#&gt; [37,] -0.05852787  0.53596999 -0.93038718 -0.235390872\n#&gt; [38,]  0.87948680  0.56536050  0.39660218  0.355452378\n#&gt; [39,]  0.85509072  1.47698328  1.35617705 -0.607402746\n#&gt; [40,] -1.30744986 -1.91397297  0.29751723 -0.130145378\n#&gt; [41,]  1.96779669 -0.81506822 -0.38538073 -0.108470512\n#&gt; [42,] -0.98969377 -0.85160534 -0.18619262  0.646302674\n#&gt; [43,] -1.34151838  0.40833518  0.48712332  0.636731051\n#&gt; [44,]  0.54503180  1.45671524 -0.29077592 -0.081486749\n#&gt; [45,]  2.77325613 -1.38819435 -0.83280797 -0.143433697\n#&gt; [46,]  0.09536670 -0.19772785 -0.01159482  0.209246429\n#&gt; [47,]  0.21472339  0.96037394 -0.61859067 -0.218628161\n#&gt; [48,]  2.08739306 -1.41052627 -0.10372163  0.130583080\n#&gt; [49,]  2.05881199  0.60512507  0.13746933  0.182253407\n#&gt; [50,]  0.62310061 -0.31778662  0.23824049 -0.164976866\n\n# by default    df_pca$x\nbroom::tidy(df_pca, matrix = \"scores\") |&gt; \n    pivot_wider(id_cols = everything(),\n                names_from = PC,\n                names_prefix = \"PC\",\n               values_from = value)\n#&gt; # A tibble: 50 × 5\n#&gt;      row     PC1     PC2     PC3      PC4\n#&gt;    &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1     1 -0.976  -1.12    0.440   0.155  \n#&gt;  2     2 -1.93   -1.06   -2.02   -0.434  \n#&gt;  3     3 -1.75    0.738  -0.0542 -0.826  \n#&gt;  4     4  0.140  -1.11   -0.113  -0.181  \n#&gt;  5     5 -2.50    1.53   -0.593  -0.339  \n#&gt;  6     6 -1.50    0.978  -1.08    0.00145\n#&gt;  7     7  1.34    1.08    0.637  -0.117  \n#&gt;  8     8 -0.0472  0.322   0.711  -0.873  \n#&gt;  9     9 -2.98   -0.0388  0.571  -0.0953 \n#&gt; 10    10 -1.62   -1.27    0.339   1.07   \n#&gt; # ℹ 40 more rows\n\n\n主成分荷载（loading）：表示主成分与原有变量的相关系数\n\nCodedf_pca$rotation\n#&gt;                 PC1        PC2        PC3         PC4\n#&gt; Murder   -0.5358995 -0.4181809  0.3412327  0.64922780\n#&gt; Assault  -0.5831836 -0.1879856  0.2681484 -0.74340748\n#&gt; UrbanPop -0.2781909  0.8728062  0.3780158  0.13387773\n#&gt; Rape     -0.5434321  0.1673186 -0.8177779  0.08902432\n\n# df_pca$Rotation\ntidy(df_pca, matrix = \"loadings\") |&gt; \n    pivot_wider(\n        names_from = PC,\n        names_prefix = \"PC\",\n        values_from = value,\n    )\n#&gt; # A tibble: 4 × 5\n#&gt;   column      PC1    PC2    PC3     PC4\n#&gt;   &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 Murder   -0.536 -0.418  0.341  0.649 \n#&gt; 2 Assault  -0.583 -0.188  0.268 -0.743 \n#&gt; 3 UrbanPop -0.278  0.873  0.378  0.134 \n#&gt; 4 Rape     -0.543  0.167 -0.818  0.0890\n\n\n例如：\n\\[\nPC_1=-0.536Murrder-0.583Assault-0.278UrbanPop-0.543Rape\n\\]\n\nCode\ntidy(df_pca, matrix = \"loadings\") |&gt;\n  ggplot(aes(value, column)) +\n  facet_wrap(~ PC) +\n  geom_col() +\n  scale_x_continuous(labels = scales::percent)\n\n\n\n\n\n\n\n特征值 eigenvalues，高维椭球的主轴长度，相关矩阵的特征值。\n方差百分比贡献。\n\nCode# screen plot\ntidy(df_pca, matrix = \"eigenvalues\") |&gt;\n    ggplot(aes(PC, percent)) +\n    geom_point(color = \"red\") +\n    geom_line()+\n    scale_y_continuous(labels = scales::percent)",
    "crumbs": [
      "预测性模型",
      "无监督学习（Unsupervised Learning）",
      "主成分分析"
    ]
  },
  {
    "objectID": "LDA.html",
    "href": "LDA.html",
    "title": "",
    "section": "",
    "text": "无监督学习（Unsupervised Learning）线性判别分析 CodeShow All CodeHide All CodeView Source\n1 线性判别分析\nFisher 线性判别分析（Linear Discriminant Analysis, LDA）：用于分类任务的降维技术。\nFisher判别法试图最大化类间差异（不同类别的数据点彼此远离）并最小化类内差异（同一类别的数据点尽可能聚集。\n它侧重于最大化类间差异（between-class variance）与类内差异（within-class variance）的比率\n\n1.0.1 MASS\n\nCode# 加载MASS包，它包含了lda函数\nlibrary(MASS)\n\n# 加载内置的鸢尾花数据集\ndata(iris)\n\n# 查看数据集结构\nstr(iris)\n#&gt; 'data.frame':    150 obs. of  5 variables:\n#&gt;  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n#&gt;  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n#&gt;  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n#&gt;  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n#&gt;  $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n# 应用Fisher线性判别分析\n# 使用鸢尾花数据集的前四列作为特征，Species作为类别\nlda_model &lt;- lda(Species ~ ., data=iris)\n\n# 查看判别模型的摘要\nsummary(lda_model)\n#&gt;         Length Class  Mode     \n#&gt; prior    3     -none- numeric  \n#&gt; counts   3     -none- numeric  \n#&gt; means   12     -none- numeric  \n#&gt; scaling  8     -none- numeric  \n#&gt; lev      3     -none- character\n#&gt; svd      2     -none- numeric  \n#&gt; N        1     -none- numeric  \n#&gt; call     3     -none- call     \n#&gt; terms    3     terms  call     \n#&gt; xlevels  0     -none- list\nlda_model\n#&gt; Call:\n#&gt; lda(Species ~ ., data = iris)\n#&gt; \n#&gt; Prior probabilities of groups:\n#&gt;     setosa versicolor  virginica \n#&gt;  0.3333333  0.3333333  0.3333333 \n#&gt; \n#&gt; Group means:\n#&gt;            Sepal.Length Sepal.Width Petal.Length Petal.Width\n#&gt; setosa            5.006       3.428        1.462       0.246\n#&gt; versicolor        5.936       2.770        4.260       1.326\n#&gt; virginica         6.588       2.974        5.552       2.026\n#&gt; \n#&gt; Coefficients of linear discriminants:\n#&gt;                     LD1         LD2\n#&gt; Sepal.Length  0.8293776 -0.02410215\n#&gt; Sepal.Width   1.5344731 -2.16452123\n#&gt; Petal.Length -2.2012117  0.93192121\n#&gt; Petal.Width  -2.8104603 -2.83918785\n#&gt; \n#&gt; Proportion of trace:\n#&gt;    LD1    LD2 \n#&gt; 0.9912 0.0088\n# 打印判别函数的系数\nprint(lda_model$coefficients)\n#&gt; NULL\n\n# 使用判别模型对数据进行分类\npredicted_species &lt;- predict(lda_model, iris)\n\n# 计算准确率\naccuracy &lt;- sum(predicted_species$class == iris$Species) / nrow(iris)\nprint(paste(\"分类准确率:\", accuracy))\n#&gt; [1] \"分类准确率: 0.98\"\n\n# 可视化判别结果\nplot(lda_model)\n\n\n\n\n\n\n\n\n\n模型调用（Call）:\n\n显示了创建LDA模型时使用的函数调用。在这个例子中，模型使用鸢尾花数据集的所有特征（Sepal.Length, Sepal.Width, Petal.Length, Petal.Width）来预测物种（Species）。\n\n\n\n组的先验概率（Prior probabilities of groups）:\n\n显示了每个物种（setosa, versicolor, virginica）的先验概率。这里每个物种的先验概率都是0.3333，意味着在没有任何额外信息的情况下，每个物种出现的概率是相同的。\n\n\n\n组内均值（Group means）:\n\n显示了每个物种在各个特征上的均值。例如，setosa物种的花萼长度（Sepal.Length）均值是5.006，花萼宽度（Sepal.Width）均值是3.428，花瓣长度（Petal.Length）均值是1.462，花瓣宽度（Petal.Width）均值是0.246。\n\n\n\n线性判别系数（Coefficients of linear discriminants）:\n\n显示了两个线性判别函数（LD1和LD2）的系数。这些系数用于计算判别分数，以区分不同的物种。例如，LD1判别函数中，花萼长度（Sepal.Length）的系数是0.8293776，花萼宽度（Sepal.Width）的系数是1.5344731，以此类推。\n\n\n\n特征值的比例（Proportion of trace）:\n\n显示了每个线性判别函数对总方差的解释比例。在这个例子中，LD1解释了99.12%的方差，而LD2仅解释了0.88%的方差。这表明LD1是主要的判别方向，而LD2的贡献相对较小。\n\n\n\n如何使用这些信息：\n\n可以使用这些系数来计算每个观测值在LD1和LD2上的判别分数。判别分数的计算公式为： LD1=0.8293776×Sepal.Length+1.5344731×Sepal.Width−2.2012117×Petal.Length−2.8104603×Petal.WidthLD1=0.8293776×Sepal.Length+1.5344731×Sepal.Width−2.2012117×Petal.Length−2.8104603×Petal.Width LD2=−0.02410215×Sepal.Length−2.16452123×Sepal.Width+0.93192121×Petal.Length−2.83918785×Petal.WidthLD2=−0.02410215×Sepal.Length−2.16452123×Sepal.Width+0.93192121×Petal.Length−2.83918785×Petal.Width\n通常，主要的判别函数（在这个例子中是LD1）足以进行有效的分类。如果需要，也可以使用LD2作为辅助。\n根据判别分数，可以确定每个观测值最有可能属于的物种类别。\n\n1.0.2 tidymodels\n\nCodelibrary(tidymodels)\n#&gt; ── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n#&gt; ✔ broom        1.0.6     ✔ rsample      1.2.1\n#&gt; ✔ dials        1.2.1     ✔ tune         1.2.1\n#&gt; ✔ infer        1.0.7     ✔ workflows    1.1.4\n#&gt; ✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n#&gt; ✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n#&gt; ✔ recipes      1.1.0\nlibrary(discrim)\n\n\n\nCodeSmarket &lt;- read_csv(\"data/Smarket.csv\")\n#&gt; Rows: 1250 Columns: 9\n#&gt; ── Column specification ────────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; chr (1): Direction\n#&gt; dbl (8): Year, Lag1, Lag2, Lag3, Lag4, Lag5, Volume, Today\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nSmarket$Direction &lt;- factor(Smarket$Direction)\nhead(Smarket)\n#&gt; # A tibble: 6 × 9\n#&gt;    Year   Lag1   Lag2   Lag3   Lag4   Lag5 Volume  Today Direction\n#&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;    \n#&gt; 1  2001  0.381 -0.192 -2.62  -1.06   5.01    1.19  0.959 Up       \n#&gt; 2  2001  0.959  0.381 -0.192 -2.62  -1.06    1.30  1.03  Up       \n#&gt; 3  2001  1.03   0.959  0.381 -0.192 -2.62    1.41 -0.623 Down     \n#&gt; 4  2001 -0.623  1.03   0.959  0.381 -0.192   1.28  0.614 Up       \n#&gt; 5  2001  0.614 -0.623  1.03   0.959  0.381   1.21  0.213 Up       \n#&gt; 6  2001  0.213  0.614 -0.623  1.03   0.959   1.35  1.39  Up\n\n\n\nCodelda_spec &lt;- discrim_linear() %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"MASS\")\nlda_fit &lt;- lda_spec %&gt;%\n  fit(Direction ~ Lag1 + Lag2, data = Smarket)\n\nlda_fit\n#&gt; parsnip model object\n#&gt; \n#&gt; Call:\n#&gt; lda(Direction ~ Lag1 + Lag2, data = data)\n#&gt; \n#&gt; Prior probabilities of groups:\n#&gt;   Down     Up \n#&gt; 0.4816 0.5184 \n#&gt; \n#&gt; Group means:\n#&gt;             Lag1        Lag2\n#&gt; Down  0.05068605  0.03229734\n#&gt; Up   -0.03969136 -0.02244444\n#&gt; \n#&gt; Coefficients of linear discriminants:\n#&gt;             LD1\n#&gt; Lag1 -0.7567605\n#&gt; Lag2 -0.4707872\n\n\n\nCodepredict(lda_fit, new_data = Smarket)\n#&gt; # A tibble: 1,250 × 1\n#&gt;    .pred_class\n#&gt;    &lt;fct&gt;      \n#&gt;  1 Up         \n#&gt;  2 Down       \n#&gt;  3 Down       \n#&gt;  4 Up         \n#&gt;  5 Up         \n#&gt;  6 Up         \n#&gt;  7 Down       \n#&gt;  8 Up         \n#&gt;  9 Up         \n#&gt; 10 Down       \n#&gt; # ℹ 1,240 more rows\npredict(lda_fit, new_data = Smarket, type = \"prob\")\n#&gt; # A tibble: 1,250 × 2\n#&gt;    .pred_Down .pred_Up\n#&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1      0.486    0.514\n#&gt;  2      0.503    0.497\n#&gt;  3      0.510    0.490\n#&gt;  4      0.482    0.518\n#&gt;  5      0.485    0.515\n#&gt;  6      0.492    0.508\n#&gt;  7      0.509    0.491\n#&gt;  8      0.490    0.510\n#&gt;  9      0.477    0.523\n#&gt; 10      0.505    0.495\n#&gt; # ℹ 1,240 more rows\naugment(lda_fit, new_data = Smarket) %&gt;%\n  conf_mat(truth = Direction, estimate = .pred_class) \n#&gt;           Truth\n#&gt; Prediction Down  Up\n#&gt;       Down  114 102\n#&gt;       Up    488 546\n\naugment(lda_fit, new_data = Smarket) %&gt;%\n  accuracy(truth = Direction, estimate = .pred_class) \n#&gt; # A tibble: 1 × 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.528\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "预测性模型",
      "无监督学习（Unsupervised Learning）",
      "线性判别分析"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\n推荐阅读\n\nMachine Learning with R (4E)\nTidy Modeling with R\ntidymodels\nt-Distributed Stochastic Neighbor Embedding (t-SNE)\nUniform Manifold Approximation and Projection (UMAP)\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "cross_validation.html",
    "href": "cross_validation.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All CodeView Source",
    "crumbs": [
      "预测性模型",
      "交叉验证"
    ]
  },
  {
    "objectID": "cross_validation.html#tidymodels",
    "href": "cross_validation.html#tidymodels",
    "title": "",
    "section": "\n1.1 tidymodels",
    "text": "1.1 tidymodels\n\nCode# 定义模型\nlog_reg &lt;- logistic_reg() %&gt;% \n  set_engine(\"glm\")\n\n# 建立工作流\nfull_workflow &lt;- workflow() %&gt;%\n  add_model(log_reg) %&gt;%\n  add_formula(default ~ .)\n\n# 拟合全模型\nfull_fit &lt;- full_workflow %&gt;% \n  fit(data = train_data)\n\n# 预测并评估全模型\nfull_predictions &lt;- full_fit %&gt;% \n  predict(test_data, type = \"prob\") %&gt;%\n  bind_cols(test_data %&gt;% select(default))\n\n\n# 绘制全模型的ROC曲线\nfull_roc &lt;- roc_curve(full_predictions,\n                      truth = default,\n                      .pred_1,\n                      event_level = \"second\") %&gt;%\n         #第二级逻辑将结果编码为0/1（在这种情况下，第二个值是事件）\n    autoplot() + ggtitle(\"ROC Curve - Full Model\")\n\nprint(full_roc)\n\n\n\n\n\n\nCode\n# 计算校准曲线\nfull_predictions &lt;- full_predictions %&gt;%\n  mutate(pred_bin = cut(.pred_1, breaks = seq(0, 1, by = 0.1)))\n\ncalibration_data &lt;- full_predictions %&gt;%\n  group_by(pred_bin) %&gt;%\n  dplyr::summarize(mean_pred = mean(.pred_1), \n            mean_actual = mean(default == \"1\"))\n\nggplot(calibration_data, aes(x = mean_pred, y = mean_actual)) +\n  geom_point() +\n  geom_line() +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") +\n  xlim(0, 1) + ylim(0, 1) +\n  ggtitle(\"Calibration Curve - Full Model\") +\n  xlab(\"Predicted Probability\") +\n  ylab(\"Observed Probability\")\n\n\n\n\n\n\n\n\nCode# 定义交叉验证\ncv_5 &lt;- vfold_cv(train_data, v = 5)\ncv_10 &lt;- vfold_cv(train_data, v = 10)\n\n# 5折交叉验证\ncv_5_results &lt;- fit_resamples(\n  full_workflow,\n  resamples = cv_5,\n  metrics = metric_set(roc_auc),\n  control = control_resamples(save_pred = TRUE)\n)\n\n# 10折交叉验证\ncv_10_results &lt;- fit_resamples(\n  full_workflow,\n  resamples = cv_10,\n  metrics = metric_set(roc_auc),\n  control = control_resamples(save_pred = TRUE)\n)\n\n# 预测和评估5折交叉验证模型\ncv_5_predictions &lt;- collect_predictions(cv_5_results)\n\n# 绘制5折交叉验证模型的ROC曲线\ncv_5_roc &lt;- roc_curve(cv_5_predictions, truth = default, .pred_1 , event_level = \"second\") %&gt;% \n  autoplot() + ggtitle(\"ROC Curve - 5-fold Cross-Validation Model\")\n\nprint(cv_5_roc)\n\n\n\n\n\n\nCode\n# 计算5折交叉验证的校准曲线\ncv_5_predictions &lt;- cv_5_predictions %&gt;%\n  mutate(pred_bin = cut(.pred_1, breaks = seq(0, 1, by = 0.1)))\n\ncalibration_data_5 &lt;- cv_5_predictions %&gt;%\n  group_by(pred_bin) %&gt;%\n  dplyr::summarize(mean_pred = mean(.pred_1), \n            mean_actual = mean(default == \"1\"))\n\nggplot(calibration_data_5, aes(x = mean_pred, y = mean_actual)) +\n  geom_point() +\n  geom_line() +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") +\n  xlim(0, 1) + ylim(0, 1) +\n  ggtitle(\"Calibration Curve - 5-fold Cross-Validation Model\") +\n  xlab(\"Predicted Probability\") +\n  ylab(\"Observed Probability\")\n\n\n\n\n\n\nCode\n# 预测和评估10折交叉验证模型\ncv_10_predictions &lt;- collect_predictions(cv_10_results)\n\n# 绘制10折交叉验证模型的ROC曲线\ncv_10_roc &lt;- roc_curve(cv_10_predictions, truth = default, .pred_1,  event_level = \"second\") %&gt;% \n  autoplot() + ggtitle(\"ROC Curve - 10-fold Cross-Validation Model\")\n\nprint(cv_10_roc)\n\n\n\n\n\n\nCode\n# 计算10折交叉验证的校准曲线\ncv_10_predictions &lt;- cv_10_predictions %&gt;%\n  mutate(pred_bin = cut(.pred_1, breaks = seq(0, 1, by = 0.1)))\n\ncalibration_data_10 &lt;- cv_10_predictions %&gt;%\n  group_by(pred_bin) %&gt;%\n  dplyr::summarize(mean_pred = mean(.pred_1), \n            mean_actual = mean(default == \"1\"))\n\nggplot(calibration_data_10, aes(x = mean_pred, y = mean_actual)) +\n  geom_point() +\n  geom_line() +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") +\n  xlim(0, 1) + ylim(0, 1) +\n  ggtitle(\"Calibration Curve - 10-fold Cross-Validation Model\") +\n  xlab(\"Predicted Probability\") +\n  ylab(\"Observed Probability\")",
    "crumbs": [
      "预测性模型",
      "交叉验证"
    ]
  },
  {
    "objectID": "Classification.html",
    "href": "Classification.html",
    "title": "",
    "section": "",
    "text": "监督学习（Supervised Learning）分类 CodeShow All CodeHide All CodeView Source",
    "crumbs": [
      "预测性模型",
      "监督学习（Supervised Learning）",
      "分类"
    ]
  },
  {
    "objectID": "Classification.html#支持向量机",
    "href": "Classification.html#支持向量机",
    "title": "",
    "section": "\n1.1 支持向量机",
    "text": "1.1 支持向量机\n支持向量机（Support Vector Machine, SVM）：用于二分类和多分类问题，寻找最佳决策边界。",
    "crumbs": [
      "预测性模型",
      "监督学习（Supervised Learning）",
      "分类"
    ]
  },
  {
    "objectID": "Classification.html#k-nearest-neighbors",
    "href": "Classification.html#k-nearest-neighbors",
    "title": "",
    "section": "\n1.2 K-Nearest Neighbors",
    "text": "1.2 K-Nearest Neighbors\n\nCodeSmarket &lt;- read_csv(\"data/Smarket.csv\")\n#&gt; Rows: 1250 Columns: 9\n#&gt; ── Column specification ────────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; chr (1): Direction\n#&gt; dbl (8): Year, Lag1, Lag2, Lag3, Lag4, Lag5, Volume, Today\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nSmarket$Direction &lt;- factor(Smarket$Direction)\nhead(Smarket)\n#&gt; # A tibble: 6 × 9\n#&gt;    Year   Lag1   Lag2   Lag3   Lag4   Lag5 Volume  Today Direction\n#&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;    \n#&gt; 1  2001  0.381 -0.192 -2.62  -1.06   5.01    1.19  0.959 Up       \n#&gt; 2  2001  0.959  0.381 -0.192 -2.62  -1.06    1.30  1.03  Up       \n#&gt; 3  2001  1.03   0.959  0.381 -0.192 -2.62    1.41 -0.623 Down     \n#&gt; 4  2001 -0.623  1.03   0.959  0.381 -0.192   1.28  0.614 Up       \n#&gt; 5  2001  0.614 -0.623  1.03   0.959  0.381   1.21  0.213 Up       \n#&gt; 6  2001  0.213  0.614 -0.623  1.03   0.959   1.35  1.39  Up\n\n\n\nCodeknn_spec &lt;- nearest_neighbor(neighbors = 3) |&gt; \n  set_mode(\"classification\") |&gt; \n  set_engine(\"kknn\")\n\nknn_fit &lt;- knn_spec |&gt;\n  fit(Direction ~ Lag1 + Lag2, data = Smarket)\n\nknn_fit\n#&gt; parsnip model object\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; kknn::train.kknn(formula = Direction ~ Lag1 + Lag2, data = data,     ks = min_rows(3, data, 5))\n#&gt; \n#&gt; Type of response variable: nominal\n#&gt; Minimal misclassification: 0.5064\n#&gt; Best kernel: optimal\n#&gt; Best k: 3\n\n\n\nCodeaugment(knn_fit, new_data = Smarket) |&gt; \n  conf_mat(truth = Direction, estimate = .pred_class) \n#&gt;           Truth\n#&gt; Prediction Down  Up\n#&gt;       Down  602   0\n#&gt;       Up      0 648\n\n\n\nCodeaugment(knn_fit, new_data = Smarket) |&gt;\n  accuracy(truth = Direction, estimate = .pred_class) \n#&gt; # A tibble: 1 × 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary             1",
    "crumbs": [
      "预测性模型",
      "监督学习（Supervised Learning）",
      "分类"
    ]
  },
  {
    "objectID": "Classification.html#lda",
    "href": "Classification.html#lda",
    "title": "",
    "section": "\n1.3 LDA",
    "text": "1.3 LDA\n\nCodelda_spec &lt;- discrim_linear() %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"MASS\")\nlda_fit &lt;- lda_spec %&gt;%\n  fit(Direction ~ Lag1 + Lag2, data = Smarket)\n\nlda_fit\n#&gt; parsnip model object\n#&gt; \n#&gt; Call:\n#&gt; lda(Direction ~ Lag1 + Lag2, data = data)\n#&gt; \n#&gt; Prior probabilities of groups:\n#&gt;   Down     Up \n#&gt; 0.4816 0.5184 \n#&gt; \n#&gt; Group means:\n#&gt;             Lag1        Lag2\n#&gt; Down  0.05068605  0.03229734\n#&gt; Up   -0.03969136 -0.02244444\n#&gt; \n#&gt; Coefficients of linear discriminants:\n#&gt;             LD1\n#&gt; Lag1 -0.7567605\n#&gt; Lag2 -0.4707872",
    "crumbs": [
      "预测性模型",
      "监督学习（Supervised Learning）",
      "分类"
    ]
  },
  {
    "objectID": "Classification.html#模型比较",
    "href": "Classification.html#模型比较",
    "title": "",
    "section": "\n1.4 模型比较",
    "text": "1.4 模型比较\n\nCodemodels &lt;- list(\"LDA\" = lda_fit,\n               \"KNN\" = knn_fit)\npreds &lt;- imap_dfr(models, augment, \n                  new_data = Smarket, .id = \"model\")\n\npreds %&gt;%\n  dplyr::select(model, Direction, .pred_class, .pred_Down, .pred_Up)\n#&gt; # A tibble: 2,500 × 5\n#&gt;    model Direction .pred_class .pred_Down .pred_Up\n#&gt;    &lt;chr&gt; &lt;fct&gt;     &lt;fct&gt;            &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1 LDA   Up        Up               0.486    0.514\n#&gt;  2 LDA   Up        Down             0.503    0.497\n#&gt;  3 LDA   Down      Down             0.510    0.490\n#&gt;  4 LDA   Up        Up               0.482    0.518\n#&gt;  5 LDA   Up        Up               0.485    0.515\n#&gt;  6 LDA   Up        Up               0.492    0.508\n#&gt;  7 LDA   Down      Down             0.509    0.491\n#&gt;  8 LDA   Up        Up               0.490    0.510\n#&gt;  9 LDA   Up        Up               0.477    0.523\n#&gt; 10 LDA   Up        Down             0.505    0.495\n#&gt; # ℹ 2,490 more rows\n\n\n\n1.4.1 灵敏度和特异性\n\nCodemulti_metric &lt;- metric_set( sensitivity, specificity)  # accuracy\n\n\npreds %&gt;%\n  group_by(model) %&gt;%\n  multi_metric(truth = Direction, estimate = .pred_class)\n#&gt; # A tibble: 4 × 4\n#&gt;   model .metric     .estimator .estimate\n#&gt;   &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 KNN   sensitivity binary         1    \n#&gt; 2 LDA   sensitivity binary         0.189\n#&gt; 3 KNN   specificity binary         1    \n#&gt; 4 LDA   specificity binary         0.843\n\n\n\n1.4.2 ROC 曲线\n\nCodepreds %&gt;%\n  group_by(model) %&gt;%\n  roc_curve(Direction, .pred_Down) %&gt;%\n  autoplot()",
    "crumbs": [
      "预测性模型",
      "监督学习（Supervised Learning）",
      "分类"
    ]
  },
  {
    "objectID": "Classification.html#朴素贝叶斯",
    "href": "Classification.html#朴素贝叶斯",
    "title": "",
    "section": "\n1.5 朴素贝叶斯\n",
    "text": "1.5 朴素贝叶斯\n\n朴素贝叶斯（Naive Bayes）：基于贝叶斯定理的简单而高效的分类算法。",
    "crumbs": [
      "预测性模型",
      "监督学习（Supervised Learning）",
      "分类"
    ]
  },
  {
    "objectID": "Classification.html#基于树的模型",
    "href": "Classification.html#基于树的模型",
    "title": "",
    "section": "\n1.6 基于树的模型",
    "text": "1.6 基于树的模型\n\n决策树（Decision Tree）：基于树状模型进行决策的分类算法。\n随机森林（Random Forest）：由多棵决策树组成的集成学习模型。\n梯度提升树（Gradient Boosting Trees）：通过加法模型和前向分步算法实现的集成学习模型。",
    "crumbs": [
      "预测性模型",
      "监督学习（Supervised Learning）",
      "分类"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\nCode1 + 1\n#&gt; [1] 2\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Clustering.html",
    "href": "Clustering.html",
    "title": "",
    "section": "",
    "text": "无监督学习（Unsupervised Learning）聚类 CodeShow All CodeHide All CodeView Source",
    "crumbs": [
      "预测性模型",
      "无监督学习（Unsupervised Learning）",
      "聚类"
    ]
  },
  {
    "objectID": "Clustering.html#划分聚类-partitioning-clustering",
    "href": "Clustering.html#划分聚类-partitioning-clustering",
    "title": "",
    "section": "\n1.1 划分聚类 partitioning clustering",
    "text": "1.1 划分聚类 partitioning clustering\n\n1.1.1 K Means Cluster Specification\nnum_clusters = 3指定中心点（centroids）即类的个数，nstart = 20指定初始位置的个数，希望找到全局最大值而不是局部最大值\n\nCodekmeans_spec &lt;-tidyclust::k_means(num_clusters = 3) %&gt;%\n  set_mode(\"partition\") %&gt;%\n  set_engine(\"stats\") %&gt;%\n  set_args(nstart = 20)\n\nkmeans_spec\n#&gt; K Means Cluster Specification (partition)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   num_clusters = 3\n#&gt; \n#&gt; Engine-Specific Arguments:\n#&gt;   nstart = 20\n#&gt; \n#&gt; Computational engine: stats\n\n\nK-means algorithm starts with random initialization\n\nCodeset.seed(100)\nkmeans_fit &lt;- kmeans_spec %&gt;%\n  fit(~., data = x_df)\n\nkmeans_fit$fit\n#&gt; K-means clustering with 3 clusters of sizes 14, 12, 24\n#&gt; \n#&gt; Cluster means:\n#&gt;           V1         V2\n#&gt; 2 -0.4354713 -0.8929796\n#&gt; 1 -0.0594887  0.8269786\n#&gt; 3  2.6977371 -3.9171729\n#&gt; \n#&gt; Clustering vector:\n#&gt;  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n#&gt;  1  1  2  2  2  2  1  2  1  2  1  1  1  2  2  2  2  1  2  1  2  1  1  1  1  3 \n#&gt; 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 \n#&gt;  3  3  3  3  3  3  3  3  3  3  3  1  3  3  3  3  3  3  3  3  3  3  3  3 \n#&gt; \n#&gt; Within cluster sum of squares by cluster:\n#&gt; [1] 19.593523  9.502891 32.730828\n#&gt;  (between_SS / total_SS =  83.4 %)\n#&gt; \n#&gt; Available components:\n#&gt; \n#&gt; [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n#&gt; [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"\n\nextract_centroids(kmeans_fit)\n#&gt; # A tibble: 3 × 3\n#&gt;   .cluster       V1     V2\n#&gt;   &lt;fct&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 Cluster_1 -0.435  -0.893\n#&gt; 2 Cluster_2 -0.0595  0.827\n#&gt; 3 Cluster_3  2.70   -3.92\nkmeans_fit$fit$centers\n#&gt;           V1         V2\n#&gt; 2 -0.4354713 -0.8929796\n#&gt; 1 -0.0594887  0.8269786\n#&gt; 3  2.6977371 -3.9171729\n\nkmeans_fit$fit$cluster\n#&gt;  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n#&gt;  1  1  2  2  2  2  1  2  1  2  1  1  1  2  2  2  2  1  2  1  2  1  1  1  1  3 \n#&gt; 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 \n#&gt;  3  3  3  3  3  3  3  3  3  3  3  1  3  3  3  3  3  3  3  3  3  3  3  3\n\n\n\nCodepredict(kmeans_fit, new_data = x_df)\n#&gt; # A tibble: 50 × 1\n#&gt;    .pred_cluster\n#&gt;    &lt;fct&gt;        \n#&gt;  1 Cluster_1    \n#&gt;  2 Cluster_1    \n#&gt;  3 Cluster_2    \n#&gt;  4 Cluster_2    \n#&gt;  5 Cluster_2    \n#&gt;  6 Cluster_2    \n#&gt;  7 Cluster_1    \n#&gt;  8 Cluster_2    \n#&gt;  9 Cluster_1    \n#&gt; 10 Cluster_2    \n#&gt; # ℹ 40 more rows\naugment(kmeans_fit, new_data = x_df)\n#&gt; # A tibble: 50 × 3\n#&gt;         V1     V2 .pred_cluster\n#&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;        \n#&gt;  1  0.0187 -0.401 Cluster_1    \n#&gt;  2 -0.184  -0.335 Cluster_1    \n#&gt;  3 -1.37    1.37  Cluster_2    \n#&gt;  4 -0.599   2.14  Cluster_2    \n#&gt;  5  0.295   0.506 Cluster_2    \n#&gt;  6  0.390   0.786 Cluster_2    \n#&gt;  7 -1.21   -0.902 Cluster_1    \n#&gt;  8 -0.364   0.533 Cluster_2    \n#&gt;  9 -1.63   -0.646 Cluster_1    \n#&gt; 10 -0.256   0.291 Cluster_2    \n#&gt; # ℹ 40 more rows\n\n\n\nCodeaugment(kmeans_fit, new_data = x_df) %&gt;%\n  ggplot(aes(V1, V2, color = .pred_cluster)) +\n  geom_point()\n\n\n\n\n\n\n\ntune_cluster()找到最适合的类的数目\n\nCodekmeans_spec_tuned &lt;- kmeans_spec %&gt;% \n  set_args(num_clusters = tune())\n\nkmeans_wf &lt;- workflow() %&gt;%\n  add_model(kmeans_spec_tuned) %&gt;%\n  add_formula(~.)\n\n\n\nCodeset.seed(1000)\nx_boots &lt;- bootstraps(x_df, times = 10)\n\nnum_clusters_grid &lt;- tibble(num_clusters = seq(1, 10))\n\ntune_res &lt;- tune_cluster(\n  object = kmeans_wf,\n  resamples = x_boots,\n  grid = num_clusters_grid\n)\n\n\n\nCodetune_res %&gt;%\n  collect_metrics()\n#&gt; # A tibble: 20 × 7\n#&gt;    num_clusters .metric          .estimator   mean     n std_err .config        \n#&gt;           &lt;int&gt; &lt;chr&gt;            &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;          \n#&gt;  1            1 sse_total        standard   381.      10  10.4   Preprocessor1_…\n#&gt;  2            1 sse_within_total standard   381.      10  10.4   Preprocessor1_…\n#&gt;  3            2 sse_total        standard   381.      10  10.4   Preprocessor1_…\n#&gt;  4            2 sse_within_total standard    81.4     10   4.36  Preprocessor1_…\n#&gt;  5            3 sse_total        standard   381.      10  10.4   Preprocessor1_…\n#&gt;  6            3 sse_within_total standard    56.8     10   3.34  Preprocessor1_…\n#&gt;  7            4 sse_total        standard   381.      10  10.4   Preprocessor1_…\n#&gt;  8            4 sse_within_total standard    40.5     10   2.33  Preprocessor1_…\n#&gt;  9            5 sse_total        standard   381.      10  10.4   Preprocessor1_…\n#&gt; 10            5 sse_within_total standard    29.8     10   1.71  Preprocessor1_…\n#&gt; 11            6 sse_total        standard   381.      10  10.4   Preprocessor1_…\n#&gt; 12            6 sse_within_total standard    21.8     10   1.43  Preprocessor1_…\n#&gt; 13            7 sse_total        standard   381.      10  10.4   Preprocessor1_…\n#&gt; 14            7 sse_within_total standard    17.0     10   1.04  Preprocessor1_…\n#&gt; 15            8 sse_total        standard   381.      10  10.4   Preprocessor1_…\n#&gt; 16            8 sse_within_total standard    13.6     10   0.842 Preprocessor1_…\n#&gt; 17            9 sse_total        standard   381.      10  10.4   Preprocessor1_…\n#&gt; 18            9 sse_within_total standard    11.0     10   0.669 Preprocessor1_…\n#&gt; 19           10 sse_total        standard   381.      10  10.4   Preprocessor1_…\n#&gt; 20           10 sse_within_total standard     8.82    10   0.595 Preprocessor1_…\n\n\nelbow method 找到最理想的类的个数。\n\nCodetune_res %&gt;%\n  autoplot()\n\n\n\n\n\n\n\n调整后的聚类\n\nCodefinal_kmeans &lt;- kmeans_wf %&gt;%\n  update_model(kmeans_spec %&gt;% set_args(num_clusters = 2)) %&gt;%\n  fit(x_df)\n\n\n\nCodeaugment(final_kmeans, new_data = x_df) %&gt;%\n  ggplot(aes(V1, V2, color = .pred_cluster)) +\n  geom_point()",
    "crumbs": [
      "预测性模型",
      "无监督学习（Unsupervised Learning）",
      "聚类"
    ]
  },
  {
    "objectID": "Clustering.html#分层聚类小样本hierarchical-clustering",
    "href": "Clustering.html#分层聚类小样本hierarchical-clustering",
    "title": "",
    "section": "\n1.2 分层聚类(小样本)Hierarchical Clustering",
    "text": "1.2 分层聚类(小样本)Hierarchical Clustering\n算法\n\n定义每个观测为一类\n计算每类与其他各类的距离\n把距离最短的两类合并成新的一类,总的类的个数减一\n重复2,3步骤,直到所有的类聚成单个类为止\n\n\n1.2.1 hclust specification\n\nCoderes_hclust_complete &lt;- tidyclust::hier_clust(linkage_method = \"complete\") %&gt;%\n  fit(~., data = x_df)\n\nres_hclust_average &lt;- hier_clust(linkage_method = \"average\") %&gt;%\n  fit(~., data = x_df)\n\nres_hclust_single &lt;- hier_clust(linkage_method = \"single\") %&gt;%\n  fit(~., data = x_df)\n\n\nfactoextra package 提取模型信息和可视化\n\nCodelibrary(factoextra)\nres_hclust_complete %&gt;%\n  extract_fit_engine() %&gt;%\n  fviz_dend(main = \"complete\", k = 2)\n#&gt; Warning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\n#&gt; of ggplot2 3.3.4.\n#&gt; ℹ The deprecated feature was likely used in the factoextra package.\n#&gt;   Please report the issue at &lt;https://github.com/kassambara/factoextra/issues&gt;.\n\n\n\n\n\n\n\n\nCoderes_hclust_average %&gt;%\n  extract_fit_engine() %&gt;%\n  fviz_dend(main = \"average\", k = 2)\n\n\n\n\n\n\n\n\nCoderes_hclust_single %&gt;%\n  extract_fit_engine() %&gt;%\n  fviz_dend(main = \"single\", k = 2)\n\n\n\n\n\n\n\n\nCodehier_rec &lt;- recipe(~., data = x_df) %&gt;%\n  step_normalize(all_numeric_predictors()) # 标准化\n\nhier_wf &lt;- workflow() %&gt;%\n  add_recipe(hier_rec) %&gt;%\n  add_model(hier_clust(linkage_method = \"complete\"))\n\nhier_fit &lt;- hier_wf %&gt;%\n  fit(data = x_df) \n\nhier_fit %&gt;%\n  extract_fit_engine() %&gt;%\n  fviz_dend(k = 2)",
    "crumbs": [
      "预测性模型",
      "无监督学习（Unsupervised Learning）",
      "聚类"
    ]
  },
  {
    "objectID": "descriptive_models.html",
    "href": "descriptive_models.html",
    "title": "",
    "section": "",
    "text": "Code",
    "crumbs": [
      "描述性模型",
      "**描述性模型**"
    ]
  },
  {
    "objectID": "descriptive_models.html#footnotes",
    "href": "descriptive_models.html#footnotes",
    "title": "",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCleveland, W. 1979. “Robust Locally Weighted Regression and Smoothing Scatterplots.” Journal of the American Statistical Association 74 (368): 829–36.↩︎\nBolstad, B. 2004. Low-Level Analysis of High-Density Oligonucleotide Array Data: Background, Normalization and Summarization. University of California, Berkeley.↩︎",
    "crumbs": [
      "描述性模型",
      "**描述性模型**"
    ]
  },
  {
    "objectID": "inferential_models.html",
    "href": "inferential_models.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\n推断性模型\n推断性模型（Inferential Models）是用于从样本数据中推断总体特征或作出关于总体的结论的统计工具和方法。这些模型不仅仅是描述数据特征，而是利用数据来进行推断、预测或测试假设。推断性模型通常关注于确定数据中变量之间的关系、评估因果关系、做出预测和进行统计推断。\n推断性模型通过假设检验、置信区间估计、回归分析等方法，对总体做出结论或预测：\n\n线性回归模型\n广义线性模型，如 t 检验，方差分析（ANOVA）\n卡方检验\n生存分析\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "推断性模型",
      "**推断性模型**"
    ]
  },
  {
    "objectID": "machine_learning.html",
    "href": "machine_learning.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All CodeView Source",
    "crumbs": [
      "预测性模型",
      "机器学习"
    ]
  },
  {
    "objectID": "machine_learning.html#探索性数据分析",
    "href": "machine_learning.html#探索性数据分析",
    "title": "",
    "section": "\n1.1 探索性数据分析",
    "text": "1.1 探索性数据分析\n\nCodelibrary(tidymodels)\n#&gt; ── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n#&gt; ✔ broom        1.0.6     ✔ rsample      1.2.1\n#&gt; ✔ dials        1.2.1     ✔ tune         1.2.1\n#&gt; ✔ infer        1.0.7     ✔ workflows    1.1.4\n#&gt; ✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n#&gt; ✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n#&gt; ✔ recipes      1.1.0\ndata(ames)\n\nggplot(ames, aes(x = Sale_Price)) + \n  geom_histogram(bins = 50, col= \"white\")\n\n\n\n\n\n\nCode\n\n\n\names &lt;- ames |&gt;mutate(Sale_Price = log10(Sale_Price))\n\nggplot(ames, aes(x = Sale_Price)) + \n  geom_histogram(bins = 50, col= \"white\")+\n    geom_vline(xintercept =quantile(ames$Sale_Price),lty=5 )",
    "crumbs": [
      "预测性模型",
      "机器学习"
    ]
  },
  {
    "objectID": "machine_learning.html#拆分训练集验证集和测试集-rsample",
    "href": "machine_learning.html#拆分训练集验证集和测试集-rsample",
    "title": "",
    "section": "\n1.2 拆分训练集、验证集和测试集 rsample\n",
    "text": "1.2 拆分训练集、验证集和测试集 rsample\n\n\n1.2.1 简单抽样\n\nCodeset.seed(10)\names_split &lt;- initial_split(ames, prop = c(0.8))\names_split\n\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\ndim(ames_train)\n\n\n\n1.2.2 分层抽样\n\nCodeset.seed(100)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\ndim(ames_train)\n#&gt; [1] 2342   74\n\n\n\n1.2.3 验证集\n\nCodeset.seed(101)\n\n# To put 60% into training, 20% in validation, and remaining 20% in testing:\names_split &lt;- initial_validation_split(ames, prop = c(0.6, 0.2),\n                                       strata = Sale_Price)\names_split\n\names_train &lt;- training(ames_split)\names_test &lt;- testing(ames_split)\names_valid &lt;- validation(ames_split)",
    "crumbs": [
      "预测性模型",
      "机器学习"
    ]
  },
  {
    "objectID": "machine_learning.html#模型选择-parsnip",
    "href": "machine_learning.html#模型选择-parsnip",
    "title": "",
    "section": "\n1.3 模型选择 parsnip\n",
    "text": "1.3 模型选择 parsnip\n\n\nCodeparsnip_addin()\n\n\n\nCodeshow_engines('linear_reg')\n#&gt; # A tibble: 7 × 2\n#&gt;   engine mode      \n#&gt;   &lt;chr&gt;  &lt;chr&gt;     \n#&gt; 1 lm     regression\n#&gt; 2 glm    regression\n#&gt; 3 glmnet regression\n#&gt; 4 stan   regression\n#&gt; 5 spark  regression\n#&gt; 6 keras  regression\n#&gt; 7 brulee regression\nshow_engines(\"logistic_reg\")\n#&gt; # A tibble: 7 × 2\n#&gt;   engine    mode          \n#&gt;   &lt;chr&gt;     &lt;chr&gt;         \n#&gt; 1 glm       classification\n#&gt; 2 glmnet    classification\n#&gt; 3 LiblineaR classification\n#&gt; 4 spark     classification\n#&gt; 5 keras     classification\n#&gt; 6 stan      classification\n#&gt; 7 brulee    classification\n\n\n\nCodelm_model &lt;- linear_reg() |&gt; \n    set_engine(\"lm\") |&gt; \n    set_mode(mode = \"regression\")\nlm_model\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lm\n\nlogistic_reg() |&gt;\n  set_mode(\"classification\") |&gt;\n  set_engine(\"glm\") \n#&gt; Logistic Regression Model Specification (classification)\n#&gt; \n#&gt; Computational engine: glm\n\nrand_forest(trees = 1000, min_n = 5) |&gt;\n  set_engine(\"ranger\", verbose = TRUE) |&gt;\n  set_mode(\"regression\") \n#&gt; Random Forest Model Specification (regression)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   trees = 1000\n#&gt;   min_n = 5\n#&gt; \n#&gt; Engine-Specific Arguments:\n#&gt;   verbose = TRUE\n#&gt; \n#&gt; Computational engine: ranger\n\ndecision_tree(min_n = 2) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n#&gt; Decision Tree Model Specification (regression)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   min_n = 2\n#&gt; \n#&gt; Computational engine: rpart",
    "crumbs": [
      "预测性模型",
      "机器学习"
    ]
  },
  {
    "objectID": "machine_learning.html#模型工作流-workflows",
    "href": "machine_learning.html#模型工作流-workflows",
    "title": "",
    "section": "\n1.4 模型工作流 workflows\n",
    "text": "1.4 模型工作流 workflows\n\n\n1.4.1 线性模型\n预处理 Preprocessor\n\nCode\n#  Preprocessor\n# None\n\nlm_wflow &lt;- \n  workflow() %&gt;% \n  add_model(lm_model)\n\nlm_wflow\n#&gt; ══ Workflow ════════════════════════════════════════════════════════════════════\n#&gt; Preprocessor: None\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Model ───────────────────────────────────────────────────────────────────────\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lm\n\n\n# Formula\nlm_wflow &lt;- lm_wflow |&gt; \n    add_formula(Sale_Price ~ Longitude + Latitude)\n\nlm_wflow\n#&gt; ══ Workflow ════════════════════════════════════════════════════════════════════\n#&gt; Preprocessor: Formula\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ────────────────────────────────────────────────────────────────\n#&gt; Sale_Price ~ Longitude + Latitude\n#&gt; \n#&gt; ── Model ───────────────────────────────────────────────────────────────────────\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lm\n\n#  拟合\nlm_fit &lt;- fit(lm_wflow, ames_train)\nlm_fit\n#&gt; ══ Workflow [trained] ══════════════════════════════════════════════════════════\n#&gt; Preprocessor: Formula\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ────────────────────────────────────────────────────────────────\n#&gt; Sale_Price ~ Longitude + Latitude\n#&gt; \n#&gt; ── Model ───────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ..y ~ ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)    Longitude     Latitude  \n#&gt;    -314.938       -2.138        2.853\n\n\n# 更换公式 再拟合\nlm_wflow %&gt;% update_formula(Sale_Price ~ Longitude) |&gt; fit(ames_train)\n#&gt; ══ Workflow [trained] ══════════════════════════════════════════════════════════\n#&gt; Preprocessor: Formula\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ────────────────────────────────────────────────────────────────\n#&gt; Sale_Price ~ Longitude\n#&gt; \n#&gt; ── Model ───────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ..y ~ ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)    Longitude  \n#&gt;    -191.309       -2.099\n\n\n\n# Variables:outcomes ~ predictors\nlm_wflow &lt;- \n  lm_wflow %&gt;% \n  remove_formula() %&gt;% \n  add_variables(outcomes  = Sale_Price, predictors = c(Longitude, Latitude))  # c(ends_with(\"tude\"))\n\nlm_wflow |&gt; fit(ames_train)\n#&gt; ══ Workflow [trained] ══════════════════════════════════════════════════════════\n#&gt; Preprocessor: Variables\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ────────────────────────────────────────────────────────────────\n#&gt; Outcomes: Sale_Price\n#&gt; Predictors: c(Longitude, Latitude)\n#&gt; \n#&gt; ── Model ───────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ..y ~ ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)    Longitude     Latitude  \n#&gt;    -314.938       -2.138        2.853\n\n\n# recepe\n\n\n\n1.4.2 预测\n\nCode\n# 回归 \"numeric\" , \"conf_int\",\"pred_int\",\"raw\".\n#  censored regression   \"time\"，\"hazard\",\"survival\"\n# 分类  \"class\", \"prob\",\n# \"quantile\"\n\n# When NULL, predict() will choose an appropriate value based on the model's mode.\npredict(lm_fit, ames_test)  # \"numeric\"\n#&gt; # A tibble: 588 × 1\n#&gt;    .pred\n#&gt;    &lt;dbl&gt;\n#&gt;  1  5.23\n#&gt;  2  5.29\n#&gt;  3  5.28\n#&gt;  4  5.27\n#&gt;  5  5.26\n#&gt;  6  5.24\n#&gt;  7  5.24\n#&gt;  8  5.24\n#&gt;  9  5.24\n#&gt; 10  5.30\n#&gt; # ℹ 578 more rows\n\n\n\n1.4.3 混合效应模型 multilevelmod\n\n\nCodelibrary(multilevelmod)\nmultilevel_spec &lt;- linear_reg() %&gt;% set_engine(\"lmer\")\nmultilevel_spec\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lmer\n\ndf &lt;-  read_delim(\"data/lme_anova.txt\",) |&gt; pivot_longer(cols = 3:7,names_to = \"time\",values_to = \"BP\") |&gt; \n    mutate_at(1:3,as.factor)\n#&gt; Rows: 15 Columns: 7\n#&gt; ── Column specification ────────────────────────────────────────────────────────\n#&gt; Delimiter: \"\\t\"\n#&gt; chr (1): induced_method\n#&gt; dbl (6): subject, t0, t1, t2, t3, t4\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\ndf\n#&gt; # A tibble: 75 × 4\n#&gt;    subject induced_method time     BP\n#&gt;    &lt;fct&gt;   &lt;fct&gt;          &lt;fct&gt; &lt;dbl&gt;\n#&gt;  1 1       A              t0      120\n#&gt;  2 1       A              t1      108\n#&gt;  3 1       A              t2      112\n#&gt;  4 1       A              t3      120\n#&gt;  5 1       A              t4      117\n#&gt;  6 2       A              t0      118\n#&gt;  7 2       A              t1      109\n#&gt;  8 2       A              t2      115\n#&gt;  9 2       A              t3      126\n#&gt; 10 2       A              t4      123\n#&gt; # ℹ 65 more rows\nmultilevel_workflow &lt;- \n  workflow() %&gt;% \n  add_variables(outcome = BP, predictors = c(induced_method,time,subject)) %&gt;% \n  add_model(multilevel_spec, \n            # This formula is given to the model\n            formula = BP ~ induced_method+time + ( 1| subject))\n\nmultilevel_workflow\n#&gt; ══ Workflow ════════════════════════════════════════════════════════════════════\n#&gt; Preprocessor: Variables\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ────────────────────────────────────────────────────────────────\n#&gt; Outcomes: BP\n#&gt; Predictors: c(induced_method, time, subject)\n#&gt; \n#&gt; ── Model ───────────────────────────────────────────────────────────────────────\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lmer\nmultilevel_workflow |&gt;  fit(data =df) \n#&gt; ══ Workflow [trained] ══════════════════════════════════════════════════════════\n#&gt; Preprocessor: Variables\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ────────────────────────────────────────────────────────────────\n#&gt; Outcomes: BP\n#&gt; Predictors: c(induced_method, time, subject)\n#&gt; \n#&gt; ── Model ───────────────────────────────────────────────────────────────────────\n#&gt; Linear mixed model fit by REML ['lmerMod']\n#&gt; Formula: BP ~ induced_method + time + (1 | subject)\n#&gt;    Data: data\n#&gt; REML criterion at convergence: 431.0591\n#&gt; Random effects:\n#&gt;  Groups   Name        Std.Dev.\n#&gt;  subject  (Intercept) 3.441   \n#&gt;  Residual             4.434   \n#&gt; Number of obs: 75, groups:  subject, 15\n#&gt; Fixed Effects:\n#&gt;     (Intercept)  induced_methodB  induced_methodC           timet1  \n#&gt;         118.360            4.800            8.520           -4.400  \n#&gt;          timet2           timet3           timet4  \n#&gt;          -4.467            9.400            6.067\n\n\n\n1.4.4 生存模型 censored\n\ntype = \"time\"  type = \"survival\"   type = \"linear_pred\"   type = \"quantile\"   type = \"hazard\"\n\nCodelibrary(censored)\n#&gt; Loading required package: survival\n\nparametric_spec &lt;- survival_reg()\n\nparametric_workflow &lt;- \n  workflow() %&gt;% \n  add_variables(outcome = c(fustat, futime), predictors = c(age, rx)) %&gt;% \n  add_model(parametric_spec, \n            formula = Surv(futime, fustat) ~ age + strata(rx))\n\nparametric_fit &lt;- fit(parametric_workflow, data = ovarian)\nparametric_fit\n#&gt; ══ Workflow [trained] ══════════════════════════════════════════════════════════\n#&gt; Preprocessor: Variables\n#&gt; Model: survival_reg()\n#&gt; \n#&gt; ── Preprocessor ────────────────────────────────────────────────────────────────\n#&gt; Outcomes: c(fustat, futime)\n#&gt; Predictors: c(age, rx)\n#&gt; \n#&gt; ── Model ───────────────────────────────────────────────────────────────────────\n#&gt; Call:\n#&gt; survival::survreg(formula = Surv(futime, fustat) ~ age + strata(rx), \n#&gt;     data = data, model = TRUE)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)         age \n#&gt;  12.8734120  -0.1033569 \n#&gt; \n#&gt; Scale:\n#&gt;      rx=1      rx=2 \n#&gt; 0.7695509 0.4703602 \n#&gt; \n#&gt; Loglik(model)= -89.4   Loglik(intercept only)= -97.1\n#&gt;  Chisq= 15.36 on 1 degrees of freedom, p= 8.88e-05 \n#&gt; n= 26\n\n\n\n1.4.5 工作流集 workflowsets\n\n\nCodelocation &lt;- list(\n  longitude = Sale_Price ~ Longitude,\n  latitude = Sale_Price ~ Latitude,\n  coords = Sale_Price ~ Longitude + Latitude,\n  neighborhood = Sale_Price ~ Neighborhood\n)\n\nlibrary(workflowsets)\nlocation_models &lt;- workflow_set(preproc = location, models = list(lm = lm_model))\nlocation_models\n#&gt; # A workflow set/tibble: 4 × 4\n#&gt;   wflow_id        info             option    result    \n#&gt;   &lt;chr&gt;           &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n#&gt; 1 longitude_lm    &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 2 latitude_lm     &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 3 coords_lm       &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n#&gt; 4 neighborhood_lm &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\nlocation_models$info[[1]]\n#&gt; # A tibble: 1 × 4\n#&gt;   workflow   preproc model      comment\n#&gt;   &lt;list&gt;     &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;  \n#&gt; 1 &lt;workflow&gt; formula linear_reg \"\"\nextract_workflow(location_models, id = \"coords_lm\")\n#&gt; ══ Workflow ════════════════════════════════════════════════════════════════════\n#&gt; Preprocessor: Formula\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ────────────────────────────────────────────────────────────────\n#&gt; Sale_Price ~ Longitude + Latitude\n#&gt; \n#&gt; ── Model ───────────────────────────────────────────────────────────────────────\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lm\n\n\nlocation_models &lt;-\n   location_models %&gt;%\n   mutate(fit = map(info, ~ fit(.x$workflow[[1]], ames_train)))\nlocation_models\n#&gt; # A workflow set/tibble: 4 × 5\n#&gt;   wflow_id        info             option    result     fit       \n#&gt;   &lt;chr&gt;           &lt;list&gt;           &lt;list&gt;    &lt;list&gt;     &lt;list&gt;    \n#&gt; 1 longitude_lm    &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; &lt;workflow&gt;\n#&gt; 2 latitude_lm     &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; &lt;workflow&gt;\n#&gt; 3 coords_lm       &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; &lt;workflow&gt;\n#&gt; 4 neighborhood_lm &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; &lt;workflow&gt;\n\nlocation_models$fit[[1]]\n#&gt; ══ Workflow [trained] ══════════════════════════════════════════════════════════\n#&gt; Preprocessor: Formula\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ────────────────────────────────────────────────────────────────\n#&gt; Sale_Price ~ Longitude\n#&gt; \n#&gt; ── Model ───────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ..y ~ ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)    Longitude  \n#&gt;    -191.309       -2.099\n\n\n\nCodefinal_lm_res &lt;- last_fit(lm_wflow, ames_split)\nfinal_lm_res\n#&gt; # Resampling results\n#&gt; # Manual resampling \n#&gt; # A tibble: 1 × 6\n#&gt;   splits             id               .metrics .notes   .predictions .workflow \n#&gt;   &lt;list&gt;             &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n#&gt; 1 &lt;split [2342/588]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\nextract_workflow(final_lm_res)\n#&gt; ══ Workflow [trained] ══════════════════════════════════════════════════════════\n#&gt; Preprocessor: Variables\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ────────────────────────────────────────────────────────────────\n#&gt; Outcomes: Sale_Price\n#&gt; Predictors: c(Longitude, Latitude)\n#&gt; \n#&gt; ── Model ───────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ..y ~ ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)    Longitude     Latitude  \n#&gt;    -314.938       -2.138        2.853\n\ncollect_metrics(final_lm_res)\n#&gt; # A tibble: 2 × 4\n#&gt;   .metric .estimator .estimate .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 rmse    standard       0.157 Preprocessor1_Model1\n#&gt; 2 rsq     standard       0.152 Preprocessor1_Model1\ncollect_predictions(final_lm_res) %&gt;% slice(1:5)\n#&gt; # A tibble: 5 × 5\n#&gt;   .pred id                .row Sale_Price .config             \n#&gt;   &lt;dbl&gt; &lt;chr&gt;            &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1  5.23 train/test split     1       5.33 Preprocessor1_Model1\n#&gt; 2  5.29 train/test split     6       5.29 Preprocessor1_Model1\n#&gt; 3  5.28 train/test split    13       5.26 Preprocessor1_Model1\n#&gt; 4  5.27 train/test split    14       5.23 Preprocessor1_Model1\n#&gt; 5  5.26 train/test split    16       5.73 Preprocessor1_Model1",
    "crumbs": [
      "预测性模型",
      "机器学习"
    ]
  },
  {
    "objectID": "machine_learning.html#特征工程-recipes",
    "href": "machine_learning.html#特征工程-recipes",
    "title": "",
    "section": "\n1.5 特征工程 recipes\n",
    "text": "1.5 特征工程 recipes\n\n\n1.5.1 虚拟变量\n\nCodesimple_ames &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,\n         data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_dummy(all_nominal_predictors())# 因子或字符变量，名义nominal\n# all_numeric_predictors()  all_numeric()  all_predictors()  all_outcomes()\n\n# 跨模型循环使用\nsimple_ames\n#&gt; \n#&gt; ── Recipe ──────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Inputs\n#&gt; Number of variables by role\n#&gt; outcome:   1\n#&gt; predictor: 4\n#&gt; \n#&gt; ── Operations\n#&gt; • Log transformation on: Gr_Liv_Area\n#&gt; • Dummy variables from: all_nominal_predictors()\n\n\n\nCodelm_wflow &lt;- \n  lm_wflow %&gt;% \n    #一次只能有一种预处理方法，需要在添加配方之前删除现有的预处理器\n  remove_variables() %&gt;% \n  add_recipe(simple_ames)\n\nlm_wflow\n#&gt; ══ Workflow ════════════════════════════════════════════════════════════════════\n#&gt; Preprocessor: Recipe\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ────────────────────────────────────────────────────────────────\n#&gt; 2 Recipe Steps\n#&gt; \n#&gt; • step_log()\n#&gt; • step_dummy()\n#&gt; \n#&gt; ── Model ───────────────────────────────────────────────────────────────────────\n#&gt; Linear Regression Model Specification (regression)\n#&gt; \n#&gt; Computational engine: lm\n\nlm_fit &lt;- fit(lm_wflow, ames_train)\npredict(lm_fit, ames_test)\n#&gt; Warning in predict.lm(object = object$fit, newdata = new_data, type =\n#&gt; \"response\", : prediction from rank-deficient fit; consider predict(.,\n#&gt; rankdeficient=\"NA\")\n#&gt; # A tibble: 588 × 1\n#&gt;    .pred\n#&gt;    &lt;dbl&gt;\n#&gt;  1  5.24\n#&gt;  2  5.27\n#&gt;  3  5.24\n#&gt;  4  5.20\n#&gt;  5  5.66\n#&gt;  6  5.12\n#&gt;  7  5.08\n#&gt;  8  5.22\n#&gt;  9  5.00\n#&gt; 10  5.37\n#&gt; # ℹ 578 more rows\n\n# 提取模型信息\nlm_fit %&gt;% \n  extract_recipe(estimated = TRUE)\n#&gt; \n#&gt; ── Recipe ──────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Inputs\n#&gt; Number of variables by role\n#&gt; outcome:   1\n#&gt; predictor: 4\n#&gt; \n#&gt; ── Training information\n#&gt; Training data contained 2342 data points and no incomplete rows.\n#&gt; \n#&gt; ── Operations\n#&gt; • Log transformation on: Gr_Liv_Area | Trained\n#&gt; • Dummy variables from: Neighborhood and Bldg_Type | Trained\n\nlm_fit %&gt;% \n  extract_fit_parsnip() %&gt;% \n  tidy()\n#&gt; # A tibble: 35 × 5\n#&gt;    term                            estimate std.error statistic   p.value\n#&gt;    &lt;chr&gt;                              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1 (Intercept)                     -1.10     0.234       -4.70  2.72e-  6\n#&gt;  2 Gr_Liv_Area                      0.646    0.0143      45.0   1.88e-318\n#&gt;  3 Year_Built                       0.00217  0.000118    18.5   3.21e- 71\n#&gt;  4 Neighborhood_College_Creek       0.00897  0.00841      1.07  2.87e-  1\n#&gt;  5 Neighborhood_Old_Town           -0.0178   0.00846     -2.11  3.51e-  2\n#&gt;  6 Neighborhood_Edwards            -0.0413   0.00769     -5.37  8.59e-  8\n#&gt;  7 Neighborhood_Somerset            0.0433   0.00992      4.37  1.30e-  5\n#&gt;  8 Neighborhood_Northridge_Heights  0.129    0.0102      12.7   6.96e- 36\n#&gt;  9 Neighborhood_Gilbert            -0.0437   0.00939     -4.65  3.43e-  6\n#&gt; 10 Neighborhood_Sawyer             -0.00785  0.00841     -0.933 3.51e-  1\n#&gt; # ℹ 25 more rows\n\n\n\nCode\nsimple_ames &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,\n         data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_other(Neighborhood, threshold = 0.01) %&gt;% \n  step_dummy(all_nominal_predictors())\n\n\n\nCodeggplot(ames_train, aes(x = Gr_Liv_Area, y = 10^Sale_Price)) + \n  geom_point(alpha = .2) + \n  facet_wrap(~ Bldg_Type) + \n  geom_smooth(method = lm, formula = y ~ x, se = FALSE, color = \"blue\") + \n  scale_x_log10() + \n  scale_y_log10() + \n  labs(x = \"Gross Living Area\", y = \"Sale Price (USD)\")\n\n\n\n\n\n\n\n\n1.5.2 交互项\nstep_interact(~ interaction terms) , +分隔不同交互效应\n\nCodesimple_ames &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,\n         data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_other(Neighborhood, threshold = 0.01) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  # Gr_Liv_Area is on the log scale from a previous step\n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") )\n\nsimple_ames \n#&gt; \n#&gt; ── Recipe ──────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Inputs\n#&gt; Number of variables by role\n#&gt; outcome:   1\n#&gt; predictor: 4\n#&gt; \n#&gt; ── Operations\n#&gt; • Log transformation on: Gr_Liv_Area\n#&gt; • Collapsing factor levels for: Neighborhood\n#&gt; • Dummy variables from: all_nominal_predictors()\n#&gt; • Interactions with: Gr_Liv_Area:starts_with(\"Bldg_Type_\")\n\n\n\n1.5.3 样条函数\n添加非线性特征\n\nCodelibrary(patchwork)\nlibrary(splines)\nplot_smoother &lt;- function(deg_free) {\n  ggplot(ames_train, aes(x = Latitude, y = 10^Sale_Price)) + \n    geom_point(alpha = .2) + \n    scale_y_log10() +\n    geom_smooth(\n      method = lm,\n      formula = y ~ ns(x, df = deg_free),# natural splines.\n      color = \"lightblue\",\n      se = FALSE\n    ) +\n    labs(title = paste(deg_free, \"Spline Terms\"),\n         y = \"Sale Price (USD)\")\n}\n\n( plot_smoother(1) + plot_smoother(5) ) / ( plot_smoother(20) + plot_smoother(100) )\n\n\n\n\n\n\n\n\nCoderecipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + Latitude,\n         data = ames_train) %&gt;%\n  step_ns(Latitude, deg_free = 20)\n#&gt; \n#&gt; ── Recipe ──────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Inputs\n#&gt; Number of variables by role\n#&gt; outcome:   1\n#&gt; predictor: 5\n#&gt; \n#&gt; ── Operations\n#&gt; • Natural splines on: Latitude\n\n\n\n1.5.4 特征提取\nPCA,\n\nCode  # Use a regular expression to capture house size predictors: \nrecipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + Latitude+Total_Bsmt_SF+First_Flr_SF+Gr_Liv_Area,\n         data = ames_train) %&gt;%\n    step_pca(matches(\"(SF$)|(Gr_Liv)\"))\n#&gt; \n#&gt; ── Recipe ──────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Inputs\n#&gt; Number of variables by role\n#&gt; outcome:   1\n#&gt; predictor: 7\n#&gt; \n#&gt; ── Operations\n#&gt; • PCA extraction with: matches(\"(SF$)|(Gr_Liv)\")\n\n\n\n1.5.5 行采样\nDownsampling，Upsampling，Hybrid\nstep_filter() step_sample() step_slice() step_arrange() skip TRUE\n\nCodelibrary(themis)\nstep_downsample(outcome_column_name)\n\n\n\n1.5.6 一般转换\nstep_mutate() 比，Bedroom_AbvGr / Full_Bath\n\n1.5.7 tidy()\n\n\nCodeames_rec &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_other(Neighborhood, threshold = 0.01) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %&gt;% \n  step_ns(Latitude, Longitude, deg_free = 20)\ntidy(ames_rec)\n#&gt; # A tibble: 5 × 6\n#&gt;   number operation type     trained skip  id            \n#&gt;    &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;    &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;         \n#&gt; 1      1 step      log      FALSE   FALSE log_qeD8W     \n#&gt; 2      2 step      other    FALSE   FALSE other_Hirvj   \n#&gt; 3      3 step      dummy    FALSE   FALSE dummy_HJDrx   \n#&gt; 4      4 step      interact FALSE   FALSE interact_B7kcu\n#&gt; 5      5 step      ns       FALSE   FALSE ns_yBiqt\n\n\n\nCodeames_rec &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_other(Neighborhood, threshold = 0.01, id = \"my_id\") %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %&gt;% \n  step_ns(Latitude, Longitude, deg_free = 20)\n\nlm_wflow &lt;- \n  workflow() %&gt;% \n  add_model(lm_model) %&gt;% \n  add_recipe(ames_rec)\n\nlm_fit &lt;- fit(lm_wflow, ames_train)\n\nestimated_recipe &lt;- \n  lm_fit %&gt;% \n  extract_recipe(estimated = TRUE)\n\ntidy(estimated_recipe, id = \"my_id\")\n#&gt; # A tibble: 21 × 3\n#&gt;    terms        retained           id   \n#&gt;    &lt;chr&gt;        &lt;chr&gt;              &lt;chr&gt;\n#&gt;  1 Neighborhood North_Ames         my_id\n#&gt;  2 Neighborhood College_Creek      my_id\n#&gt;  3 Neighborhood Old_Town           my_id\n#&gt;  4 Neighborhood Edwards            my_id\n#&gt;  5 Neighborhood Somerset           my_id\n#&gt;  6 Neighborhood Northridge_Heights my_id\n#&gt;  7 Neighborhood Gilbert            my_id\n#&gt;  8 Neighborhood Sawyer             my_id\n#&gt;  9 Neighborhood Northwest_Ames     my_id\n#&gt; 10 Neighborhood Sawyer_West        my_id\n#&gt; # ℹ 11 more rows\ntidy(estimated_recipe, number = 2)\n#&gt; # A tibble: 21 × 3\n#&gt;    terms        retained           id   \n#&gt;    &lt;chr&gt;        &lt;chr&gt;              &lt;chr&gt;\n#&gt;  1 Neighborhood North_Ames         my_id\n#&gt;  2 Neighborhood College_Creek      my_id\n#&gt;  3 Neighborhood Old_Town           my_id\n#&gt;  4 Neighborhood Edwards            my_id\n#&gt;  5 Neighborhood Somerset           my_id\n#&gt;  6 Neighborhood Northridge_Heights my_id\n#&gt;  7 Neighborhood Gilbert            my_id\n#&gt;  8 Neighborhood Sawyer             my_id\n#&gt;  9 Neighborhood Northwest_Ames     my_id\n#&gt; 10 Neighborhood Sawyer_West        my_id\n#&gt; # ℹ 11 more rows\n\n\n\n1.5.8 列角色\n\nCodeames_rec %&gt;% update_role(Year_Built, new_role = \"Street\")\n#&gt; \n#&gt; ── Recipe ──────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Inputs\n#&gt; Number of variables by role\n#&gt; outcome:   1\n#&gt; predictor: 5\n#&gt; Street:    1\n#&gt; \n#&gt; ── Operations\n#&gt; • Log transformation on: Gr_Liv_Area\n#&gt; • Collapsing factor levels for: Neighborhood\n#&gt; • Dummy variables from: all_nominal_predictors()\n#&gt; • Interactions with: Gr_Liv_Area:starts_with(\"Bldg_Type_\")\n#&gt; • Natural splines on: Latitude and Longitude\n\n\n\n1.5.9 自然语言处理\ntextrecipes",
    "crumbs": [
      "预测性模型",
      "机器学习"
    ]
  },
  {
    "objectID": "machine_learning.html#模型评估-yardstick",
    "href": "machine_learning.html#模型评估-yardstick",
    "title": "",
    "section": "\n1.6 模型评估 yardstick\n",
    "text": "1.6 模型评估 yardstick\n\n\n1.6.1 回归指标\n\nCodeames_test_res &lt;- predict(lm_fit, new_data = ames_test %&gt;% select(-Sale_Price))\names_test_res\n#&gt; # A tibble: 588 × 1\n#&gt;    .pred\n#&gt;    &lt;dbl&gt;\n#&gt;  1  5.24\n#&gt;  2  5.27\n#&gt;  3  5.24\n#&gt;  4  5.21\n#&gt;  5  5.65\n#&gt;  6  5.13\n#&gt;  7  5.04\n#&gt;  8  5.22\n#&gt;  9  5.00\n#&gt; 10  5.32\n#&gt; # ℹ 578 more rows\n\names_test_res &lt;- bind_cols(ames_test_res, ames_test %&gt;% select(Sale_Price))\names_test_res\n#&gt; # A tibble: 588 × 2\n#&gt;    .pred Sale_Price\n#&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n#&gt;  1  5.24       5.33\n#&gt;  2  5.27       5.29\n#&gt;  3  5.24       5.26\n#&gt;  4  5.21       5.23\n#&gt;  5  5.65       5.73\n#&gt;  6  5.13       5.17\n#&gt;  7  5.04       5.06\n#&gt;  8  5.22       5.26\n#&gt;  9  5.00       5.02\n#&gt; 10  5.32       5.24\n#&gt; # ℹ 578 more rows\n\n\n\nCodeggplot(ames_test_res, aes(x = Sale_Price, y = .pred)) + \n  # Create a diagonal line:\n  geom_abline(lty = 2) + \n  geom_point(alpha = 0.5) + \n  labs(y = \"Predicted Sale Price (log10)\", x = \"Sale Price (log10)\") +\n  # Scale and size the x- and y-axis uniformly:\n  coord_obs_pred()\n\n\n\n\n\n\n\n\n1.6.1.1 均方根误差RMSE\n\nCodermse(ames_test_res, truth = Sale_Price, estimate = .pred)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 rmse    standard      0.0772\n\n\n\n1.6.1.2 决定系数R2，平均绝对误差MAE\n\nCodeames_metrics &lt;- metric_set(rmse, rsq, mae)\names_metrics(ames_test_res, truth = Sale_Price, estimate = .pred)\n#&gt; # A tibble: 3 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 rmse    standard      0.0772\n#&gt; 2 rsq     standard      0.795 \n#&gt; 3 mae     standard      0.0550\n\n\n\n1.6.2 二分类指标\n\nCodedata(two_class_example)\ntibble(two_class_example)\n#&gt; # A tibble: 500 × 4\n#&gt;    truth   Class1   Class2 predicted\n#&gt;    &lt;fct&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;    \n#&gt;  1 Class2 0.00359 0.996    Class2   \n#&gt;  2 Class1 0.679   0.321    Class1   \n#&gt;  3 Class2 0.111   0.889    Class2   \n#&gt;  4 Class1 0.735   0.265    Class1   \n#&gt;  5 Class2 0.0162  0.984    Class2   \n#&gt;  6 Class1 0.999   0.000725 Class1   \n#&gt;  7 Class1 0.999   0.000799 Class1   \n#&gt;  8 Class1 0.812   0.188    Class1   \n#&gt;  9 Class2 0.457   0.543    Class2   \n#&gt; 10 Class2 0.0976  0.902    Class2   \n#&gt; # ℹ 490 more rows\n\n\n\nCode# 混淆矩阵\n# A confusion matrix: \nconf_mat(two_class_example, truth = truth, estimate = predicted)\n#&gt;           Truth\n#&gt; Prediction Class1 Class2\n#&gt;     Class1    227     50\n#&gt;     Class2     31    192\n\n\n\nCode# Accuracy:\naccuracy(two_class_example, truth, predicted)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.838\n\n\n\nCode# Matthews correlation coefficient:\nmcc(two_class_example, truth, predicted)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 mcc     binary         0.677\n\n\n\nCode# F1 metric:\nf_meas(two_class_example, truth, predicted)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 f_meas  binary         0.849\n\n\n\nCode# Combining these three classification metrics together\nclassification_metrics &lt;- metric_set(accuracy, mcc, f_meas)\nclassification_metrics(two_class_example, truth = truth, estimate = predicted)\n#&gt; # A tibble: 3 × 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.838\n#&gt; 2 mcc      binary         0.677\n#&gt; 3 f_meas   binary         0.849\n\n\n感兴趣 事件水平\n第二级逻辑将结果编码为0/1（在这种情况下，第二个值是事件）\n\nCodef_meas(two_class_example, truth, predicted, event_level = \"second\")\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 f_meas  binary         0.826\n\n\n\n1.6.2.1 ROC，AUC\n不使用预测类列,对于两类问题，感兴趣事件的概率列将传递到函数中\n\nCodetwo_class_curve &lt;- roc_curve(two_class_example, truth, Class1)\ntwo_class_curve\n#&gt; # A tibble: 502 × 3\n#&gt;    .threshold specificity sensitivity\n#&gt;         &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n#&gt;  1 -Inf           0                 1\n#&gt;  2    1.79e-7     0                 1\n#&gt;  3    4.50e-6     0.00413           1\n#&gt;  4    5.81e-6     0.00826           1\n#&gt;  5    5.92e-6     0.0124            1\n#&gt;  6    1.22e-5     0.0165            1\n#&gt;  7    1.40e-5     0.0207            1\n#&gt;  8    1.43e-5     0.0248            1\n#&gt;  9    2.38e-5     0.0289            1\n#&gt; 10    3.30e-5     0.0331            1\n#&gt; # ℹ 492 more rows\nroc_auc(two_class_example, truth, Class1)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 roc_auc binary         0.939\n\n\n\nCodeautoplot(two_class_curve)+\n    annotate(\"text\",x=0.5,y=0.25,label=\"AUC=0.939\")\n\n\n\n\n\n\n\n\n1.6.3 多分类指标\n\nCodedata(hpc_cv)\ntibble(hpc_cv)\n#&gt; # A tibble: 3,467 × 7\n#&gt;    obs   pred     VF      F       M          L Resample\n#&gt;    &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;   \n#&gt;  1 VF    VF    0.914 0.0779 0.00848 0.0000199  Fold01  \n#&gt;  2 VF    VF    0.938 0.0571 0.00482 0.0000101  Fold01  \n#&gt;  3 VF    VF    0.947 0.0495 0.00316 0.00000500 Fold01  \n#&gt;  4 VF    VF    0.929 0.0653 0.00579 0.0000156  Fold01  \n#&gt;  5 VF    VF    0.942 0.0543 0.00381 0.00000729 Fold01  \n#&gt;  6 VF    VF    0.951 0.0462 0.00272 0.00000384 Fold01  \n#&gt;  7 VF    VF    0.914 0.0782 0.00767 0.0000354  Fold01  \n#&gt;  8 VF    VF    0.918 0.0744 0.00726 0.0000157  Fold01  \n#&gt;  9 VF    VF    0.843 0.128  0.0296  0.000192   Fold01  \n#&gt; 10 VF    VF    0.920 0.0728 0.00703 0.0000147  Fold01  \n#&gt; # ℹ 3,457 more rows\n\n\n\nCodeaccuracy(hpc_cv, obs, pred)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy multiclass     0.709\nmcc(hpc_cv, obs, pred)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 mcc     multiclass     0.515\n\n\n二分类可拓展到多分类\n\nCodesensitivity(hpc_cv, obs, pred, estimator = \"macro\")\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 sensitivity macro          0.560\nsensitivity(hpc_cv, obs, pred, estimator = \"macro_weighted\")\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric     .estimator     .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;              &lt;dbl&gt;\n#&gt; 1 sensitivity macro_weighted     0.709\nsensitivity(hpc_cv, obs, pred, estimator = \"micro\")\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 sensitivity micro          0.709\n\n\n多分类\n\nCoderoc_auc(hpc_cv, obs, VF, F, M, L)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 roc_auc hand_till      0.829\nroc_auc(hpc_cv, obs, VF, F, M, L, estimator = \"macro_weighted\")\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator     .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;\n#&gt; 1 roc_auc macro_weighted     0.868\n\n\n\nCodehpc_cv %&gt;% \n  group_by(Resample) %&gt;% \n  accuracy(obs, pred)\n#&gt; # A tibble: 10 × 4\n#&gt;    Resample .metric  .estimator .estimate\n#&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt;  1 Fold01   accuracy multiclass     0.726\n#&gt;  2 Fold02   accuracy multiclass     0.712\n#&gt;  3 Fold03   accuracy multiclass     0.758\n#&gt;  4 Fold04   accuracy multiclass     0.712\n#&gt;  5 Fold05   accuracy multiclass     0.712\n#&gt;  6 Fold06   accuracy multiclass     0.697\n#&gt;  7 Fold07   accuracy multiclass     0.675\n#&gt;  8 Fold08   accuracy multiclass     0.721\n#&gt;  9 Fold09   accuracy multiclass     0.673\n#&gt; 10 Fold10   accuracy multiclass     0.699\n\n# Four 1-vs-all ROC curves for each fold\nhpc_cv %&gt;% \n  group_by(Resample) %&gt;% \n  roc_curve(obs, VF, F, M, L) %&gt;% \n  autoplot()",
    "crumbs": [
      "预测性模型",
      "机器学习"
    ]
  },
  {
    "objectID": "machine_learning.html#重采样-rsample",
    "href": "machine_learning.html#重采样-rsample",
    "title": "",
    "section": "\n1.7 重采样 rsample\n",
    "text": "1.7 重采样 rsample\n\n\nCodeames_rec\n#&gt; \n#&gt; ── Recipe ──────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Inputs\n#&gt; Number of variables by role\n#&gt; outcome:   1\n#&gt; predictor: 6\n#&gt; \n#&gt; ── Operations\n#&gt; • Log transformation on: Gr_Liv_Area\n#&gt; • Collapsing factor levels for: Neighborhood\n#&gt; • Dummy variables from: all_nominal_predictors()\n#&gt; • Interactions with: Gr_Liv_Area:starts_with(\"Bldg_Type_\")\n#&gt; • Natural splines on: Latitude and Longitude\n\n\n\nCoderf_model &lt;- \n  rand_forest(trees = 1000) %&gt;% \n  set_engine(\"ranger\") %&gt;% \n  set_mode(\"regression\")\n\nrf_wflow &lt;- \n  workflow() %&gt;% \n  add_formula(\n    Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n      Latitude + Longitude) %&gt;% \n  add_model(rf_model) \n\nrf_fit &lt;- rf_wflow %&gt;% fit(data = ames_train)\n\n\n\nCodeestimate_perf &lt;- function(model, dat) {\n  # Capture the names of the `model` and `dat` objects\n  cl &lt;- match.call()\n  obj_name &lt;- as.character(cl$model)\n  data_name &lt;- as.character(cl$dat)\n  data_name &lt;- gsub(\"ames_\", \"\", data_name)\n  \n  # Estimate these metrics:\n  reg_metrics &lt;- metric_set(rmse, rsq)\n  \n  model %&gt;%\n    predict(dat) %&gt;%\n    bind_cols(dat %&gt;% select(Sale_Price)) %&gt;%\n    reg_metrics(Sale_Price, .pred) %&gt;%\n    select(-.estimator) %&gt;%\n    mutate(object = obj_name, data = data_name)\n}\n\nestimate_perf(rf_fit, ames_train)\n#&gt; # A tibble: 2 × 4\n#&gt;   .metric .estimate object data \n#&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n#&gt; 1 rmse       0.0364 rf_fit train\n#&gt; 2 rsq        0.962  rf_fit train\nestimate_perf(lm_fit, ames_train)\n#&gt; # A tibble: 2 × 4\n#&gt;   .metric .estimate object data \n#&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n#&gt; 1 rmse       0.0749 lm_fit train\n#&gt; 2 rsq        0.824  lm_fit train\n\n\n\nCodeestimate_perf(rf_fit, ames_test)\n#&gt; # A tibble: 2 × 4\n#&gt;   .metric .estimate object data \n#&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n#&gt; 1 rmse       0.0677 rf_fit test \n#&gt; 2 rsq        0.843  rf_fit test\n\n\n\n\n\n\n\n1.7.1 交叉验证\nV-fold cross-validation，数据被随机划分为样本量大致相等的V组（称为折叠），例如10重交叉验证\n\nCodeset.seed(1001)\n# 10-fold cross-validation\names_folds &lt;- vfold_cv(ames_train, v = 10)\n\n# 分析集/评估集\names_folds\n#&gt; #  10-fold cross-validation \n#&gt; # A tibble: 10 × 2\n#&gt;    splits             id    \n#&gt;    &lt;list&gt;             &lt;chr&gt; \n#&gt;  1 &lt;split [2107/235]&gt; Fold01\n#&gt;  2 &lt;split [2107/235]&gt; Fold02\n#&gt;  3 &lt;split [2108/234]&gt; Fold03\n#&gt;  4 &lt;split [2108/234]&gt; Fold04\n#&gt;  5 &lt;split [2108/234]&gt; Fold05\n#&gt;  6 &lt;split [2108/234]&gt; Fold06\n#&gt;  7 &lt;split [2108/234]&gt; Fold07\n#&gt;  8 &lt;split [2108/234]&gt; Fold08\n#&gt;  9 &lt;split [2108/234]&gt; Fold09\n#&gt; 10 &lt;split [2108/234]&gt; Fold10\n\n\n# 检索分区数据\names_folds$splits[[1]] %&gt;% analysis() |&gt; dim()\n#&gt; [1] 2107   74\n\n\nmodel_spec %&gt;% fit_resamples(formula, resamples, ...)\nmodel_spec %&gt;% fit_resamples(recipe, resamples, ...)\nworkflow %&gt;% fit_resamples( resamples, ...)\n\nCodekeep_pred &lt;- control_resamples(save_pred = TRUE, save_workflow = TRUE)\n\nset.seed(1003)\nrf_res &lt;- \n  rf_wflow %&gt;% \n  fit_resamples(resamples = ames_folds, control = keep_pred)\nrf_res\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation \n#&gt; # A tibble: 10 × 5\n#&gt;    splits             id     .metrics         .notes           .predictions\n#&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n#&gt;  1 &lt;split [2107/235]&gt; Fold01 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n#&gt;  2 &lt;split [2107/235]&gt; Fold02 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n#&gt;  3 &lt;split [2108/234]&gt; Fold03 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n#&gt;  4 &lt;split [2108/234]&gt; Fold04 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n#&gt;  5 &lt;split [2108/234]&gt; Fold05 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n#&gt;  6 &lt;split [2108/234]&gt; Fold06 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n#&gt;  7 &lt;split [2108/234]&gt; Fold07 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n#&gt;  8 &lt;split [2108/234]&gt; Fold08 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n#&gt;  9 &lt;split [2108/234]&gt; Fold09 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n#&gt; 10 &lt;split [2108/234]&gt; Fold10 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;\ncollect_metrics(rf_res)\n#&gt; # A tibble: 2 × 6\n#&gt;   .metric .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 rmse    standard   0.0726    10 0.00258 Preprocessor1_Model1\n#&gt; 2 rsq     standard   0.834     10 0.0116  Preprocessor1_Model1\ncollect_metrics(rf_res,summarize = F)\n#&gt; # A tibble: 20 × 5\n#&gt;    id     .metric .estimator .estimate .config             \n#&gt;    &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n#&gt;  1 Fold01 rmse    standard      0.0647 Preprocessor1_Model1\n#&gt;  2 Fold01 rsq     standard      0.861  Preprocessor1_Model1\n#&gt;  3 Fold02 rmse    standard      0.0662 Preprocessor1_Model1\n#&gt;  4 Fold02 rsq     standard      0.860  Preprocessor1_Model1\n#&gt;  5 Fold03 rmse    standard      0.0793 Preprocessor1_Model1\n#&gt;  6 Fold03 rsq     standard      0.808  Preprocessor1_Model1\n#&gt;  7 Fold04 rmse    standard      0.0861 Preprocessor1_Model1\n#&gt;  8 Fold04 rsq     standard      0.761  Preprocessor1_Model1\n#&gt;  9 Fold05 rmse    standard      0.0738 Preprocessor1_Model1\n#&gt; 10 Fold05 rsq     standard      0.857  Preprocessor1_Model1\n#&gt; 11 Fold06 rmse    standard      0.0678 Preprocessor1_Model1\n#&gt; 12 Fold06 rsq     standard      0.860  Preprocessor1_Model1\n#&gt; 13 Fold07 rmse    standard      0.0644 Preprocessor1_Model1\n#&gt; 14 Fold07 rsq     standard      0.873  Preprocessor1_Model1\n#&gt; 15 Fold08 rmse    standard      0.0673 Preprocessor1_Model1\n#&gt; 16 Fold08 rsq     standard      0.829  Preprocessor1_Model1\n#&gt; 17 Fold09 rmse    standard      0.0719 Preprocessor1_Model1\n#&gt; 18 Fold09 rsq     standard      0.840  Preprocessor1_Model1\n#&gt; 19 Fold10 rmse    standard      0.0848 Preprocessor1_Model1\n#&gt; 20 Fold10 rsq     standard      0.790  Preprocessor1_Model1\nassess_res &lt;- collect_predictions(rf_res)\nassess_res\n#&gt; # A tibble: 2,342 × 5\n#&gt;    .pred id      .row Sale_Price .config             \n#&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;               \n#&gt;  1  5.17 Fold01    10       5.05 Preprocessor1_Model1\n#&gt;  2  5.04 Fold01    27       5.06 Preprocessor1_Model1\n#&gt;  3  5.11 Fold01    47       5.10 Preprocessor1_Model1\n#&gt;  4  5.14 Fold01    52       5.11 Preprocessor1_Model1\n#&gt;  5  5.09 Fold01    59       5.05 Preprocessor1_Model1\n#&gt;  6  4.88 Fold01    63       4.77 Preprocessor1_Model1\n#&gt;  7  5.01 Fold01    65       5.10 Preprocessor1_Model1\n#&gt;  8  5.16 Fold01    66       5    Preprocessor1_Model1\n#&gt;  9  4.96 Fold01    67       5.10 Preprocessor1_Model1\n#&gt; 10  4.84 Fold01    68       4.91 Preprocessor1_Model1\n#&gt; # ℹ 2,332 more rows\n\n\n\nCodeassess_res %&gt;% \n  ggplot(aes(x = Sale_Price, y = .pred)) + \n  geom_point(alpha = .15) +\n  geom_abline(color = \"red\") + \n  coord_obs_pred() + \n  ylab(\"Predicted\")\n\n\n\n\n\n\n\n\nCode# 找到残差最大的2个\nover_predicted &lt;- \n  assess_res %&gt;% \n  mutate(residual = Sale_Price - .pred) %&gt;% \n  arrange(desc(abs(residual))) %&gt;% \n  slice(1:2)\nover_predicted\n#&gt; # A tibble: 2 × 6\n#&gt;   .pred id      .row Sale_Price .config              residual\n#&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;                   &lt;dbl&gt;\n#&gt; 1  4.97 Fold10    33       4.11 Preprocessor1_Model1   -0.858\n#&gt; 2  4.93 Fold04   323       4.12 Preprocessor1_Model1   -0.814\n\names_train %&gt;% \n  slice(over_predicted$.row) %&gt;% \n  select(Gr_Liv_Area, Neighborhood, Year_Built, Bedroom_AbvGr, Full_Bath)\n#&gt; # A tibble: 2 × 5\n#&gt;   Gr_Liv_Area Neighborhood           Year_Built Bedroom_AbvGr Full_Bath\n#&gt;         &lt;int&gt; &lt;fct&gt;                       &lt;int&gt;         &lt;int&gt;     &lt;int&gt;\n#&gt; 1         832 Old_Town                     1923             2         1\n#&gt; 2         733 Iowa_DOT_and_Rail_Road       1952             2         1\n\n\n重复交叉验证\n\nCode# 10-fold cross-validation repeated 5 times \nvfold_cv(ames_train, v = 10, repeats = 5)\n#&gt; #  10-fold cross-validation repeated 5 times \n#&gt; # A tibble: 50 × 3\n#&gt;    splits             id      id2   \n#&gt;    &lt;list&gt;             &lt;chr&gt;   &lt;chr&gt; \n#&gt;  1 &lt;split [2107/235]&gt; Repeat1 Fold01\n#&gt;  2 &lt;split [2107/235]&gt; Repeat1 Fold02\n#&gt;  3 &lt;split [2108/234]&gt; Repeat1 Fold03\n#&gt;  4 &lt;split [2108/234]&gt; Repeat1 Fold04\n#&gt;  5 &lt;split [2108/234]&gt; Repeat1 Fold05\n#&gt;  6 &lt;split [2108/234]&gt; Repeat1 Fold06\n#&gt;  7 &lt;split [2108/234]&gt; Repeat1 Fold07\n#&gt;  8 &lt;split [2108/234]&gt; Repeat1 Fold08\n#&gt;  9 &lt;split [2108/234]&gt; Repeat1 Fold09\n#&gt; 10 &lt;split [2108/234]&gt; Repeat1 Fold10\n#&gt; # ℹ 40 more rows\n\n\n留一交叉验证\nleave-one-out (LOO) cross-validation\nloo_cv()\n蒙特卡罗交叉验证\nMonte Carlo cross-validation，MCCV，将固定比例的数据分配给分析集和评估集。该比例的数据每次都是随机选择的，导致评估集不相互排斥\n\nCodemc_cv(ames_train, prop = 9/10, times = 20)\n#&gt; # Monte Carlo cross-validation (0.9/0.1) with 20 resamples  \n#&gt; # A tibble: 20 × 2\n#&gt;    splits             id        \n#&gt;    &lt;list&gt;             &lt;chr&gt;     \n#&gt;  1 &lt;split [2107/235]&gt; Resample01\n#&gt;  2 &lt;split [2107/235]&gt; Resample02\n#&gt;  3 &lt;split [2107/235]&gt; Resample03\n#&gt;  4 &lt;split [2107/235]&gt; Resample04\n#&gt;  5 &lt;split [2107/235]&gt; Resample05\n#&gt;  6 &lt;split [2107/235]&gt; Resample06\n#&gt;  7 &lt;split [2107/235]&gt; Resample07\n#&gt;  8 &lt;split [2107/235]&gt; Resample08\n#&gt;  9 &lt;split [2107/235]&gt; Resample09\n#&gt; 10 &lt;split [2107/235]&gt; Resample10\n#&gt; 11 &lt;split [2107/235]&gt; Resample11\n#&gt; 12 &lt;split [2107/235]&gt; Resample12\n#&gt; 13 &lt;split [2107/235]&gt; Resample13\n#&gt; 14 &lt;split [2107/235]&gt; Resample14\n#&gt; 15 &lt;split [2107/235]&gt; Resample15\n#&gt; 16 &lt;split [2107/235]&gt; Resample16\n#&gt; 17 &lt;split [2107/235]&gt; Resample17\n#&gt; 18 &lt;split [2107/235]&gt; Resample18\n#&gt; 19 &lt;split [2107/235]&gt; Resample19\n#&gt; 20 &lt;split [2107/235]&gt; Resample20\n\n\n\n1.7.2 验证集\n\nCodeset.seed(101)\n\n# To put 60% into training, 20% in validation, and remaining 20% in testing:\names_validation_split &lt;- initial_validation_split(ames, prop = c(0.6, 0.2))\names_validation_split\n#&gt; &lt;Training/Validation/Testing/Total&gt;\n#&gt; &lt;1758/586/586/2930&gt;\n\n\n# Object used for resampling: \nval_set &lt;- validation_set(ames_validation_split)\nval_set\n#&gt; # A tibble: 1 × 2\n#&gt;   splits             id        \n#&gt;   &lt;list&gt;             &lt;chr&gt;     \n#&gt; 1 &lt;split [1758/586]&gt; validation\n\n\n\nCodeval_res &lt;- rf_wflow %&gt;% fit_resamples(resamples = val_set)\nval_res\n#&gt; # Resampling results\n#&gt; # Validation Set (0.75/0.25) \n#&gt; # A tibble: 1 × 4\n#&gt;   splits             id         .metrics         .notes          \n#&gt;   &lt;list&gt;             &lt;chr&gt;      &lt;list&gt;           &lt;list&gt;          \n#&gt; 1 &lt;split [1758/586]&gt; validation &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\ncollect_metrics(val_res)\n#&gt; # A tibble: 2 × 6\n#&gt;   .metric .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 rmse    standard   0.0809     1      NA Preprocessor1_Model1\n#&gt; 2 rsq     standard   0.818      1      NA Preprocessor1_Model1\n\n\n\n1.7.3 自助法\nBootstrap resampling\nreplacement\nout-of-bag sample\n\nCodebootstraps(ames_train, times = 5)\n#&gt; # Bootstrap sampling \n#&gt; # A tibble: 5 × 2\n#&gt;   splits             id        \n#&gt;   &lt;list&gt;             &lt;chr&gt;     \n#&gt; 1 &lt;split [2342/882]&gt; Bootstrap1\n#&gt; 2 &lt;split [2342/849]&gt; Bootstrap2\n#&gt; 3 &lt;split [2342/870]&gt; Bootstrap3\n#&gt; 4 &lt;split [2342/875]&gt; Bootstrap4\n#&gt; 5 &lt;split [2342/849]&gt; Bootstrap5\n\n\n\n1.7.4 Rolling forecast origin resampling\n滚动预测原点重采样\n时间序列数据\n\nCodetime_slices &lt;- \n  tibble(x = 1:365) %&gt;% \n  rolling_origin(initial = 6 * 30, assess = 30, skip = 29, cumulative = FALSE)\n\ndata_range &lt;- function(x) {\n  summarize(x, first = min(x), last = max(x))\n}\n\nmap_dfr(time_slices$splits, ~   analysis(.x) %&gt;% data_range())\n#&gt; # A tibble: 6 × 2\n#&gt;   first  last\n#&gt;   &lt;int&gt; &lt;int&gt;\n#&gt; 1     1   180\n#&gt; 2    31   210\n#&gt; 3    61   240\n#&gt; 4    91   270\n#&gt; 5   121   300\n#&gt; 6   151   330\n\nmap_dfr(time_slices$splits, ~ assessment(.x) %&gt;% data_range())\n#&gt; # A tibble: 6 × 2\n#&gt;   first  last\n#&gt;   &lt;int&gt; &lt;int&gt;\n#&gt; 1   181   210\n#&gt; 2   211   240\n#&gt; 3   241   270\n#&gt; 4   271   300\n#&gt; 5   301   330\n#&gt; 6   331   360\n\n\n\n1.7.5 并行计算\n\nCodeparallel::detectCores(logical = FALSE)\n#&gt; [1] 4\nparallel::detectCores(logical = TRUE)\n#&gt; [1] 8\n\n\n\n1.7.6 保存重采样对象\n\nCodeames_rec &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %&gt;%\n  step_other(Neighborhood, threshold = 0.01) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %&gt;% \n  step_ns(Latitude, Longitude, deg_free = 20)\n\nlm_wflow &lt;-  \n  workflow() %&gt;% \n  add_recipe(ames_rec) %&gt;% \n  add_model(linear_reg() %&gt;% set_engine(\"lm\")) \n\nlm_fit &lt;- lm_wflow %&gt;% fit(data = ames_train)\n\n# Select the recipe: \nextract_recipe(lm_fit, estimated = TRUE)\n#&gt; \n#&gt; ── Recipe ──────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Inputs\n#&gt; Number of variables by role\n#&gt; outcome:   1\n#&gt; predictor: 6\n#&gt; \n#&gt; ── Training information\n#&gt; Training data contained 2342 data points and no incomplete rows.\n#&gt; \n#&gt; ── Operations\n#&gt; • Collapsing factor levels for: Neighborhood | Trained\n#&gt; • Dummy variables from: Neighborhood and Bldg_Type | Trained\n#&gt; • Interactions with: Gr_Liv_Area:(Bldg_Type_TwoFmCon + Bldg_Type_Duplex +\n#&gt;   Bldg_Type_Twnhs + Bldg_Type_TwnhsE) | Trained\n#&gt; • Natural splines on: Latitude and Longitude | Trained\n\n\n\nCodeget_model &lt;- function(x) {\n  extract_fit_parsnip(x) %&gt;% tidy()\n}\n\nget_model(lm_fit)\n#&gt; # A tibble: 72 × 5\n#&gt;    term                             estimate  std.error statistic   p.value\n#&gt;    &lt;chr&gt;                               &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1 (Intercept)                      0.895    0.303          2.95  3.17e-  3\n#&gt;  2 Gr_Liv_Area                      0.000177 0.00000444    39.8   4.16e-263\n#&gt;  3 Year_Built                       0.00204  0.000139      14.7   1.51e- 46\n#&gt;  4 Neighborhood_College_Creek      -0.0283   0.0334        -0.849 3.96e-  1\n#&gt;  5 Neighborhood_Old_Town           -0.0489   0.0125        -3.91  9.48e-  5\n#&gt;  6 Neighborhood_Edwards            -0.0823   0.0273        -3.01  2.62e-  3\n#&gt;  7 Neighborhood_Somerset            0.0735   0.0192         3.83  1.33e-  4\n#&gt;  8 Neighborhood_Northridge_Heights  0.148    0.0278         5.33  1.07e-  7\n#&gt;  9 Neighborhood_Gilbert             0.0306   0.0218         1.41  1.60e-  1\n#&gt; 10 Neighborhood_Sawyer             -0.111    0.0257        -4.33  1.56e-  5\n#&gt; # ℹ 62 more rows\n\n\n\nCodectrl &lt;- control_resamples(extract = get_model)\n\nlm_res &lt;- lm_wflow %&gt;%  fit_resamples(resamples = ames_folds, control = ctrl)\nlm_res\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation \n#&gt; # A tibble: 10 × 5\n#&gt;    splits             id     .metrics         .notes           .extracts       \n#&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;          \n#&gt;  1 &lt;split [2107/235]&gt; Fold01 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n#&gt;  2 &lt;split [2107/235]&gt; Fold02 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n#&gt;  3 &lt;split [2108/234]&gt; Fold03 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n#&gt;  4 &lt;split [2108/234]&gt; Fold04 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n#&gt;  5 &lt;split [2108/234]&gt; Fold05 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n#&gt;  6 &lt;split [2108/234]&gt; Fold06 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n#&gt;  7 &lt;split [2108/234]&gt; Fold07 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n#&gt;  8 &lt;split [2108/234]&gt; Fold08 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n#&gt;  9 &lt;split [2108/234]&gt; Fold09 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n#&gt; 10 &lt;split [2108/234]&gt; Fold10 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n\n\n\nCodelm_res$.extracts[[1]]\n#&gt; # A tibble: 1 × 2\n#&gt;   .extracts         .config             \n#&gt;   &lt;list&gt;            &lt;chr&gt;               \n#&gt; 1 &lt;tibble [72 × 5]&gt; Preprocessor1_Model1\n\nlm_res$.extracts[[1]][[1]]\n#&gt; [[1]]\n#&gt; # A tibble: 72 × 5\n#&gt;    term                             estimate  std.error statistic   p.value\n#&gt;    &lt;chr&gt;                               &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1 (Intercept)                      0.950    0.324          2.93  3.42e-  3\n#&gt;  2 Gr_Liv_Area                      0.000175 0.00000481    36.3   4.09e-223\n#&gt;  3 Year_Built                       0.00201  0.000149      13.5   1.08e- 39\n#&gt;  4 Neighborhood_College_Creek      -0.0273   0.0359        -0.762 4.46e-  1\n#&gt;  5 Neighborhood_Old_Town           -0.0543   0.0134        -4.04  5.51e-  5\n#&gt;  6 Neighborhood_Edwards            -0.0825   0.0293        -2.81  4.95e-  3\n#&gt;  7 Neighborhood_Somerset            0.0706   0.0207         3.40  6.76e-  4\n#&gt;  8 Neighborhood_Northridge_Heights  0.139    0.0298         4.65  3.55e-  6\n#&gt;  9 Neighborhood_Gilbert             0.0197   0.0235         0.835 4.04e-  1\n#&gt; 10 Neighborhood_Sawyer             -0.117    0.0273        -4.30  1.77e-  5\n#&gt; # ℹ 62 more rows\n\n\n\nCodeall_coef &lt;- map_dfr(lm_res$.extracts, ~ .x[[1]][[1]])\n# Show the replicates for a single predictor:\nfilter(all_coef, term == \"Year_Built\")\n#&gt; # A tibble: 10 × 5\n#&gt;    term       estimate std.error statistic  p.value\n#&gt;    &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1 Year_Built  0.00201  0.000149      13.5 1.08e-39\n#&gt;  2 Year_Built  0.00203  0.000152      13.3 6.61e-39\n#&gt;  3 Year_Built  0.00189  0.000147      12.9 1.85e-36\n#&gt;  4 Year_Built  0.00210  0.000145      14.5 1.98e-45\n#&gt;  5 Year_Built  0.00211  0.000148      14.3 2.19e-44\n#&gt;  6 Year_Built  0.00204  0.000147      13.9 4.52e-42\n#&gt;  7 Year_Built  0.00215  0.000150      14.3 1.97e-44\n#&gt;  8 Year_Built  0.00213  0.000144      14.7 9.43e-47\n#&gt;  9 Year_Built  0.00208  0.000151      13.8 1.77e-41\n#&gt; 10 Year_Built  0.00212  0.000148      14.3 4.48e-44",
    "crumbs": [
      "预测性模型",
      "机器学习"
    ]
  },
  {
    "objectID": "predictive_models.html",
    "href": "predictive_models.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\n预测性模型\n预测性模型（Predictive Models）是指利用统计学、数学、计算机科学等方法，对已知数据进行分析和建模，从而对未来或未知数据进行预测的模型。\n预测性模型可以根据其构建方法和应用领域进行分类：\n\n基于统计学的方法：\n\n回归分析：如线性回归、非线性回归、逻辑回归等，用于建模和预测变量之间的关系。\n时间序列分析：如ARIMA模型、指数平滑法，用于分析和预测时间序列数据。\n\n基于经验驱动的模型（Empirically Driven Models），如机器学习\n\n监督学习：如支持向量机、神经网络、K-最近邻、决策树、随机森林等，通过训练数据建立预测模型。\n无监督学习：如聚类分析，用于发现数据中的潜在模式和结构。\n深度学习：如卷积神经网络（CNN）、循环神经网络（RNN）、生成对抗网络（GAN）等，用于处理复杂的高维数据和非线性关系。\n\n基于第一性原理（First Principles）的机理模型（Mechanistic Models）：\n\n量子力学计算：如密度泛函理论（DFT）、分子动力学（MD）等，从基本物理定律出发，预测分子和材料的性质和行为。\n多尺度建模：结合不同尺度的模型（如原子尺度、分子尺度、宏观尺度），实现从微观到宏观的综合预测。\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "预测性模型",
      "**预测性模型**"
    ]
  },
  {
    "objectID": "survival.html",
    "href": "survival.html",
    "title": "",
    "section": "",
    "text": "监督学习（Supervised Learning）生存模型 CodeShow All CodeHide All CodeView Source\n0.1 生存模型\nhttps://censored.tidymodels.org/articles/examples.html\ntype = \"time\"  type = \"survival\"   type = \"linear_pred\"   type = \"quantile\"   type = \"hazard\"\n\nCodelibrary(tidymodels)\nlibrary(censored)\n\nparametric_spec &lt;- survival_reg()\n\nparametric_workflow &lt;- \n  workflow() %&gt;% \n  add_variables(outcome = c(fustat, futime), predictors = c(age, rx)) %&gt;% \n  add_model(parametric_spec, \n            formula = Surv(futime, fustat) ~ age + strata(rx))\n\nparametric_fit &lt;- fit(parametric_workflow, data = ovarian)\nparametric_fit\n#&gt; ══ Workflow [trained] ══════════════════════════════════════════════════════════\n#&gt; Preprocessor: Variables\n#&gt; Model: survival_reg()\n#&gt; \n#&gt; ── Preprocessor ────────────────────────────────────────────────────────────────\n#&gt; Outcomes: c(fustat, futime)\n#&gt; Predictors: c(age, rx)\n#&gt; \n#&gt; ── Model ───────────────────────────────────────────────────────────────────────\n#&gt; Call:\n#&gt; survival::survreg(formula = Surv(futime, fustat) ~ age + strata(rx), \n#&gt;     data = data, model = TRUE)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)         age \n#&gt;  12.8734120  -0.1033569 \n#&gt; \n#&gt; Scale:\n#&gt;      rx=1      rx=2 \n#&gt; 0.7695509 0.4703602 \n#&gt; \n#&gt; Loglik(model)= -89.4   Loglik(intercept only)= -97.1\n#&gt;  Chisq= 15.36 on 1 degrees of freedom, p= 8.88e-05 \n#&gt; n= 26\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "预测性模型",
      "监督学习（Supervised Learning）",
      "生存模型"
    ]
  },
  {
    "objectID": "tree-based_models.html",
    "href": "tree-based_models.html",
    "title": "",
    "section": "",
    "text": "监督学习（Supervised Learning）基于树的模型 CodeShow All CodeHide All CodeView Source",
    "crumbs": [
      "预测性模型",
      "监督学习（Supervised Learning）",
      "基于树的模型"
    ]
  },
  {
    "objectID": "tree-based_models.html#决策树-分类",
    "href": "tree-based_models.html#决策树-分类",
    "title": "",
    "section": "\n1.1 决策树 （分类）",
    "text": "1.1 决策树 （分类）\n\nCodedf &lt;- read_csv(\"data/breast-cancer-wisconsin.data\",col_names = F,na = c(\"\",\"NA\",\"?\"))\n#&gt; Rows: 699 Columns: 11\n#&gt; ── Column specification ────────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; dbl (11): X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncolnames(df) &lt;- c(\"id\",\"肿块厚度\",\"细胞大小均匀性\",\"细胞形状均匀性\",\"边际附着力\",\"单个上皮细胞大小\",\n               \"裸核\",\"bland_chromatin\",\"正常核\",\"有丝分裂\",\"class\")\n\ndf &lt;- df |&gt;\n    select(-1) |&gt;\n    mutate(class = factor(class, levels = c(2, 4), labels = c(\"良性\", \"恶性\"))) |&gt;\n    drop_na()\nglimpse(df)\n#&gt; Rows: 683\n#&gt; Columns: 10\n#&gt; $ 肿块厚度         &lt;dbl&gt; 5, 5, 3, 6, 4, 8, 1, 2, 2, 4, 1, 2, 5, 1, 8, 7, 4, 4,…\n#&gt; $ 细胞大小均匀性   &lt;dbl&gt; 1, 4, 1, 8, 1, 10, 1, 1, 1, 2, 1, 1, 3, 1, 7, 4, 1, 1…\n#&gt; $ 细胞形状均匀性   &lt;dbl&gt; 1, 4, 1, 8, 1, 10, 1, 2, 1, 1, 1, 1, 3, 1, 5, 6, 1, 1…\n#&gt; $ 边际附着力       &lt;dbl&gt; 1, 5, 1, 1, 3, 8, 1, 1, 1, 1, 1, 1, 3, 1, 10, 4, 1, 1…\n#&gt; $ 单个上皮细胞大小 &lt;dbl&gt; 2, 7, 2, 3, 2, 7, 2, 2, 2, 2, 1, 2, 2, 2, 7, 6, 2, 2,…\n#&gt; $ 裸核             &lt;dbl&gt; 1, 10, 2, 4, 1, 10, 10, 1, 1, 1, 1, 1, 3, 3, 9, 1, 1,…\n#&gt; $ bland_chromatin  &lt;dbl&gt; 3, 3, 3, 3, 3, 9, 3, 3, 1, 2, 3, 2, 4, 3, 5, 4, 2, 3,…\n#&gt; $ 正常核           &lt;dbl&gt; 1, 2, 1, 7, 1, 7, 1, 1, 1, 1, 1, 1, 4, 1, 5, 3, 1, 1,…\n#&gt; $ 有丝分裂         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 4, 1, 1, 1,…\n#&gt; $ class            &lt;fct&gt; 良性, 良性, 良性, 良性, 良性, 恶性, 良性, 良性, 良性,…\n\n\n\nCode# 2=良性 4=恶性\ntable(df$class)\n#&gt; \n#&gt; 良性 恶性 \n#&gt;  444  239\n# 拆分训练集和测试集         ####\nset.seed(100)\nsplit &lt;- initial_split(df, prop = 0.70, strata = class)\n\nsplit\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;477/206/683&gt;\ntrain &lt;- training(split)\ntest  &lt;-  testing(split)\n\ntable(train$class)\n#&gt; \n#&gt; 良性 恶性 \n#&gt;  310  167\ntable(test$class)\n#&gt; \n#&gt; 良性 恶性 \n#&gt;  134   72\n\n\n\nCodeclass_tree_spec &lt;- decision_tree() %&gt;%\n    set_engine(\"rpart\") %&gt;%\n    set_mode(\"classification\") \n\ncdtree &lt;- class_tree_spec |&gt; fit(class ~ . ,data = train)\ncdtree\n#&gt; parsnip model object\n#&gt; \n#&gt; n= 477 \n#&gt; \n#&gt; node), split, n, loss, yval, (yprob)\n#&gt;       * denotes terminal node\n#&gt; \n#&gt;  1) root 477 167 良性 (0.64989518 0.35010482)  \n#&gt;    2) 细胞大小均匀性&lt; 2.5 293  11 良性 (0.96245734 0.03754266)  \n#&gt;      4) 裸核&lt; 5.5 285   4 良性 (0.98596491 0.01403509) *\n#&gt;      5) 裸核&gt;=5.5 8   1 恶性 (0.12500000 0.87500000) *\n#&gt;    3) 细胞大小均匀性&gt;=2.5 184  28 恶性 (0.15217391 0.84782609)  \n#&gt;      6) 细胞形状均匀性&lt; 2.5 16   3 良性 (0.81250000 0.18750000) *\n#&gt;      7) 细胞形状均匀性&gt;=2.5 168  15 恶性 (0.08928571 0.91071429)  \n#&gt;       14) 细胞大小均匀性&lt; 4.5 46  12 恶性 (0.26086957 0.73913043)  \n#&gt;         28) 裸核&lt; 2.5 10   3 良性 (0.70000000 0.30000000) *\n#&gt;         29) 裸核&gt;=2.5 36   5 恶性 (0.13888889 0.86111111) *\n#&gt;       15) 细胞大小均匀性&gt;=4.5 122   3 恶性 (0.02459016 0.97540984) *\n\n\n\n1.1.1 模型可视化\n\nCoderpart::plotcp(cdtree$fit)\n\n\n\n\n\n\nCodecdtree %&gt;%\n    extract_fit_engine() %&gt;%\n    rpart.plot::rpart.plot(roundint = F)\n\n\n\n\n\n\n\n\n1.1.2 模型性能评估\n\nCodeaugment(cdtree, new_data = test) %&gt;%\n    accuracy(truth = class, estimate = .pred_class)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.971\n\naugment(cdtree, new_data = test) %&gt;%\n    conf_mat(truth = class, estimate = .pred_class)\n#&gt;           Truth\n#&gt; Prediction 良性 恶性\n#&gt;       良性  132    4\n#&gt;       恶性    2   68",
    "crumbs": [
      "预测性模型",
      "监督学习（Supervised Learning）",
      "基于树的模型"
    ]
  },
  {
    "objectID": "tree-based_models.html#随机森林-分类",
    "href": "tree-based_models.html#随机森林-分类",
    "title": "",
    "section": "\n1.2 随机森林 （分类）",
    "text": "1.2 随机森林 （分类）\n随机森林是装袋法的一种扩展，它不仅对数据进行随机抽样，还对特征进行随机抽样，以此增加模型的多样性和泛化能力。随机森林由大量决策树组成，每棵树都是在一个随机抽取的样本和特征子集上训练的。\n\nCodecf_spec_class &lt;-\n    rand_forest(#mtry = .cols(), \n        trees = 500 ,min_n = 1) %&gt;%\n    set_engine('randomForest', importance = TRUE) %&gt;%\n    set_mode('classification')\n\nclass_rf_fit &lt;- cf_spec_class |&gt; \n    fit(class ~ . , data = train)\nclass_rf_fit\n#&gt; parsnip model object\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt;  randomForest(x = maybe_data_frame(x), y = y, ntree = ~500, nodesize = min_rows(~1,      x), importance = ~TRUE) \n#&gt;                Type of random forest: classification\n#&gt;                      Number of trees: 500\n#&gt; No. of variables tried at each split: 3\n#&gt; \n#&gt;         OOB estimate of  error rate: 3.77%\n#&gt; Confusion matrix:\n#&gt;      良性 恶性 class.error\n#&gt; 良性  300   10  0.03225806\n#&gt; 恶性    8  159  0.04790419\n\n\n\n1.2.1 特征重要性：基于Gini系数的减少\nOOB ，out of bag 袋外预测误差\n这种方法主要用于分类任务。\nMean Decrease in Gini (MDG) 这种方法通过衡量某个特征对分类纯度的贡献来计算其重要性。具体步骤如下：\n\n训练模型：使用所有特征训练随机森林模型。\n计算Gini系数：在决策树中，每次节点分裂都会计算Gini系数减少量。Gini系数用于衡量数据集的纯度，越低表示越纯。\n累加Gini减少量：在每棵树中，计算每个特征在分裂过程中带来的Gini减少量，并将这些减少量累加起来。\n计算平均值：对所有树的累加值取平均值，作为该特征的重要性得分。\n\n\nCodeclass_rf_fit %&gt;% vip::vi()\n#&gt; # A tibble: 9 × 2\n#&gt;   Variable         Importance\n#&gt;   &lt;chr&gt;                 &lt;dbl&gt;\n#&gt; 1 裸核                  25.6 \n#&gt; 2 肿块厚度              21.0 \n#&gt; 3 bland_chromatin       19.2 \n#&gt; 4 细胞大小均匀性        18.4 \n#&gt; 5 细胞形状均匀性        17.9 \n#&gt; 6 边际附着力            14.0 \n#&gt; 7 正常核                13.9 \n#&gt; 8 单个上皮细胞大小      10.6 \n#&gt; 9 有丝分裂               5.88\nclass_rf_fit %&gt;% vip::vip()\n\n\n\n\n\n\n\n\nCodeclass_rf_fit$fit$importance\n#&gt;                         良性        恶性 MeanDecreaseAccuracy MeanDecreaseGini\n#&gt; 肿块厚度         0.045985557 0.041538273          0.044359259        12.465397\n#&gt; 细胞大小均匀性   0.049172066 0.076704399          0.058509661        47.927558\n#&gt; 细胞形状均匀性   0.009876762 0.096802558          0.040209397        41.331213\n#&gt; 边际附着力       0.016551632 0.035342949          0.023040888         6.945234\n#&gt; 单个上皮细胞大小 0.012280597 0.010654858          0.011723405        16.171318\n#&gt; 裸核             0.061713383 0.066225145          0.063075655        40.588065\n#&gt; bland_chromatin  0.017570370 0.058096398          0.031839179        31.183405\n#&gt; 正常核           0.027576658 0.021034748          0.025191390        17.830550\n#&gt; 有丝分裂         0.003095108 0.001130822          0.002400936         1.946241\n\n\n\nCode\naugment(class_rf_fit, new_data = test) %&gt;%\n    accuracy(truth = class, estimate = .pred_class)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.990\n\naugment(class_rf_fit, new_data = test) %&gt;%\n    conf_mat(truth = class, estimate = .pred_class)\n#&gt;           Truth\n#&gt; Prediction 良性 恶性\n#&gt;       良性  132    0\n#&gt;       恶性    2   72\n\n\n\n1.2.2 基于表达数据的应用\n\nCodedf &lt;- dendextend::khan\n\ndf$train.classes\n#&gt;  [1] EWS    EWS    EWS    EWS    EWS    EWS    EWS    EWS    EWS    EWS   \n#&gt; [11] EWS    EWS    EWS    EWS    EWS    EWS    EWS    EWS    EWS    EWS   \n#&gt; [21] EWS    EWS    EWS    BL-NHL BL-NHL BL-NHL BL-NHL BL-NHL BL-NHL BL-NHL\n#&gt; [31] BL-NHL NB     NB     NB     NB     NB     NB     NB     NB     NB    \n#&gt; [41] NB     NB     NB     RMS    RMS    RMS    RMS    RMS    RMS    RMS   \n#&gt; [51] RMS    RMS    RMS    RMS    RMS    RMS    RMS    RMS    RMS    RMS   \n#&gt; [61] RMS    RMS    RMS    RMS   \n#&gt; Levels: EWS BL-NHL NB RMS\ntrain &lt;- t(df$train) |&gt; bind_cols(tibble(class=df$train.classes)) |&gt; \n    relocate(class, .before = 1) |&gt; \n    mutate(\n        class=factor(class,levels = c(\"EWS\", \"BL-NHL\", \"NB\",\"RMS\"))\n    )\nstr(train$class)\n#&gt;  Factor w/ 4 levels \"EWS\",\"BL-NHL\",..: 1 1 1 1 1 1 1 1 1 1 ...\ntable(train$class)\n#&gt; \n#&gt;    EWS BL-NHL     NB    RMS \n#&gt;     23      8     12     21\n\n\n\nCodedf$test.classes\n#&gt;  [1] Normal Normal Normal NB     RMS    Normal Normal NB     EWS    RMS   \n#&gt; [11] BL-NHL EWS    RMS    EWS    EWS    EWS    RMS    BL-NHL RMS    NB    \n#&gt; [21] NB     NB     NB     BL-NHL EWS   \n#&gt; Levels: EWS BL-NHL NB RMS Normal\n\ntest &lt;- t(df$test) |&gt; bind_cols(tibble(class=df$test.classes)) |&gt; \n    relocate(class, .before = 1) |&gt; \n    mutate(\n        class=factor(class,levels = c(\"EWS\", \"BL-NHL\", \"NB\",\"RMS\",\"Normal\"))\n    )\nstr(test$class)\n#&gt;  Factor w/ 5 levels \"EWS\",\"BL-NHL\",..: 5 5 5 3 4 5 5 3 1 4 ...\ntable(test$class)\n#&gt; \n#&gt;    EWS BL-NHL     NB    RMS Normal \n#&gt;      6      3      6      5      5\n\n\n\nCode#\ndt &lt;- class_tree_spec |&gt; fit(class ~ . ,data = train)\nrpart::plotcp(dt$fit)\n\n\n\n\n\n\nCode\ndt%&gt;%\n    extract_fit_engine() %&gt;%\n    rpart.plot::rpart.plot(roundint = F)\n\n\n\n\n\n\n\n\nCode#\nrf_fit_eg &lt;- cf_spec_class |&gt; \n    fit(class ~ . , data = train)\nrf_fit_eg\n#&gt; parsnip model object\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt;  randomForest(x = maybe_data_frame(x), y = y, ntree = ~500, nodesize = min_rows(~1,      x), importance = ~TRUE) \n#&gt;                Type of random forest: classification\n#&gt;                      Number of trees: 500\n#&gt; No. of variables tried at each split: 17\n#&gt; \n#&gt;         OOB estimate of  error rate: 0%\n#&gt; Confusion matrix:\n#&gt;        EWS BL-NHL NB RMS class.error\n#&gt; EWS     23      0  0   0           0\n#&gt; BL-NHL   0      8  0   0           0\n#&gt; NB       0      0 12   0           0\n#&gt; RMS      0      0  0  21           0",
    "crumbs": [
      "预测性模型",
      "监督学习（Supervised Learning）",
      "基于树的模型"
    ]
  },
  {
    "objectID": "tree-based_models.html#随机森林-回归",
    "href": "tree-based_models.html#随机森林-回归",
    "title": "",
    "section": "\n1.3 随机森林 （回归）",
    "text": "1.3 随机森林 （回归）\n\nCodelibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidymodels)\ndata &lt;- haven::read_sav(\"data/抑郁随机森林模型变量重要性.sav\") \n\n\n# glimpse(data)\n# \n# attributes(data$Gender)\n# attributes(data$ZD)\ndata &lt;- data %&gt;% \n    mutate(\n        Gender = factor(Gender),\n        XK = factor(XK),\n        DQ = factor(DQ),\n        SYDLX = factor(SYDLX),\n        \n    )\n\nset.seed(123)\nsplit &lt;- initial_split(data, prop = 0.7)\ntrain_data &lt;- training(split)\ntest_data &lt;- testing(split)\n\n\n\nCode# 构建随机森林模型\nrf_sepc_reg &lt;- rand_forest(mtry = 5, trees = 500) %&gt;%\n    set_engine(\"randomForest\" , importance = TRUE\n               ) %&gt;%\n    set_mode(\"regression\")\n\n# 创建配方\nrf_recipe &lt;- recipe(ZD ~ ., data = train_data) \n\n# 工作流程\nrf_workflow &lt;- workflow() %&gt;%\n    add_recipe(rf_recipe) %&gt;%\n    add_model(rf_sepc_reg)\n\n# 训练模型\nreg_rf_fit &lt;- rf_workflow %&gt;%\n    fit(data = train_data)\nreg_rf_fit %&gt;% extract_fit_parsnip()\n#&gt; parsnip model object\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt;  randomForest(x = maybe_data_frame(x), y = y, ntree = ~500, mtry = min_cols(~5,      x), importance = ~TRUE) \n#&gt;                Type of random forest: regression\n#&gt;                      Number of trees: 500\n#&gt; No. of variables tried at each split: 5\n#&gt; \n#&gt;           Mean of squared residuals: 48.45618\n#&gt;                     % Var explained: 87.48\n\n\n\n1.3.1 特征重要性：基于均方误差（MSE）的减少\n这种方法主要用于回归任务。\nMean Decrease in Accuracy (MDA) 这种方法通过衡量某个特征对整体模型预测准确性的贡献来计算其重要性。具体步骤如下：\n\n训练模型：使用所有特征训练随机森林模型。\n计算基线误差：使用训练好的模型在验证集上计算基线误差（例如，均方误差）。\n扰动特征值：对于每个特征，随机打乱验证集中该特征的值，从而破坏其与目标变量的关系。 重新计算误差：使用扰动后的数据再次计算模型误差。\n计算重要性：特征重要性得分等于扰动后的误差与基线误差之差。误差增加越多，说明该特征对模型预测的贡献越大。\n\n\nCode# 查看特征重要性\nimportance &lt;- reg_rf_fit %&gt;%\n    extract_fit_parsnip() %&gt;%\n    vip::vi()\nvip::vi(reg_rf_fit$fit$fit) \n#&gt; # A tibble: 26 × 2\n#&gt;    Variable Importance\n#&gt;    &lt;chr&gt;         &lt;dbl&gt;\n#&gt;  1 HL             34.9\n#&gt;  2 NE_A           29.4\n#&gt;  3 Sport          21.9\n#&gt;  4 SA             14.6\n#&gt;  5 I              14.1\n#&gt;  6 SMML           13.6\n#&gt;  7 FC             13.5\n#&gt;  8 BECKNC         13.2\n#&gt;  9 YS             12.8\n#&gt; 10 MVS            12.6\n#&gt; # ℹ 16 more rows\n\n\n# 可视化特征重要性\nggplot(importance, aes(x = reorder(Variable, Importance), y = Importance)) + \n    geom_col(fill = \"skyblue\") +\n    coord_flip() +\n    labs(title = \"特征重要性\", x = \"特征\", y = \"重要性得分\")",
    "crumbs": [
      "预测性模型",
      "监督学习（Supervised Learning）",
      "基于树的模型"
    ]
  },
  {
    "objectID": "tree-based_models.html#装袋法",
    "href": "tree-based_models.html#装袋法",
    "title": "",
    "section": "\n1.4 装袋法",
    "text": "1.4 装袋法\n装袋法（Bagging）或称自助聚合（Bootstrap Aggregation）是基于树的模型的一种集成学习技术。\n装袋法是一种集成学习技术，它通过构建多个模型（通常是决策树）并将其预测结果进行平均（对于回归任务）或投票（对于分类任务）来提高模型的准确性和稳健性。其主要步骤如下：\n\n数据抽样：从原始训练数据集中通过自助法（Bootstrap）随机有放回地抽取多个子集。每个子集的大小与原始数据集相同，但由于是有放回地抽样，因此每个子集中可能包含重复的样本。\n模型训练：对每个子集训练一个模型（通常是决策树模型）。由于每个子集的样本可能不同，训练得到的每个模型也可能不同。\n模型集成：在进行预测时，将所有模型的预测结果进行整合。对于分类任务，采用多数投票法，即选择出现次数最多的类别作为最终预测结果；对于回归任务，则采用平均法，即取各模型预测值的平均值作为最终预测结果。",
    "crumbs": [
      "预测性模型",
      "监督学习（Supervised Learning）",
      "基于树的模型"
    ]
  },
  {
    "objectID": "tree-based_models.html#梯度提升树",
    "href": "tree-based_models.html#梯度提升树",
    "title": "",
    "section": "\n1.5 梯度提升树",
    "text": "1.5 梯度提升树\n梯度提升树（Gradient Boosting Trees, GBT）是一种强大的集成学习方法，它通过逐步构建多个决策树，并将它们的预测结果进行加权组合来提高模型的预测性能。与装袋法（Bagging）不同，梯度提升树是一个迭代的过程，在每一步中都试图纠正前一步模型的错误。\n梯度提升树的基本思想是通过逐步构建一系列的弱学习器（通常是决策树）来逼近目标函数。每一个新的树都在先前树的基础上进行改进，使整体模型的预测误差逐步减小。其主要步骤如下：\n\n初始化模型：首先用一个简单的模型（如常数值模型）初始化预测值。\n计算残差：计算初始模型的预测值与实际值之间的差值（残差），这些残差代表了当前模型的误差。\n训练新树：基于残差训练一个新的决策树，目的是学习如何纠正当前模型的误差。\n更新模型：将新树的预测结果加权加入到当前模型中，从而更新整体模型。更新公式通常为：\n\n\\[\nF_m(x)=F_{m-1}(x)+ηh_m(x)F_{m}(x)\n\\] 其中，\\(F_{m}(x)\\) 是第 m 次迭代的模型，\\(\\eta\\) 是学习率（通常在0和1之间），\\(h_m(x)\\) 是第 m 棵树的预测值。\n\n\n重复迭代：重复步骤2-4，直到达到预定的树的数量或误差收敛。",
    "crumbs": [
      "预测性模型",
      "监督学习（Supervised Learning）",
      "基于树的模型"
    ]
  },
  {
    "objectID": "tree-based_models.html#贝叶斯相加回归树",
    "href": "tree-based_models.html#贝叶斯相加回归树",
    "title": "",
    "section": "\n1.6 贝叶斯相加回归树",
    "text": "1.6 贝叶斯相加回归树",
    "crumbs": [
      "预测性模型",
      "监督学习（Supervised Learning）",
      "基于树的模型"
    ]
  }
]