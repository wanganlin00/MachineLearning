[
  {
    "objectID": "resample.html",
    "href": "resample.html",
    "title": "",
    "section": "",
    "text": "机器学习重采样rsample CodeShow All CodeHide All CodeView Source",
    "crumbs": [
      "预测性模型",
      "机器学习",
      "重采样`rsample`"
    ]
  },
  {
    "objectID": "resample.html#交叉验证",
    "href": "resample.html#交叉验证",
    "title": "",
    "section": "\n1.1 交叉验证",
    "text": "1.1 交叉验证\n\n1.1.1 V 重交叉验证\nV-fold cross-validation，数据被随机划分为样本量大致相等的V组（称为折叠），例如10重交叉验证\n\nCodeset.seed(1001)\n# 10-fold cross-validation\names_folds &lt;- vfold_cv(ames_train, v = 10)\n\n# 分析集/评估集\names_folds\n#&gt; #  10-fold cross-validation \n#&gt; # A tibble: 10 × 2\n#&gt;    splits             id    \n#&gt;    &lt;list&gt;             &lt;chr&gt; \n#&gt;  1 &lt;split [2107/235]&gt; Fold01\n#&gt;  2 &lt;split [2107/235]&gt; Fold02\n#&gt;  3 &lt;split [2108/234]&gt; Fold03\n#&gt;  4 &lt;split [2108/234]&gt; Fold04\n#&gt;  5 &lt;split [2108/234]&gt; Fold05\n#&gt;  6 &lt;split [2108/234]&gt; Fold06\n#&gt;  7 &lt;split [2108/234]&gt; Fold07\n#&gt;  8 &lt;split [2108/234]&gt; Fold08\n#&gt;  9 &lt;split [2108/234]&gt; Fold09\n#&gt; 10 &lt;split [2108/234]&gt; Fold10\n\n\n# 检索分区数据\names_folds$splits[[1]] %&gt;% analysis() |&gt; dim()\n#&gt; [1] 2107   74\n\n\nmodel_spec %&gt;% fit_resamples(formula, resamples, ...)\nmodel_spec %&gt;% fit_resamples(recipe, resamples, ...)\nworkflow %&gt;% fit_resamples( resamples, ...)\n\nCodekeep_pred &lt;- control_resamples(save_pred = TRUE, save_workflow = TRUE)\n\nset.seed(1003)\nrf_res &lt;- \n  rf_wflow %&gt;% \n  fit_resamples(resamples = ames_folds, control = keep_pred)\nrf_res\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation \n#&gt; # A tibble: 10 × 5\n#&gt;    splits             id     .metrics         .notes           .predictions\n#&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n#&gt;  1 &lt;split [2107/235]&gt; Fold01 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n#&gt;  2 &lt;split [2107/235]&gt; Fold02 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n#&gt;  3 &lt;split [2108/234]&gt; Fold03 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n#&gt;  4 &lt;split [2108/234]&gt; Fold04 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n#&gt;  5 &lt;split [2108/234]&gt; Fold05 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n#&gt;  6 &lt;split [2108/234]&gt; Fold06 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n#&gt;  7 &lt;split [2108/234]&gt; Fold07 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n#&gt;  8 &lt;split [2108/234]&gt; Fold08 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n#&gt;  9 &lt;split [2108/234]&gt; Fold09 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n#&gt; 10 &lt;split [2108/234]&gt; Fold10 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;\ncollect_metrics(rf_res)\n#&gt; # A tibble: 2 × 6\n#&gt;   .metric .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 rmse    standard   0.0726    10 0.00258 Preprocessor1_Model1\n#&gt; 2 rsq     standard   0.834     10 0.0116  Preprocessor1_Model1\ncollect_metrics(rf_res,summarize = F)\n#&gt; # A tibble: 20 × 5\n#&gt;    id     .metric .estimator .estimate .config             \n#&gt;    &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n#&gt;  1 Fold01 rmse    standard      0.0647 Preprocessor1_Model1\n#&gt;  2 Fold01 rsq     standard      0.861  Preprocessor1_Model1\n#&gt;  3 Fold02 rmse    standard      0.0662 Preprocessor1_Model1\n#&gt;  4 Fold02 rsq     standard      0.860  Preprocessor1_Model1\n#&gt;  5 Fold03 rmse    standard      0.0793 Preprocessor1_Model1\n#&gt;  6 Fold03 rsq     standard      0.808  Preprocessor1_Model1\n#&gt;  7 Fold04 rmse    standard      0.0861 Preprocessor1_Model1\n#&gt;  8 Fold04 rsq     standard      0.761  Preprocessor1_Model1\n#&gt;  9 Fold05 rmse    standard      0.0738 Preprocessor1_Model1\n#&gt; 10 Fold05 rsq     standard      0.857  Preprocessor1_Model1\n#&gt; 11 Fold06 rmse    standard      0.0678 Preprocessor1_Model1\n#&gt; 12 Fold06 rsq     standard      0.860  Preprocessor1_Model1\n#&gt; 13 Fold07 rmse    standard      0.0644 Preprocessor1_Model1\n#&gt; 14 Fold07 rsq     standard      0.873  Preprocessor1_Model1\n#&gt; 15 Fold08 rmse    standard      0.0673 Preprocessor1_Model1\n#&gt; 16 Fold08 rsq     standard      0.829  Preprocessor1_Model1\n#&gt; 17 Fold09 rmse    standard      0.0719 Preprocessor1_Model1\n#&gt; 18 Fold09 rsq     standard      0.840  Preprocessor1_Model1\n#&gt; 19 Fold10 rmse    standard      0.0848 Preprocessor1_Model1\n#&gt; 20 Fold10 rsq     standard      0.790  Preprocessor1_Model1\nassess_res &lt;- collect_predictions(rf_res)\nassess_res\n#&gt; # A tibble: 2,342 × 5\n#&gt;    .pred id      .row Sale_Price .config             \n#&gt;    &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;               \n#&gt;  1  5.17 Fold01    10       5.05 Preprocessor1_Model1\n#&gt;  2  5.04 Fold01    27       5.06 Preprocessor1_Model1\n#&gt;  3  5.11 Fold01    47       5.10 Preprocessor1_Model1\n#&gt;  4  5.14 Fold01    52       5.11 Preprocessor1_Model1\n#&gt;  5  5.09 Fold01    59       5.05 Preprocessor1_Model1\n#&gt;  6  4.88 Fold01    63       4.77 Preprocessor1_Model1\n#&gt;  7  5.01 Fold01    65       5.10 Preprocessor1_Model1\n#&gt;  8  5.16 Fold01    66       5    Preprocessor1_Model1\n#&gt;  9  4.96 Fold01    67       5.10 Preprocessor1_Model1\n#&gt; 10  4.84 Fold01    68       4.91 Preprocessor1_Model1\n#&gt; # ℹ 2,332 more rows\n\n\n\nCodeassess_res %&gt;% \n  ggplot(aes(x = Sale_Price, y = .pred)) + \n  geom_point(alpha = .15) +\n  geom_abline(color = \"red\") + \n  coord_obs_pred() + \n  ylab(\"Predicted\")\n\n\n\n\n\n\n\n\nCode# 找到残差最大的2个\nover_predicted &lt;- \n  assess_res %&gt;% \n  mutate(residual = Sale_Price - .pred) %&gt;% \n  arrange(desc(abs(residual))) %&gt;% \n  slice(1:2)\nover_predicted\n#&gt; # A tibble: 2 × 6\n#&gt;   .pred id      .row Sale_Price .config              residual\n#&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;                   &lt;dbl&gt;\n#&gt; 1  4.97 Fold10    33       4.11 Preprocessor1_Model1   -0.858\n#&gt; 2  4.93 Fold04   323       4.12 Preprocessor1_Model1   -0.814\n\names_train %&gt;% \n  slice(over_predicted$.row) %&gt;% \n  select(Gr_Liv_Area, Neighborhood, Year_Built, Bedroom_AbvGr, Full_Bath)\n#&gt; # A tibble: 2 × 5\n#&gt;   Gr_Liv_Area Neighborhood           Year_Built Bedroom_AbvGr Full_Bath\n#&gt;         &lt;int&gt; &lt;fct&gt;                       &lt;int&gt;         &lt;int&gt;     &lt;int&gt;\n#&gt; 1         832 Old_Town                     1923             2         1\n#&gt; 2         733 Iowa_DOT_and_Rail_Road       1952             2         1\n\n\n\n1.1.2 重复交叉验证\n\nCode# 10-fold cross-validation repeated 5 times \nvfold_cv(ames_train, v = 10, repeats = 5)\n#&gt; #  10-fold cross-validation repeated 5 times \n#&gt; # A tibble: 50 × 3\n#&gt;    splits             id      id2   \n#&gt;    &lt;list&gt;             &lt;chr&gt;   &lt;chr&gt; \n#&gt;  1 &lt;split [2107/235]&gt; Repeat1 Fold01\n#&gt;  2 &lt;split [2107/235]&gt; Repeat1 Fold02\n#&gt;  3 &lt;split [2108/234]&gt; Repeat1 Fold03\n#&gt;  4 &lt;split [2108/234]&gt; Repeat1 Fold04\n#&gt;  5 &lt;split [2108/234]&gt; Repeat1 Fold05\n#&gt;  6 &lt;split [2108/234]&gt; Repeat1 Fold06\n#&gt;  7 &lt;split [2108/234]&gt; Repeat1 Fold07\n#&gt;  8 &lt;split [2108/234]&gt; Repeat1 Fold08\n#&gt;  9 &lt;split [2108/234]&gt; Repeat1 Fold09\n#&gt; 10 &lt;split [2108/234]&gt; Repeat1 Fold10\n#&gt; # ℹ 40 more rows\n\n\n\n1.1.3 留一交叉验证\nleave-one-out (LOO) cross-validation\nloo_cv()\n\n1.1.4 蒙特卡罗交叉验证\nMonte Carlo cross-validation，MCCV，将固定比例的数据分配给分析集和评估集。该比例的数据每次都是随机选择的，导致评估集不相互排斥\n\nCodemc_cv(ames_train, prop = 9/10, times = 20)\n#&gt; # Monte Carlo cross-validation (0.9/0.1) with 20 resamples  \n#&gt; # A tibble: 20 × 2\n#&gt;    splits             id        \n#&gt;    &lt;list&gt;             &lt;chr&gt;     \n#&gt;  1 &lt;split [2107/235]&gt; Resample01\n#&gt;  2 &lt;split [2107/235]&gt; Resample02\n#&gt;  3 &lt;split [2107/235]&gt; Resample03\n#&gt;  4 &lt;split [2107/235]&gt; Resample04\n#&gt;  5 &lt;split [2107/235]&gt; Resample05\n#&gt;  6 &lt;split [2107/235]&gt; Resample06\n#&gt;  7 &lt;split [2107/235]&gt; Resample07\n#&gt;  8 &lt;split [2107/235]&gt; Resample08\n#&gt;  9 &lt;split [2107/235]&gt; Resample09\n#&gt; 10 &lt;split [2107/235]&gt; Resample10\n#&gt; 11 &lt;split [2107/235]&gt; Resample11\n#&gt; 12 &lt;split [2107/235]&gt; Resample12\n#&gt; 13 &lt;split [2107/235]&gt; Resample13\n#&gt; 14 &lt;split [2107/235]&gt; Resample14\n#&gt; 15 &lt;split [2107/235]&gt; Resample15\n#&gt; 16 &lt;split [2107/235]&gt; Resample16\n#&gt; 17 &lt;split [2107/235]&gt; Resample17\n#&gt; 18 &lt;split [2107/235]&gt; Resample18\n#&gt; 19 &lt;split [2107/235]&gt; Resample19\n#&gt; 20 &lt;split [2107/235]&gt; Resample20",
    "crumbs": [
      "预测性模型",
      "机器学习",
      "重采样`rsample`"
    ]
  },
  {
    "objectID": "resample.html#验证集",
    "href": "resample.html#验证集",
    "title": "",
    "section": "\n1.2 验证集",
    "text": "1.2 验证集\n\nCodeset.seed(101)\n\n# To put 60% into training, 20% in validation, and remaining 20% in testing:\names_validation_split &lt;- initial_validation_split(ames, prop = c(0.6, 0.2))\names_validation_split\n#&gt; &lt;Training/Validation/Testing/Total&gt;\n#&gt; &lt;1758/586/586/2930&gt;\n\n\n# Object used for resampling: \nval_set &lt;- validation_set(ames_validation_split)\nval_set\n#&gt; # A tibble: 1 × 2\n#&gt;   splits             id        \n#&gt;   &lt;list&gt;             &lt;chr&gt;     \n#&gt; 1 &lt;split [1758/586]&gt; validation\n\n\n\nCodeval_res &lt;- rf_wflow %&gt;% fit_resamples(resamples = val_set)\nval_res\n#&gt; # Resampling results\n#&gt; # Validation Set (0.75/0.25) \n#&gt; # A tibble: 1 × 4\n#&gt;   splits             id         .metrics         .notes          \n#&gt;   &lt;list&gt;             &lt;chr&gt;      &lt;list&gt;           &lt;list&gt;          \n#&gt; 1 &lt;split [1758/586]&gt; validation &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\ncollect_metrics(val_res)\n#&gt; # A tibble: 2 × 6\n#&gt;   .metric .estimator   mean     n std_err .config             \n#&gt;   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n#&gt; 1 rmse    standard   0.0809     1      NA Preprocessor1_Model1\n#&gt; 2 rsq     standard   0.818      1      NA Preprocessor1_Model1",
    "crumbs": [
      "预测性模型",
      "机器学习",
      "重采样`rsample`"
    ]
  },
  {
    "objectID": "resample.html#自助法",
    "href": "resample.html#自助法",
    "title": "",
    "section": "\n1.3 自助法",
    "text": "1.3 自助法\nBootstrap resampling\nreplacement\nout-of-bag sample\n\nCodebootstraps(ames_train, times = 5)\n#&gt; # Bootstrap sampling \n#&gt; # A tibble: 5 × 2\n#&gt;   splits             id        \n#&gt;   &lt;list&gt;             &lt;chr&gt;     \n#&gt; 1 &lt;split [2342/882]&gt; Bootstrap1\n#&gt; 2 &lt;split [2342/849]&gt; Bootstrap2\n#&gt; 3 &lt;split [2342/870]&gt; Bootstrap3\n#&gt; 4 &lt;split [2342/875]&gt; Bootstrap4\n#&gt; 5 &lt;split [2342/849]&gt; Bootstrap5",
    "crumbs": [
      "预测性模型",
      "机器学习",
      "重采样`rsample`"
    ]
  },
  {
    "objectID": "resample.html#rolling-forecast-origin-resampling",
    "href": "resample.html#rolling-forecast-origin-resampling",
    "title": "",
    "section": "\n1.4 Rolling forecast origin resampling",
    "text": "1.4 Rolling forecast origin resampling\n滚动预测原点重采样\n时间序列数据\n\nCodetime_slices &lt;- \n  tibble(x = 1:365) %&gt;% \n  rolling_origin(initial = 6 * 30, assess = 30, skip = 29, cumulative = FALSE)\n\ndata_range &lt;- function(x) {\n  summarize(x, first = min(x), last = max(x))\n}\n\nmap_dfr(time_slices$splits, ~   analysis(.x) %&gt;% data_range())\n#&gt; # A tibble: 6 × 2\n#&gt;   first  last\n#&gt;   &lt;int&gt; &lt;int&gt;\n#&gt; 1     1   180\n#&gt; 2    31   210\n#&gt; 3    61   240\n#&gt; 4    91   270\n#&gt; 5   121   300\n#&gt; 6   151   330\n\nmap_dfr(time_slices$splits, ~ assessment(.x) %&gt;% data_range())\n#&gt; # A tibble: 6 × 2\n#&gt;   first  last\n#&gt;   &lt;int&gt; &lt;int&gt;\n#&gt; 1   181   210\n#&gt; 2   211   240\n#&gt; 3   241   270\n#&gt; 4   271   300\n#&gt; 5   301   330\n#&gt; 6   331   360",
    "crumbs": [
      "预测性模型",
      "机器学习",
      "重采样`rsample`"
    ]
  },
  {
    "objectID": "resample.html#保存重采样对象",
    "href": "resample.html#保存重采样对象",
    "title": "",
    "section": "\n1.5 保存重采样对象",
    "text": "1.5 保存重采样对象\n\nCodeames_rec &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %&gt;%\n  step_other(Neighborhood, threshold = 0.01) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %&gt;% \n  step_ns(Latitude, Longitude, deg_free = 20)\n\nlm_wflow &lt;-  \n  workflow() %&gt;% \n  add_recipe(ames_rec) %&gt;% \n  add_model(linear_reg() %&gt;% set_engine(\"lm\")) \n\nlm_fit &lt;- lm_wflow %&gt;% fit(data = ames_train)\n\n# Select the recipe: \nextract_recipe(lm_fit, estimated = TRUE)\n#&gt; \n#&gt; ── Recipe ──────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; ── Inputs\n#&gt; Number of variables by role\n#&gt; outcome:   1\n#&gt; predictor: 6\n#&gt; \n#&gt; ── Training information\n#&gt; Training data contained 2342 data points and no incomplete rows.\n#&gt; \n#&gt; ── Operations\n#&gt; • Collapsing factor levels for: Neighborhood | Trained\n#&gt; • Dummy variables from: Neighborhood and Bldg_Type | Trained\n#&gt; • Interactions with: Gr_Liv_Area:(Bldg_Type_TwoFmCon + Bldg_Type_Duplex +\n#&gt;   Bldg_Type_Twnhs + Bldg_Type_TwnhsE) | Trained\n#&gt; • Natural splines on: Latitude and Longitude | Trained\n\n\n\nCodeget_model &lt;- function(x) {\n  extract_fit_parsnip(x) %&gt;% tidy()\n}\n\nget_model(lm_fit)\n#&gt; # A tibble: 72 × 5\n#&gt;    term                             estimate  std.error statistic   p.value\n#&gt;    &lt;chr&gt;                               &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1 (Intercept)                      0.895    0.303          2.95  3.17e-  3\n#&gt;  2 Gr_Liv_Area                      0.000177 0.00000444    39.8   4.16e-263\n#&gt;  3 Year_Built                       0.00204  0.000139      14.7   1.51e- 46\n#&gt;  4 Neighborhood_College_Creek      -0.0283   0.0334        -0.849 3.96e-  1\n#&gt;  5 Neighborhood_Old_Town           -0.0489   0.0125        -3.91  9.48e-  5\n#&gt;  6 Neighborhood_Edwards            -0.0823   0.0273        -3.01  2.62e-  3\n#&gt;  7 Neighborhood_Somerset            0.0735   0.0192         3.83  1.33e-  4\n#&gt;  8 Neighborhood_Northridge_Heights  0.148    0.0278         5.33  1.07e-  7\n#&gt;  9 Neighborhood_Gilbert             0.0306   0.0218         1.41  1.60e-  1\n#&gt; 10 Neighborhood_Sawyer             -0.111    0.0257        -4.33  1.56e-  5\n#&gt; # ℹ 62 more rows\n\n\n\nCodectrl &lt;- control_resamples(extract = get_model)\n\nlm_res &lt;- lm_wflow %&gt;%  \n    fit_resamples(resamples = ames_folds, control = ctrl)\n\nlm_res\n#&gt; # Resampling results\n#&gt; # 10-fold cross-validation \n#&gt; # A tibble: 10 × 5\n#&gt;    splits             id     .metrics         .notes           .extracts       \n#&gt;    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;          \n#&gt;  1 &lt;split [2107/235]&gt; Fold01 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n#&gt;  2 &lt;split [2107/235]&gt; Fold02 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n#&gt;  3 &lt;split [2108/234]&gt; Fold03 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n#&gt;  4 &lt;split [2108/234]&gt; Fold04 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n#&gt;  5 &lt;split [2108/234]&gt; Fold05 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n#&gt;  6 &lt;split [2108/234]&gt; Fold06 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n#&gt;  7 &lt;split [2108/234]&gt; Fold07 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n#&gt;  8 &lt;split [2108/234]&gt; Fold08 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n#&gt;  9 &lt;split [2108/234]&gt; Fold09 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n#&gt; 10 &lt;split [2108/234]&gt; Fold10 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n\n\n\nCodelm_res$.extracts[[1]]\n#&gt; # A tibble: 1 × 2\n#&gt;   .extracts         .config             \n#&gt;   &lt;list&gt;            &lt;chr&gt;               \n#&gt; 1 &lt;tibble [72 × 5]&gt; Preprocessor1_Model1\n\nlm_res$.extracts[[1]][[1]]\n#&gt; [[1]]\n#&gt; # A tibble: 72 × 5\n#&gt;    term                             estimate  std.error statistic   p.value\n#&gt;    &lt;chr&gt;                               &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1 (Intercept)                      0.950    0.324          2.93  3.42e-  3\n#&gt;  2 Gr_Liv_Area                      0.000175 0.00000481    36.3   4.09e-223\n#&gt;  3 Year_Built                       0.00201  0.000149      13.5   1.08e- 39\n#&gt;  4 Neighborhood_College_Creek      -0.0273   0.0359        -0.762 4.46e-  1\n#&gt;  5 Neighborhood_Old_Town           -0.0543   0.0134        -4.04  5.51e-  5\n#&gt;  6 Neighborhood_Edwards            -0.0825   0.0293        -2.81  4.95e-  3\n#&gt;  7 Neighborhood_Somerset            0.0706   0.0207         3.40  6.76e-  4\n#&gt;  8 Neighborhood_Northridge_Heights  0.139    0.0298         4.65  3.55e-  6\n#&gt;  9 Neighborhood_Gilbert             0.0197   0.0235         0.835 4.04e-  1\n#&gt; 10 Neighborhood_Sawyer             -0.117    0.0273        -4.30  1.77e-  5\n#&gt; # ℹ 62 more rows\n\n\n\nCodeall_coef &lt;- map_dfr(lm_res$.extracts, ~ .x[[1]][[1]])\n# Show the replicates for a single predictor:\nall_coef %&gt;% \n    dplyr::filter( term == \"Year_Built\")\n#&gt; # A tibble: 10 × 5\n#&gt;    term       estimate std.error statistic  p.value\n#&gt;    &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1 Year_Built  0.00201  0.000149      13.5 1.08e-39\n#&gt;  2 Year_Built  0.00203  0.000152      13.3 6.61e-39\n#&gt;  3 Year_Built  0.00189  0.000147      12.9 1.85e-36\n#&gt;  4 Year_Built  0.00210  0.000145      14.5 1.98e-45\n#&gt;  5 Year_Built  0.00211  0.000148      14.3 2.19e-44\n#&gt;  6 Year_Built  0.00204  0.000147      13.9 4.52e-42\n#&gt;  7 Year_Built  0.00215  0.000150      14.3 1.97e-44\n#&gt;  8 Year_Built  0.00213  0.000144      14.7 9.43e-47\n#&gt;  9 Year_Built  0.00208  0.000151      13.8 1.77e-41\n#&gt; 10 Year_Built  0.00212  0.000148      14.3 4.48e-44",
    "crumbs": [
      "预测性模型",
      "机器学习",
      "重采样`rsample`"
    ]
  },
  {
    "objectID": "UMAP.html",
    "href": "UMAP.html",
    "title": "",
    "section": "",
    "text": "无监督学习（Unsupervised Learning）UMAP CodeShow All CodeHide All CodeView Source\n1 UMAP\nUniform Manifold Approximation and Projection (UMAP)\nhttps://github.com/jlmelville/uwot\nhttps://github.com/tkonopka/umap\n2018年McInnes提出了算法，UMAP（Uniform Manifold Approximation and Projection for Dimension Reduction，一致的流形逼近和投影以进行降维）。 一致的流形近似和投影（UMAP）是一种降维技术，类似于t-SNE，可用于可视化，但也可用于一般的非线性降维。 该算法基于关于数据的三个假设：\n\n数据均匀分布在黎曼流形上（Riemannian manifold）；\n黎曼度量是局部恒定的（或可以这样近似）；\n流形是局部连接的。\n\nhttps://jlmelville.github.io/uwot/index.html\n\nCodelibrary(uwot)\nhead(iris)\n#&gt;   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#&gt; 1          5.1         3.5          1.4         0.2  setosa\n#&gt; 2          4.9         3.0          1.4         0.2  setosa\n#&gt; 3          4.7         3.2          1.3         0.2  setosa\n#&gt; 4          4.6         3.1          1.5         0.2  setosa\n#&gt; 5          5.0         3.6          1.4         0.2  setosa\n#&gt; 6          5.4         3.9          1.7         0.4  setosa\ncolors = rainbow(length(unique(iris$Species)))\nnames(colors) = unique(iris$Species)\n# umap2 is a version of the umap() function with better defaults\niris_umap2 &lt;- umap2(iris[1:4]) |&gt; as_tibble()\n#&gt; Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n#&gt; `.name_repair` is omitted as of tibble 2.0.0.\n#&gt; ℹ Using compatibility `.name_repair`.\n\nggplot(iris_umap2,aes(V1,V2))+\n    geom_text(aes(label=iris$Species),color=colors[iris$Species])\n\n\n\n\n\n\n\n\nCode# but you can still use the umap function (which most of the existing \n# documentation does)\niris_umap &lt;- umap(iris[1:4]) |&gt; as_tibble()\n\nggplot(iris_umap,aes(V1,V2))+\n    geom_text(aes(label=iris$Species),color=colors[iris$Species])\n\n\n\n\n\n\n\n\nCodelibrary(uwot)\n\nset.seed(42) # 为了结果可重复\nuwot_result &lt;- umap(iris[1:4])\n\n# 将结果转换为数据框\nuwot_df &lt;- as.data.frame(uwot_result)\ncolnames(uwot_df) &lt;- c(\"UMAP1\", \"UMAP2\")\nuwot_df$Species &lt;- iris$Species\n\n# 可视化\nggplot(uwot_df, aes(x = UMAP1, y = UMAP2, color = Species)) +\n  geom_point(size = 2) +\n  labs(title = \"UMAP of Iris Dataset (uwot)\",\n       x = \"UMAP Dimension 1\",\n       y = \"UMAP Dimension 2\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "预测性模型",
      "无监督学习（Unsupervised Learning）",
      "UMAP"
    ]
  },
  {
    "objectID": "t-SNE.html",
    "href": "t-SNE.html",
    "title": "",
    "section": "",
    "text": "无监督学习（Unsupervised Learning）t-SNE CodeShow All CodeHide All CodeView Source\n1 t-SNE\nt-Distributed Stochastic Neighbor Embedding (t-SNE)是一种降维技术，用于在二维或三维的低维空间中表示高维数据集，从而使其可视化。与其他降维算法(如PCA)相比，t-SNE创建了一个缩小的特征空间，相似的样本由附近的点建模，不相似的样本由高概率的远点建模。\nt-Distributed Stochastic Neighbor Embedding (t-SNE)\n\nCodelibrary(tsne)\n\nhead(iris)\n#&gt;   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#&gt; 1          5.1         3.5          1.4         0.2  setosa\n#&gt; 2          4.9         3.0          1.4         0.2  setosa\n#&gt; 3          4.7         3.2          1.3         0.2  setosa\n#&gt; 4          4.6         3.1          1.5         0.2  setosa\n#&gt; 5          5.0         3.6          1.4         0.2  setosa\n#&gt; 6          5.4         3.9          1.7         0.4  setosa\n\ncolors = rainbow(length(unique(iris$Species)))\nnames(colors) = unique(iris$Species)\necb = function(x, y) {\n    plot(x, t = 'n')\n    text(x, labels = iris$Species, col = colors[iris$Species])\n}\ntsne_iris = tsne(iris[,1:4], epoch_callback = ecb, perplexity=50)\n#&gt; sigma summary: Min. : 0.565012665854053 |1st Qu. : 0.681985646004023 |Median : 0.713004330336136 |Mean : 0.716213420895748 |3rd Qu. : 0.74581655363904 |Max. : 0.874979764925049 |\n#&gt; Epoch: Iteration #100 error is: 12.0521968962333\n#&gt; Epoch: Iteration #200 error is: 0.278293775495887\n\n\n\n\n\n\n#&gt; Epoch: Iteration #300 error is: 0.277972566238466\n\n\n\n\n\n\n#&gt; Epoch: Iteration #400 error is: 0.277972360425316\n\n\n\n\n\n\n#&gt; Epoch: Iteration #500 error is: 0.277972360400364\n\n\n\n\n\n\n#&gt; Epoch: Iteration #600 error is: 0.277972360400287\n\n\n\n\n\n\n#&gt; Epoch: Iteration #700 error is: 0.277972360400287\n\n\n\n\n\n\n#&gt; Epoch: Iteration #800 error is: 0.277972360400287\n\n\n\n\n\n\n#&gt; Epoch: Iteration #900 error is: 0.277972360400287\n\n\n\n\n\n\n#&gt; Epoch: Iteration #1000 error is: 0.277972360400287\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# compare to PCA\ndev.new()\npca_iris = princomp(iris[,1:4])$scores[,1:2]\nplot(pca_iris, t='n')\ntext(pca_iris, labels=iris$Species,col=colors[iris$Species])\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "预测性模型",
      "无监督学习（Unsupervised Learning）",
      "t-SNE"
    ]
  },
  {
    "objectID": "survival.html",
    "href": "survival.html",
    "title": "",
    "section": "",
    "text": "监督学习（Supervised Learning）生存模型 CodeShow All CodeHide All CodeView Source\n0.1 生存模型\nhttps://censored.tidymodels.org/articles/examples.html\ntype = \"time\"  type = \"survival\"   type = \"linear_pred\"   type = \"quantile\"   type = \"hazard\"\n\nCodelibrary(tidymodels)\nlibrary(censored)\n\nparametric_spec &lt;- survival_reg()\n\nparametric_workflow &lt;- \n  workflow() %&gt;% \n  add_variables(outcome = c(fustat, futime), predictors = c(age, rx)) %&gt;% \n  add_model(parametric_spec, \n            formula = Surv(futime, fustat) ~ age + strata(rx))\n\nparametric_fit &lt;- fit(parametric_workflow, data = ovarian)\nparametric_fit\n#&gt; ══ Workflow [trained] ══════════════════════════════════════════════════════════\n#&gt; Preprocessor: Variables\n#&gt; Model: survival_reg()\n#&gt; \n#&gt; ── Preprocessor ────────────────────────────────────────────────────────────────\n#&gt; Outcomes: c(fustat, futime)\n#&gt; Predictors: c(age, rx)\n#&gt; \n#&gt; ── Model ───────────────────────────────────────────────────────────────────────\n#&gt; Call:\n#&gt; survival::survreg(formula = Surv(futime, fustat) ~ age + strata(rx), \n#&gt;     data = data, model = TRUE)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)         age \n#&gt;  12.8734120  -0.1033569 \n#&gt; \n#&gt; Scale:\n#&gt;      rx=1      rx=2 \n#&gt; 0.7695509 0.4703602 \n#&gt; \n#&gt; Loglik(model)= -89.4   Loglik(intercept only)= -97.1\n#&gt;  Chisq= 15.36 on 1 degrees of freedom, p= 8.88e-05 \n#&gt; n= 26\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "预测性模型",
      "监督学习（Supervised Learning）",
      "生存模型"
    ]
  },
  {
    "objectID": "tree-based_models.html",
    "href": "tree-based_models.html",
    "title": "",
    "section": "",
    "text": "监督学习（Supervised Learning）基于树的模型 CodeShow All CodeHide All CodeView Source",
    "crumbs": [
      "预测性模型",
      "监督学习（Supervised Learning）",
      "基于树的模型"
    ]
  },
  {
    "objectID": "tree-based_models.html#决策树-分类",
    "href": "tree-based_models.html#决策树-分类",
    "title": "",
    "section": "\n1.1 决策树 （分类）",
    "text": "1.1 决策树 （分类）\n\nCodedf &lt;- read_csv(\n    \"data/breast-cancer-wisconsin.data\",\n    col_names = F,\n    na = c(\"\", \"NA\", \"?\")\n)\n#&gt; Rows: 699 Columns: 11\n#&gt; ── Column specification ────────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; dbl (11): X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncolnames(df) &lt;- c(\n    \"sample_id\",\n    \"肿块厚度\",\n    \"细胞大小一致性\",\n    \"细胞形态一致性\",\n    \"边际粘附力\",\n    \"单上皮细胞大小\",\n    \"裸核\",\n    \"染色质颜色\",\n    \"正常核仁\",\n    \"有丝分裂\",\n    \"class\"\n)# 2 for benign, 4 for malignant\n\n\ndf &lt;- df |&gt;\n    select(-1) |&gt;\n    mutate(class = factor(class, levels = c(2, 4), labels = c(\"良性\", \"恶性\"))) |&gt;\n    drop_na()\nglimpse(df)\n#&gt; Rows: 683\n#&gt; Columns: 10\n#&gt; $ 肿块厚度       &lt;dbl&gt; 5, 5, 3, 6, 4, 8, 1, 2, 2, 4, 1, 2, 5, 1, 8, 7, 4, 4, 1…\n#&gt; $ 细胞大小一致性 &lt;dbl&gt; 1, 4, 1, 8, 1, 10, 1, 1, 1, 2, 1, 1, 3, 1, 7, 4, 1, 1, …\n#&gt; $ 细胞形态一致性 &lt;dbl&gt; 1, 4, 1, 8, 1, 10, 1, 2, 1, 1, 1, 1, 3, 1, 5, 6, 1, 1, …\n#&gt; $ 边际粘附力     &lt;dbl&gt; 1, 5, 1, 1, 3, 8, 1, 1, 1, 1, 1, 1, 3, 1, 10, 4, 1, 1, …\n#&gt; $ 单上皮细胞大小 &lt;dbl&gt; 2, 7, 2, 3, 2, 7, 2, 2, 2, 2, 1, 2, 2, 2, 7, 6, 2, 2, 4…\n#&gt; $ 裸核           &lt;dbl&gt; 1, 10, 2, 4, 1, 10, 10, 1, 1, 1, 1, 1, 3, 3, 9, 1, 1, 1…\n#&gt; $ 染色质颜色     &lt;dbl&gt; 3, 3, 3, 3, 3, 9, 3, 3, 1, 2, 3, 2, 4, 3, 5, 4, 2, 3, 4…\n#&gt; $ 正常核仁       &lt;dbl&gt; 1, 2, 1, 7, 1, 7, 1, 1, 1, 1, 1, 1, 4, 1, 5, 3, 1, 1, 1…\n#&gt; $ 有丝分裂       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 4, 1, 1, 1, 2…\n#&gt; $ class          &lt;fct&gt; 良性, 良性, 良性, 良性, 良性, 恶性, 良性, 良性, 良性, …\n\n\n\nCode# 2=良性 4=恶性\ntable(df$class)\n#&gt; \n#&gt; 良性 恶性 \n#&gt;  444  239\n# 拆分训练集和测试集         ####\nset.seed(100)\nsplit &lt;- initial_split(df, prop = 0.70, strata = class)\n\nsplit\n#&gt; &lt;Training/Testing/Total&gt;\n#&gt; &lt;477/206/683&gt;\ntrain &lt;- training(split)\ntest  &lt;-  testing(split)\n\ntable(train$class)\n#&gt; \n#&gt; 良性 恶性 \n#&gt;  310  167\ntable(test$class)\n#&gt; \n#&gt; 良性 恶性 \n#&gt;  134   72\n\n\n\nCodeclass_tree_spec &lt;- decision_tree() %&gt;%\n    set_engine(\"rpart\") %&gt;%\n    set_mode(\"classification\") \n\ncdtree &lt;- class_tree_spec |&gt; fit(class ~ . ,data = train)\ncdtree\n#&gt; parsnip model object\n#&gt; \n#&gt; n= 477 \n#&gt; \n#&gt; node), split, n, loss, yval, (yprob)\n#&gt;       * denotes terminal node\n#&gt; \n#&gt;  1) root 477 167 良性 (0.64989518 0.35010482)  \n#&gt;    2) 细胞大小均匀性&lt; 2.5 293  11 良性 (0.96245734 0.03754266)  \n#&gt;      4) 裸核&lt; 5.5 285   4 良性 (0.98596491 0.01403509) *\n#&gt;      5) 裸核&gt;=5.5 8   1 恶性 (0.12500000 0.87500000) *\n#&gt;    3) 细胞大小均匀性&gt;=2.5 184  28 恶性 (0.15217391 0.84782609)  \n#&gt;      6) 细胞形状均匀性&lt; 2.5 16   3 良性 (0.81250000 0.18750000) *\n#&gt;      7) 细胞形状均匀性&gt;=2.5 168  15 恶性 (0.08928571 0.91071429)  \n#&gt;       14) 细胞大小均匀性&lt; 4.5 46  12 恶性 (0.26086957 0.73913043)  \n#&gt;         28) 裸核&lt; 2.5 10   3 良性 (0.70000000 0.30000000) *\n#&gt;         29) 裸核&gt;=2.5 36   5 恶性 (0.13888889 0.86111111) *\n#&gt;       15) 细胞大小均匀性&gt;=4.5 122   3 恶性 (0.02459016 0.97540984) *\n\n\n\n1.1.1 模型可视化\n\nCoderpart::plotcp(cdtree$fit)\n\n\n\n\n\n\nCodecdtree %&gt;%\n    extract_fit_engine() %&gt;%\n    rpart.plot::rpart.plot(roundint = F)\n\n\n\n\n\n\n\n\n1.1.2 模型性能评估\n\nCodeaugment(cdtree, new_data = test) %&gt;%\n    accuracy(truth = class, estimate = .pred_class)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.971\n\naugment(cdtree, new_data = test) %&gt;%\n    conf_mat(truth = class, estimate = .pred_class)\n#&gt;           Truth\n#&gt; Prediction 良性 恶性\n#&gt;       良性  132    4\n#&gt;       恶性    2   68",
    "crumbs": [
      "预测性模型",
      "监督学习（Supervised Learning）",
      "基于树的模型"
    ]
  },
  {
    "objectID": "tree-based_models.html#随机森林-分类",
    "href": "tree-based_models.html#随机森林-分类",
    "title": "",
    "section": "\n1.2 随机森林 （分类）",
    "text": "1.2 随机森林 （分类）\n随机森林是装袋法的一种扩展，它不仅对数据进行随机抽样，还对特征进行随机抽样，以此增加模型的多样性和泛化能力。随机森林由大量决策树组成，每棵树都是在一个随机抽取的样本和特征子集上训练的。\n\nCodecf_spec_class &lt;-\n    rand_forest(#mtry = .cols(), \n        trees = 500 ,min_n = 1) %&gt;%\n    set_engine('randomForest', importance = TRUE) %&gt;%\n    set_mode('classification')\n\nclass_rf_fit &lt;- cf_spec_class |&gt; \n    fit(class ~ . , data = train)\nclass_rf_fit\n#&gt; parsnip model object\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt;  randomForest(x = maybe_data_frame(x), y = y, ntree = ~500, nodesize = min_rows(~1,      x), importance = ~TRUE) \n#&gt;                Type of random forest: classification\n#&gt;                      Number of trees: 500\n#&gt; No. of variables tried at each split: 3\n#&gt; \n#&gt;         OOB estimate of  error rate: 3.77%\n#&gt; Confusion matrix:\n#&gt;      良性 恶性 class.error\n#&gt; 良性  300   10  0.03225806\n#&gt; 恶性    8  159  0.04790419\n\n\n\n1.2.1 特征重要性：基于Gini系数的减少\nOOB ，out of bag 袋外预测误差\n这种方法主要用于分类任务。\nMean Decrease in Gini (MDG) 这种方法通过衡量某个特征对分类纯度的贡献来计算其重要性。具体步骤如下：\n\n训练模型：使用所有特征训练随机森林模型。\n计算Gini系数：在决策树中，每次节点分裂都会计算Gini系数减少量。Gini系数用于衡量数据集的纯度，越低表示越纯。\n累加Gini减少量：在每棵树中，计算每个特征在分裂过程中带来的Gini减少量，并将这些减少量累加起来。\n计算平均值：对所有树的累加值取平均值，作为该特征的重要性得分。\n\n\nCodeclass_rf_fit %&gt;% vip::vi()\n#&gt; # A tibble: 9 × 2\n#&gt;   Variable         Importance\n#&gt;   &lt;chr&gt;                 &lt;dbl&gt;\n#&gt; 1 裸核                  25.6 \n#&gt; 2 肿块厚度              21.0 \n#&gt; 3 bland_chromatin       19.2 \n#&gt; 4 细胞大小均匀性        18.4 \n#&gt; 5 细胞形状均匀性        17.9 \n#&gt; 6 边际附着力            14.0 \n#&gt; 7 正常核                13.9 \n#&gt; 8 单个上皮细胞大小      10.6 \n#&gt; 9 有丝分裂               5.88\nclass_rf_fit %&gt;% vip::vip()\n\n\n\n\n\n\n\n\nCodeclass_rf_fit$fit$importance\n#&gt;                         良性        恶性 MeanDecreaseAccuracy MeanDecreaseGini\n#&gt; 肿块厚度         0.045985557 0.041538273          0.044359259        12.465397\n#&gt; 细胞大小均匀性   0.049172066 0.076704399          0.058509661        47.927558\n#&gt; 细胞形状均匀性   0.009876762 0.096802558          0.040209397        41.331213\n#&gt; 边际附着力       0.016551632 0.035342949          0.023040888         6.945234\n#&gt; 单个上皮细胞大小 0.012280597 0.010654858          0.011723405        16.171318\n#&gt; 裸核             0.061713383 0.066225145          0.063075655        40.588065\n#&gt; bland_chromatin  0.017570370 0.058096398          0.031839179        31.183405\n#&gt; 正常核           0.027576658 0.021034748          0.025191390        17.830550\n#&gt; 有丝分裂         0.003095108 0.001130822          0.002400936         1.946241\n\n\n\nCode\naugment(class_rf_fit, new_data = test) %&gt;%\n    accuracy(truth = class, estimate = .pred_class)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.990\n\naugment(class_rf_fit, new_data = test) %&gt;%\n    conf_mat(truth = class, estimate = .pred_class)\n#&gt;           Truth\n#&gt; Prediction 良性 恶性\n#&gt;       良性  132    0\n#&gt;       恶性    2   72\n\n\n\n1.2.2 基于表达数据的应用\n\nCodedf &lt;- dendextend::khan\n\ndf$train.classes\n#&gt;  [1] EWS    EWS    EWS    EWS    EWS    EWS    EWS    EWS    EWS    EWS   \n#&gt; [11] EWS    EWS    EWS    EWS    EWS    EWS    EWS    EWS    EWS    EWS   \n#&gt; [21] EWS    EWS    EWS    BL-NHL BL-NHL BL-NHL BL-NHL BL-NHL BL-NHL BL-NHL\n#&gt; [31] BL-NHL NB     NB     NB     NB     NB     NB     NB     NB     NB    \n#&gt; [41] NB     NB     NB     RMS    RMS    RMS    RMS    RMS    RMS    RMS   \n#&gt; [51] RMS    RMS    RMS    RMS    RMS    RMS    RMS    RMS    RMS    RMS   \n#&gt; [61] RMS    RMS    RMS    RMS   \n#&gt; Levels: EWS BL-NHL NB RMS\ntrain &lt;- t(df$train) |&gt; bind_cols(tibble(class=df$train.classes)) |&gt; \n    relocate(class, .before = 1) |&gt; \n    mutate(\n        class=factor(class,levels = c(\"EWS\", \"BL-NHL\", \"NB\",\"RMS\"))\n    )\nstr(train$class)\n#&gt;  Factor w/ 4 levels \"EWS\",\"BL-NHL\",..: 1 1 1 1 1 1 1 1 1 1 ...\ntable(train$class)\n#&gt; \n#&gt;    EWS BL-NHL     NB    RMS \n#&gt;     23      8     12     21\n\n\n\nCodedf$test.classes\n#&gt;  [1] Normal Normal Normal NB     RMS    Normal Normal NB     EWS    RMS   \n#&gt; [11] BL-NHL EWS    RMS    EWS    EWS    EWS    RMS    BL-NHL RMS    NB    \n#&gt; [21] NB     NB     NB     BL-NHL EWS   \n#&gt; Levels: EWS BL-NHL NB RMS Normal\n\ntest &lt;- t(df$test) |&gt; bind_cols(tibble(class=df$test.classes)) |&gt; \n    relocate(class, .before = 1) |&gt; \n    mutate(\n        class=factor(class,levels = c(\"EWS\", \"BL-NHL\", \"NB\",\"RMS\",\"Normal\"))\n    )\nstr(test$class)\n#&gt;  Factor w/ 5 levels \"EWS\",\"BL-NHL\",..: 5 5 5 3 4 5 5 3 1 4 ...\ntable(test$class)\n#&gt; \n#&gt;    EWS BL-NHL     NB    RMS Normal \n#&gt;      6      3      6      5      5\n\n\n\nCode#\ndt &lt;- class_tree_spec |&gt; fit(class ~ . ,data = train)\nrpart::plotcp(dt$fit)\n\n\n\n\n\n\nCode\ndt%&gt;%\n    extract_fit_engine() %&gt;%\n    rpart.plot::rpart.plot(roundint = F)\n\n\n\n\n\n\n\n\nCode#\nrf_fit_eg &lt;- cf_spec_class |&gt; \n    fit(class ~ . , data = train)\nrf_fit_eg\n#&gt; parsnip model object\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt;  randomForest(x = maybe_data_frame(x), y = y, ntree = ~500, nodesize = min_rows(~1,      x), importance = ~TRUE) \n#&gt;                Type of random forest: classification\n#&gt;                      Number of trees: 500\n#&gt; No. of variables tried at each split: 17\n#&gt; \n#&gt;         OOB estimate of  error rate: 0%\n#&gt; Confusion matrix:\n#&gt;        EWS BL-NHL NB RMS class.error\n#&gt; EWS     23      0  0   0           0\n#&gt; BL-NHL   0      8  0   0           0\n#&gt; NB       0      0 12   0           0\n#&gt; RMS      0      0  0  21           0",
    "crumbs": [
      "预测性模型",
      "监督学习（Supervised Learning）",
      "基于树的模型"
    ]
  },
  {
    "objectID": "tree-based_models.html#随机森林-回归",
    "href": "tree-based_models.html#随机森林-回归",
    "title": "",
    "section": "\n1.3 随机森林 （回归）",
    "text": "1.3 随机森林 （回归）\n\nCodelibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidymodels)\ndata &lt;- haven::read_sav(\"data/抑郁随机森林模型变量重要性.sav\") \n\n\n# glimpse(data)\n# \n# attributes(data$Gender)\n# attributes(data$ZD)\ndata &lt;- data %&gt;% \n    mutate(\n        Gender = factor(Gender),\n        XK = factor(XK),\n        DQ = factor(DQ),\n        SYDLX = factor(SYDLX),\n        \n    )\n\nset.seed(123)\nsplit &lt;- initial_split(data, prop = 0.7)\ntrain_data &lt;- training(split)\ntest_data &lt;- testing(split)\n\n\n\nCode# 构建随机森林模型\nrf_sepc_reg &lt;- rand_forest(mtry = 5, trees = 500) %&gt;%\n    set_engine(\"randomForest\" , importance = TRUE\n               ) %&gt;%\n    set_mode(\"regression\")\n\n# 创建配方\nrf_recipe &lt;- recipe(ZD ~ ., data = train_data) \n\n# 工作流程\nrf_workflow &lt;- workflow() %&gt;%\n    add_recipe(rf_recipe) %&gt;%\n    add_model(rf_sepc_reg)\n\n# 训练模型\nreg_rf_fit &lt;- rf_workflow %&gt;%\n    fit(data = train_data)\nreg_rf_fit %&gt;% extract_fit_parsnip()\n#&gt; parsnip model object\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt;  randomForest(x = maybe_data_frame(x), y = y, ntree = ~500, mtry = min_cols(~5,      x), importance = ~TRUE) \n#&gt;                Type of random forest: regression\n#&gt;                      Number of trees: 500\n#&gt; No. of variables tried at each split: 5\n#&gt; \n#&gt;           Mean of squared residuals: 48.45618\n#&gt;                     % Var explained: 87.48\n\n\n\n1.3.1 特征重要性：基于均方误差（MSE）的减少\n这种方法主要用于回归任务。\nMean Decrease in Accuracy (MDA) 这种方法通过衡量某个特征对整体模型预测准确性的贡献来计算其重要性。具体步骤如下：\n\n训练模型：使用所有特征训练随机森林模型。\n计算基线误差：使用训练好的模型在验证集上计算基线误差（例如，均方误差）。\n扰动特征值：对于每个特征，随机打乱验证集中该特征的值，从而破坏其与目标变量的关系。 重新计算误差：使用扰动后的数据再次计算模型误差。\n计算重要性：特征重要性得分等于扰动后的误差与基线误差之差。误差增加越多，说明该特征对模型预测的贡献越大。\n\n\nCode# 查看特征重要性\nimportance &lt;- reg_rf_fit %&gt;%\n    extract_fit_parsnip() %&gt;%\n    vip::vi()\nvip::vi(reg_rf_fit$fit$fit) \n#&gt; # A tibble: 26 × 2\n#&gt;    Variable Importance\n#&gt;    &lt;chr&gt;         &lt;dbl&gt;\n#&gt;  1 HL             34.9\n#&gt;  2 NE_A           29.4\n#&gt;  3 Sport          21.9\n#&gt;  4 SA             14.6\n#&gt;  5 I              14.1\n#&gt;  6 SMML           13.6\n#&gt;  7 FC             13.5\n#&gt;  8 BECKNC         13.2\n#&gt;  9 YS             12.8\n#&gt; 10 MVS            12.6\n#&gt; # ℹ 16 more rows\n\n\n# 可视化特征重要性\nggplot(importance, aes(x = reorder(Variable, Importance), y = Importance)) + \n    geom_col(fill = \"skyblue\") +\n    coord_flip() +\n    labs(title = \"特征重要性\", x = \"特征\", y = \"重要性得分\")",
    "crumbs": [
      "预测性模型",
      "监督学习（Supervised Learning）",
      "基于树的模型"
    ]
  },
  {
    "objectID": "tree-based_models.html#装袋法",
    "href": "tree-based_models.html#装袋法",
    "title": "",
    "section": "\n1.4 装袋法",
    "text": "1.4 装袋法\n装袋法（Bagging）或称自助聚合（Bootstrap Aggregation）是基于树的模型的一种集成学习技术。\n装袋法是一种集成学习技术，它通过构建多个模型（通常是决策树）并将其预测结果进行平均（对于回归任务）或投票（对于分类任务）来提高模型的准确性和稳健性。其主要步骤如下：\n\n数据抽样：从原始训练数据集中通过自助法（Bootstrap）随机有放回地抽取多个子集。每个子集的大小与原始数据集相同，但由于是有放回地抽样，因此每个子集中可能包含重复的样本。\n模型训练：对每个子集训练一个模型（通常是决策树模型）。由于每个子集的样本可能不同，训练得到的每个模型也可能不同。\n模型集成：在进行预测时，将所有模型的预测结果进行整合。对于分类任务，采用多数投票法，即选择出现次数最多的类别作为最终预测结果；对于回归任务，则采用平均法，即取各模型预测值的平均值作为最终预测结果。",
    "crumbs": [
      "预测性模型",
      "监督学习（Supervised Learning）",
      "基于树的模型"
    ]
  },
  {
    "objectID": "tree-based_models.html#梯度提升树",
    "href": "tree-based_models.html#梯度提升树",
    "title": "",
    "section": "\n1.5 梯度提升树",
    "text": "1.5 梯度提升树\n梯度提升树（Gradient Boosting Trees, GBT）是一种强大的集成学习方法，它通过逐步构建多个决策树，并将它们的预测结果进行加权组合来提高模型的预测性能。与装袋法（Bagging）不同，梯度提升树是一个迭代的过程，在每一步中都试图纠正前一步模型的错误。\n梯度提升树的基本思想是通过逐步构建一系列的弱学习器（通常是决策树）来逼近目标函数。每一个新的树都在先前树的基础上进行改进，使整体模型的预测误差逐步减小。其主要步骤如下：\n\n初始化模型：首先用一个简单的模型（如常数值模型）初始化预测值。\n计算残差：计算初始模型的预测值与实际值之间的差值（残差），这些残差代表了当前模型的误差。\n训练新树：基于残差训练一个新的决策树，目的是学习如何纠正当前模型的误差。\n更新模型：将新树的预测结果加权加入到当前模型中，从而更新整体模型。更新公式通常为：\n\n\\[\nF_m(x)=F_{m-1}(x)+ηh_m(x)F_{m}(x)\n\\] 其中，\\(F_{m}(x)\\) 是第 m 次迭代的模型，\\(\\eta\\) 是学习率（通常在0和1之间），\\(h_m(x)\\) 是第 m 棵树的预测值。\n\n\n重复迭代：重复步骤2-4，直到达到预定的树的数量或误差收敛。",
    "crumbs": [
      "预测性模型",
      "监督学习（Supervised Learning）",
      "基于树的模型"
    ]
  },
  {
    "objectID": "tree-based_models.html#贝叶斯相加回归树",
    "href": "tree-based_models.html#贝叶斯相加回归树",
    "title": "",
    "section": "\n1.6 贝叶斯相加回归树",
    "text": "1.6 贝叶斯相加回归树",
    "crumbs": [
      "预测性模型",
      "监督学习（Supervised Learning）",
      "基于树的模型"
    ]
  },
  {
    "objectID": "predict_evaluate.html",
    "href": "predict_evaluate.html",
    "title": "",
    "section": "",
    "text": "机器学习模型评估 CodeShow All CodeHide All CodeView Source",
    "crumbs": [
      "预测性模型",
      "机器学习",
      "模型评估"
    ]
  },
  {
    "objectID": "predict_evaluate.html#预测",
    "href": "predict_evaluate.html#预测",
    "title": "",
    "section": "\n1.1 预测",
    "text": "1.1 预测\npredict(object, new_data, type = NULL, opts = list(), ...)\n\n1.1.1 回归\n“raw”，“numeric” ，“conf_int”，“pred_int”\n\nCodepredict(lm_fit,new_data =  ames_test,type =\"conf_int\")\n#&gt; # A tibble: 588 × 2\n#&gt;    .pred_lower .pred_upper\n#&gt;          &lt;dbl&gt;       &lt;dbl&gt;\n#&gt;  1        5.22        5.27\n#&gt;  2        5.25        5.29\n#&gt;  3        5.22        5.26\n#&gt;  4        5.18        5.24\n#&gt;  5        5.62        5.69\n#&gt;  6        5.10        5.15\n#&gt;  7        5.02        5.07\n#&gt;  8        5.19        5.25\n#&gt;  9        4.97        5.03\n#&gt; 10        5.28        5.35\n#&gt; # ℹ 578 more rows\n\n\n\n1.1.2 分类\n\nCode# 分类  \"class\", \"prob\",\n\n\n\n1.1.3 生存\ncensored regression\n\nCode#  \"time\"，\"hazard\",\"survival\"，\"linear_pred\"\n\n\n\n1.1.4 特殊\n\nCode# \"quantile\", \"raw\"",
    "crumbs": [
      "预测性模型",
      "机器学习",
      "模型评估"
    ]
  },
  {
    "objectID": "predict_evaluate.html#添加",
    "href": "predict_evaluate.html#添加",
    "title": "",
    "section": "\n1.2 添加",
    "text": "1.2 添加\naugment(x = fit, new_data, eval_time = NULL, ...)\n\nCodeaugment(lm_fit, new_data = ames_test) %&gt;% select(1:2,Sale_Price)\n#&gt; # A tibble: 588 × 3\n#&gt;    .pred  .resid Sale_Price\n#&gt;    &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n#&gt;  1  5.24  0.0881       5.33\n#&gt;  2  5.27  0.0162       5.29\n#&gt;  3  5.24  0.0161       5.26\n#&gt;  4  5.21  0.0263       5.23\n#&gt;  5  5.65  0.0761       5.73\n#&gt;  6  5.13  0.0462       5.17\n#&gt;  7  5.04  0.0160       5.06\n#&gt;  8  5.22  0.0465       5.26\n#&gt;  9  5.00  0.0214       5.02\n#&gt; 10  5.32 -0.0730       5.24\n#&gt; # ℹ 578 more rows",
    "crumbs": [
      "预测性模型",
      "机器学习",
      "模型评估"
    ]
  },
  {
    "objectID": "predict_evaluate.html#tidy",
    "href": "predict_evaluate.html#tidy",
    "title": "",
    "section": "\n1.3 tidy()\n",
    "text": "1.3 tidy()\n\n\nCodetidy\n#&gt; function (x, ...) \n#&gt; {\n#&gt;     UseMethod(\"tidy\")\n#&gt; }\n#&gt; &lt;bytecode: 0x00000226c37550b8&gt;\n#&gt; &lt;environment: namespace:generics&gt;\ntidy(rec)\n#&gt; # A tibble: 5 × 6\n#&gt;   number operation type     trained skip  id            \n#&gt;    &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;    &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;         \n#&gt; 1      1 step      log      FALSE   FALSE log10         \n#&gt; 2      2 step      other    FALSE   FALSE other_id      \n#&gt; 3      3 step      dummy    FALSE   FALSE dummy_1ntLu   \n#&gt; 4      4 step      interact FALSE   FALSE interact_WyI52\n#&gt; 5      5 step      ns       FALSE   FALSE ns_NGGbc\ntidy(rec,id = \"other_id\")\n#&gt; # A tibble: 1 × 3\n#&gt;   terms        retained id      \n#&gt;   &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;   \n#&gt; 1 Neighborhood &lt;NA&gt;     other_id\ntidy(rec, number = 2)\n#&gt; # A tibble: 1 × 3\n#&gt;   terms        retained id      \n#&gt;   &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;   \n#&gt; 1 Neighborhood &lt;NA&gt;     other_id\n\n\n\nCodetidy(lm_fit)\n#&gt; # A tibble: 72 × 5\n#&gt;    term                            estimate std.error statistic   p.value\n#&gt;    &lt;chr&gt;                              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1 (Intercept)                     -0.662    0.298        -2.22 2.66e-  2\n#&gt;  2 Gr_Liv_Area                      0.650    0.0155       41.9  1.05e-284\n#&gt;  3 Year_Built                       0.00194  0.000137     14.2  5.05e- 44\n#&gt;  4 Neighborhood_College_Creek      -0.0377   0.0327       -1.15 2.49e-  1\n#&gt;  5 Neighborhood_Old_Town           -0.0355   0.0123       -2.90 3.81e-  3\n#&gt;  6 Neighborhood_Edwards            -0.0785   0.0268       -2.93 3.41e-  3\n#&gt;  7 Neighborhood_Somerset            0.0638   0.0188        3.39 7.23e-  4\n#&gt;  8 Neighborhood_Northridge_Heights  0.143    0.0272        5.26 1.56e-  7\n#&gt;  9 Neighborhood_Gilbert             0.0223   0.0213        1.04 2.96e-  1\n#&gt; 10 Neighborhood_Sawyer             -0.115    0.0252       -4.58 5.00e-  6\n#&gt; # ℹ 62 more rows",
    "crumbs": [
      "预测性模型",
      "机器学习",
      "模型评估"
    ]
  },
  {
    "objectID": "predict_evaluate.html#模型评估-yardstick",
    "href": "predict_evaluate.html#模型评估-yardstick",
    "title": "",
    "section": "\n4 模型评估 yardstick\n",
    "text": "4 模型评估 yardstick\n\n\n4.1 回归指标\n\nCodeames_test_res &lt;- predict(lm_fit, new_data = ames_test %&gt;% select(-Sale_Price))\names_test_res\n#&gt; # A tibble: 588 × 1\n#&gt;    .pred\n#&gt;    &lt;dbl&gt;\n#&gt;  1  5.24\n#&gt;  2  5.27\n#&gt;  3  5.24\n#&gt;  4  5.21\n#&gt;  5  5.65\n#&gt;  6  5.13\n#&gt;  7  5.04\n#&gt;  8  5.22\n#&gt;  9  5.00\n#&gt; 10  5.32\n#&gt; # ℹ 578 more rows\n\names_test_res &lt;- bind_cols(ames_test_res, ames_test %&gt;% select(Sale_Price))\names_test_res\n#&gt; # A tibble: 588 × 2\n#&gt;    .pred Sale_Price\n#&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n#&gt;  1  5.24       5.33\n#&gt;  2  5.27       5.29\n#&gt;  3  5.24       5.26\n#&gt;  4  5.21       5.23\n#&gt;  5  5.65       5.73\n#&gt;  6  5.13       5.17\n#&gt;  7  5.04       5.06\n#&gt;  8  5.22       5.26\n#&gt;  9  5.00       5.02\n#&gt; 10  5.32       5.24\n#&gt; # ℹ 578 more rows\n\n\n\nCodeggplot(ames_test_res, aes(x = Sale_Price, y = .pred)) + \n  # Create a diagonal line:\n  geom_abline(lty = 2) + \n  geom_point(alpha = 0.5) + \n  labs(y = \"Predicted Sale Price (log10)\", x = \"Sale Price (log10)\") +\n  # Scale and size the x- and y-axis uniformly:\n  coord_obs_pred()\n\n\n\n\n\n\n\n\n4.1.1 均方根误差RMSE\n\nCodermse(ames_test_res, truth = Sale_Price, estimate = .pred)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 rmse    standard      0.0772\n\n\n\n4.1.2 决定系数R2，平均绝对误差MAE\n\nCodemetrics &lt;- metric_set(rmse, rsq, mae)\nmetrics(ames_test_res, truth = Sale_Price, estimate = .pred)\n#&gt; # A tibble: 3 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 rmse    standard      0.0772\n#&gt; 2 rsq     standard      0.795 \n#&gt; 3 mae     standard      0.0550\n\n\n\n4.2 二分类指标\n\nCodedata(two_class_example)\ntibble(two_class_example)\n#&gt; # A tibble: 500 × 4\n#&gt;    truth   Class1   Class2 predicted\n#&gt;    &lt;fct&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;    \n#&gt;  1 Class2 0.00359 0.996    Class2   \n#&gt;  2 Class1 0.679   0.321    Class1   \n#&gt;  3 Class2 0.111   0.889    Class2   \n#&gt;  4 Class1 0.735   0.265    Class1   \n#&gt;  5 Class2 0.0162  0.984    Class2   \n#&gt;  6 Class1 0.999   0.000725 Class1   \n#&gt;  7 Class1 0.999   0.000799 Class1   \n#&gt;  8 Class1 0.812   0.188    Class1   \n#&gt;  9 Class2 0.457   0.543    Class2   \n#&gt; 10 Class2 0.0976  0.902    Class2   \n#&gt; # ℹ 490 more rows\n\n\n\nCode# 混淆矩阵\n# A confusion matrix: \nconf_mat(two_class_example, truth = truth, estimate = predicted)\n#&gt;           Truth\n#&gt; Prediction Class1 Class2\n#&gt;     Class1    227     50\n#&gt;     Class2     31    192\n\n\n\nCode# Accuracy:\naccuracy(two_class_example, truth, predicted)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.838\n\n\n\nCode# Matthews correlation coefficient:\nmcc(two_class_example, truth, predicted)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 mcc     binary         0.677\n\n\n\nCode# F1 metric:\nf_meas(two_class_example, truth, predicted)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 f_meas  binary         0.849\n\n\n\nCode# Combining these three classification metrics together\nclassification_metrics &lt;- metric_set(accuracy, mcc, f_meas)\nclassification_metrics(two_class_example, truth = truth, estimate = predicted)\n#&gt; # A tibble: 3 × 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.838\n#&gt; 2 mcc      binary         0.677\n#&gt; 3 f_meas   binary         0.849\n\n\n感兴趣 事件水平\n第二级逻辑将结果编码为0/1（在这种情况下，第二个值是事件）\n\nCodef_meas(two_class_example, truth, predicted, event_level = \"second\")\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 f_meas  binary         0.826\n\n\n\n4.2.1 ROC，AUC\n不使用预测类列,对于两类问题，感兴趣事件的概率列将传递到函数中\n\nCodetwo_class_curve &lt;- roc_curve(two_class_example, truth, Class1)\ntwo_class_curve\n#&gt; # A tibble: 502 × 3\n#&gt;    .threshold specificity sensitivity\n#&gt;         &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n#&gt;  1 -Inf           0                 1\n#&gt;  2    1.79e-7     0                 1\n#&gt;  3    4.50e-6     0.00413           1\n#&gt;  4    5.81e-6     0.00826           1\n#&gt;  5    5.92e-6     0.0124            1\n#&gt;  6    1.22e-5     0.0165            1\n#&gt;  7    1.40e-5     0.0207            1\n#&gt;  8    1.43e-5     0.0248            1\n#&gt;  9    2.38e-5     0.0289            1\n#&gt; 10    3.30e-5     0.0331            1\n#&gt; # ℹ 492 more rows\nroc_auc(two_class_example, truth, Class1)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 roc_auc binary         0.939\n\n\n\nCodeautoplot(two_class_curve)+\n    annotate(\"text\",x=0.5,y=0.25,label=\"AUC=0.939\")\n\n\n\n\n\n\n\n\n4.3 多分类指标\n\nCodedata(hpc_cv)\ntibble(hpc_cv)\n#&gt; # A tibble: 3,467 × 7\n#&gt;    obs   pred     VF      F       M          L Resample\n#&gt;    &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;   \n#&gt;  1 VF    VF    0.914 0.0779 0.00848 0.0000199  Fold01  \n#&gt;  2 VF    VF    0.938 0.0571 0.00482 0.0000101  Fold01  \n#&gt;  3 VF    VF    0.947 0.0495 0.00316 0.00000500 Fold01  \n#&gt;  4 VF    VF    0.929 0.0653 0.00579 0.0000156  Fold01  \n#&gt;  5 VF    VF    0.942 0.0543 0.00381 0.00000729 Fold01  \n#&gt;  6 VF    VF    0.951 0.0462 0.00272 0.00000384 Fold01  \n#&gt;  7 VF    VF    0.914 0.0782 0.00767 0.0000354  Fold01  \n#&gt;  8 VF    VF    0.918 0.0744 0.00726 0.0000157  Fold01  \n#&gt;  9 VF    VF    0.843 0.128  0.0296  0.000192   Fold01  \n#&gt; 10 VF    VF    0.920 0.0728 0.00703 0.0000147  Fold01  \n#&gt; # ℹ 3,457 more rows\n\n\n\nCodeaccuracy(hpc_cv, obs, pred)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy multiclass     0.709\nmcc(hpc_cv, obs, pred)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 mcc     multiclass     0.515\n\n\n二分类可拓展到多分类\n\nCodesensitivity(hpc_cv, obs, pred, estimator = \"macro\")\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 sensitivity macro          0.560\nsensitivity(hpc_cv, obs, pred, estimator = \"macro_weighted\")\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric     .estimator     .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;              &lt;dbl&gt;\n#&gt; 1 sensitivity macro_weighted     0.709\nsensitivity(hpc_cv, obs, pred, estimator = \"micro\")\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 sensitivity micro          0.709\n\n\n多分类\n\nCoderoc_auc(hpc_cv, obs, VF, F, M, L)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 roc_auc hand_till      0.829\nroc_auc(hpc_cv, obs, VF, F, M, L, estimator = \"macro_weighted\")\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator     .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;\n#&gt; 1 roc_auc macro_weighted     0.868\n\n\n\nCodehpc_cv %&gt;% \n  group_by(Resample) %&gt;% \n  accuracy(obs, pred)\n#&gt; # A tibble: 10 × 4\n#&gt;    Resample .metric  .estimator .estimate\n#&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt;  1 Fold01   accuracy multiclass     0.726\n#&gt;  2 Fold02   accuracy multiclass     0.712\n#&gt;  3 Fold03   accuracy multiclass     0.758\n#&gt;  4 Fold04   accuracy multiclass     0.712\n#&gt;  5 Fold05   accuracy multiclass     0.712\n#&gt;  6 Fold06   accuracy multiclass     0.697\n#&gt;  7 Fold07   accuracy multiclass     0.675\n#&gt;  8 Fold08   accuracy multiclass     0.721\n#&gt;  9 Fold09   accuracy multiclass     0.673\n#&gt; 10 Fold10   accuracy multiclass     0.699\n\n# Four 1-vs-all ROC curves for each fold\nhpc_cv %&gt;% \n  group_by(Resample) %&gt;% \n  roc_curve(obs, VF, F, M, L) %&gt;% \n  autoplot()",
    "crumbs": [
      "预测性模型",
      "机器学习",
      "拟合"
    ]
  },
  {
    "objectID": "predict_evaluate.html#回归指标",
    "href": "predict_evaluate.html#回归指标",
    "title": "",
    "section": "\n1.4 回归指标",
    "text": "1.4 回归指标\n\nCodeames_test_res &lt;- predict(lm_fit, new_data = ames_test %&gt;% select(-Sale_Price))\names_test_res\n#&gt; # A tibble: 588 × 1\n#&gt;    .pred\n#&gt;    &lt;dbl&gt;\n#&gt;  1  5.24\n#&gt;  2  5.27\n#&gt;  3  5.24\n#&gt;  4  5.21\n#&gt;  5  5.65\n#&gt;  6  5.13\n#&gt;  7  5.04\n#&gt;  8  5.22\n#&gt;  9  5.00\n#&gt; 10  5.32\n#&gt; # ℹ 578 more rows\n\names_test_res &lt;- bind_cols(ames_test_res, ames_test %&gt;% select(Sale_Price))\names_test_res\n#&gt; # A tibble: 588 × 2\n#&gt;    .pred Sale_Price\n#&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n#&gt;  1  5.24       5.33\n#&gt;  2  5.27       5.29\n#&gt;  3  5.24       5.26\n#&gt;  4  5.21       5.23\n#&gt;  5  5.65       5.73\n#&gt;  6  5.13       5.17\n#&gt;  7  5.04       5.06\n#&gt;  8  5.22       5.26\n#&gt;  9  5.00       5.02\n#&gt; 10  5.32       5.24\n#&gt; # ℹ 578 more rows\n\n\n\nCodeggplot(ames_test_res, aes(x = Sale_Price, y = .pred)) + \n  # Create a diagonal line:\n  geom_abline(lty = 2) + \n  geom_point(alpha = 0.5) + \n  labs(y = \"Predicted Sale Price (log10)\", x = \"Sale Price (log10)\") +\n  # Scale and size the x- and y-axis uniformly:\n  coord_obs_pred()\n\n\n\n\n\n\n\n\n1.4.0.1 均方根误差RMSE\n\nCodermse(ames_test_res, truth = Sale_Price, estimate = .pred)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 rmse    standard      0.0772\n\n\n\n1.4.0.2 决定系数R2，平均绝对误差MAE\n\nCodemetrics &lt;- metric_set(rmse, rsq, mae)\nmetrics(ames_test_res, truth = Sale_Price, estimate = .pred)\n#&gt; # A tibble: 3 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 rmse    standard      0.0772\n#&gt; 2 rsq     standard      0.795 \n#&gt; 3 mae     standard      0.0550",
    "crumbs": [
      "预测性模型",
      "机器学习",
      "模型评估"
    ]
  },
  {
    "objectID": "predict_evaluate.html#分类指标",
    "href": "predict_evaluate.html#分类指标",
    "title": "",
    "section": "\n1.5 分类指标",
    "text": "1.5 分类指标\n\n1.5.1 二分类\n\nCodedata(two_class_example)\ntibble(two_class_example)\n#&gt; # A tibble: 500 × 4\n#&gt;    truth   Class1   Class2 predicted\n#&gt;    &lt;fct&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;    \n#&gt;  1 Class2 0.00359 0.996    Class2   \n#&gt;  2 Class1 0.679   0.321    Class1   \n#&gt;  3 Class2 0.111   0.889    Class2   \n#&gt;  4 Class1 0.735   0.265    Class1   \n#&gt;  5 Class2 0.0162  0.984    Class2   \n#&gt;  6 Class1 0.999   0.000725 Class1   \n#&gt;  7 Class1 0.999   0.000799 Class1   \n#&gt;  8 Class1 0.812   0.188    Class1   \n#&gt;  9 Class2 0.457   0.543    Class2   \n#&gt; 10 Class2 0.0976  0.902    Class2   \n#&gt; # ℹ 490 more rows\n\n\n\nCode# 混淆矩阵\n# A confusion matrix: \nconf_mat(two_class_example, truth = truth, estimate = predicted)\n#&gt;           Truth\n#&gt; Prediction Class1 Class2\n#&gt;     Class1    227     50\n#&gt;     Class2     31    192\n\n\n\nCode# Accuracy:\naccuracy(two_class_example, truth, predicted)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.838\n\n\n\nCode# Matthews correlation coefficient:\nmcc(two_class_example, truth, predicted)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 mcc     binary         0.677\n\n\n\nCode# F1 metric:\nf_meas(two_class_example, truth, predicted)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 f_meas  binary         0.849\n\n\n\nCode# Combining these three classification metrics together\nclassification_metrics &lt;- metric_set(accuracy, mcc, f_meas)\nclassification_metrics(two_class_example, truth = truth, estimate = predicted)\n#&gt; # A tibble: 3 × 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy binary         0.838\n#&gt; 2 mcc      binary         0.677\n#&gt; 3 f_meas   binary         0.849\n\n\n感兴趣 事件水平\n第二级逻辑将结果编码为0/1（在这种情况下，第二个值是事件）\n\nCodef_meas(two_class_example, truth, predicted, event_level = \"second\")\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 f_meas  binary         0.826\n\n\n\n1.5.1.1 ROC，AUC\n不使用预测类列,对于两类问题，感兴趣事件的概率列将传递到函数中\n\nCodetwo_class_curve &lt;- roc_curve(two_class_example, truth, Class1)\ntwo_class_curve\n#&gt; # A tibble: 502 × 3\n#&gt;    .threshold specificity sensitivity\n#&gt;         &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n#&gt;  1 -Inf           0                 1\n#&gt;  2    1.79e-7     0                 1\n#&gt;  3    4.50e-6     0.00413           1\n#&gt;  4    5.81e-6     0.00826           1\n#&gt;  5    5.92e-6     0.0124            1\n#&gt;  6    1.22e-5     0.0165            1\n#&gt;  7    1.40e-5     0.0207            1\n#&gt;  8    1.43e-5     0.0248            1\n#&gt;  9    2.38e-5     0.0289            1\n#&gt; 10    3.30e-5     0.0331            1\n#&gt; # ℹ 492 more rows\nroc_auc(two_class_example, truth, Class1)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 roc_auc binary         0.939\n\n\n\nCodeautoplot(two_class_curve)+\n    annotate(\"text\",x=0.5,y=0.25,label=\"AUC=0.939\")\n\n\n\n\n\n\n\n\n1.5.2 多分类\n\nCodedata(hpc_cv)\ntibble(hpc_cv)\n#&gt; # A tibble: 3,467 × 7\n#&gt;    obs   pred     VF      F       M          L Resample\n#&gt;    &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;   \n#&gt;  1 VF    VF    0.914 0.0779 0.00848 0.0000199  Fold01  \n#&gt;  2 VF    VF    0.938 0.0571 0.00482 0.0000101  Fold01  \n#&gt;  3 VF    VF    0.947 0.0495 0.00316 0.00000500 Fold01  \n#&gt;  4 VF    VF    0.929 0.0653 0.00579 0.0000156  Fold01  \n#&gt;  5 VF    VF    0.942 0.0543 0.00381 0.00000729 Fold01  \n#&gt;  6 VF    VF    0.951 0.0462 0.00272 0.00000384 Fold01  \n#&gt;  7 VF    VF    0.914 0.0782 0.00767 0.0000354  Fold01  \n#&gt;  8 VF    VF    0.918 0.0744 0.00726 0.0000157  Fold01  \n#&gt;  9 VF    VF    0.843 0.128  0.0296  0.000192   Fold01  \n#&gt; 10 VF    VF    0.920 0.0728 0.00703 0.0000147  Fold01  \n#&gt; # ℹ 3,457 more rows\n\n\n\nCodeaccuracy(hpc_cv, obs, pred)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric  .estimator .estimate\n#&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 accuracy multiclass     0.709\nmcc(hpc_cv, obs, pred)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 mcc     multiclass     0.515\n\n\n二分类可拓展到多分类\n\nCodesensitivity(hpc_cv, obs, pred, estimator = \"macro\")\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 sensitivity macro          0.560\nsensitivity(hpc_cv, obs, pred, estimator = \"macro_weighted\")\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric     .estimator     .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;              &lt;dbl&gt;\n#&gt; 1 sensitivity macro_weighted     0.709\nsensitivity(hpc_cv, obs, pred, estimator = \"micro\")\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric     .estimator .estimate\n#&gt;   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 sensitivity micro          0.709\n\n\n多分类\n\nCoderoc_auc(hpc_cv, obs, VF, F, M, L)\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n#&gt; 1 roc_auc hand_till      0.829\nroc_auc(hpc_cv, obs, VF, F, M, L, estimator = \"macro_weighted\")\n#&gt; # A tibble: 1 × 3\n#&gt;   .metric .estimator     .estimate\n#&gt;   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;\n#&gt; 1 roc_auc macro_weighted     0.868\n\n\n\nCodehpc_cv %&gt;% \n  group_by(Resample) %&gt;% \n  accuracy(obs, pred)\n#&gt; # A tibble: 10 × 4\n#&gt;    Resample .metric  .estimator .estimate\n#&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n#&gt;  1 Fold01   accuracy multiclass     0.726\n#&gt;  2 Fold02   accuracy multiclass     0.712\n#&gt;  3 Fold03   accuracy multiclass     0.758\n#&gt;  4 Fold04   accuracy multiclass     0.712\n#&gt;  5 Fold05   accuracy multiclass     0.712\n#&gt;  6 Fold06   accuracy multiclass     0.697\n#&gt;  7 Fold07   accuracy multiclass     0.675\n#&gt;  8 Fold08   accuracy multiclass     0.721\n#&gt;  9 Fold09   accuracy multiclass     0.673\n#&gt; 10 Fold10   accuracy multiclass     0.699\n\n# Four 1-vs-all ROC curves for each fold\nhpc_cv %&gt;% \n  group_by(Resample) %&gt;% \n  roc_curve(obs, VF, F, M, L) %&gt;% \n  autoplot()",
    "crumbs": [
      "预测性模型",
      "机器学习",
      "模型评估"
    ]
  }
]